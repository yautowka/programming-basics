Id,OwnerUserId,CreationDate,ParentId,Score,Body
92,61.0,2008-08-01T14:45:37Z,90,13,"Version Control with Subversion A very good resource for source control in general. Not really TortoiseSVN specific, though."
124,26.0,2008-08-01T16:09:47Z,80,12,"I wound up using this. It is a kind of a hack, but it actually works pretty well. The only thing is you have to be very careful with your semicolons. : D var strSql:String = stream.readUTFBytes(stream.bytesAvailable);      
var i:Number = 0;
var strSqlSplit:Array = strSql.split("";"");
for (i = 0; i < strSqlSplit.length; i++){
    NonQuery(strSqlSplit[i].toString());
}"
199,50.0,2008-08-01T19:36:46Z,180,1,"I've read somewhere the human eye can't distinguish between less than 4 values apart. so This is something to keep in mind. The following algorithm does not compensate for this. I'm not sure this is exactly what you want, but this is one way to randomly generate non-repeating color values: (beware, inconsistent pseudo-code ahead) //colors entered as 0-255 [R, G, B] colors = []; //holds final colors to be used rand = new Random(); //assumes n is less than 16,777,216 randomGen(int n){ while (len(colors) < n){ //generate a random number between 0,255 for each color newRed = rand.next(256); newGreen = rand.next(256); newBlue = rand.next(256); temp = [newRed, newGreen, newBlue]; //only adds new colors to the array if temp not in colors { colors.append(temp); } } } One way you could optimize this for better visibility would be to compare the distance between each new color and all the colors in the array: for item in color{ itemSq = (item[0]^2 + item[1]^2 + item[2]^2])^(.5); tempSq = (temp[0]^2 + temp[1]^2 + temp[2]^2])^(.5); dist = itemSq - tempSq; dist = abs(dist); } //NUMBER can be your chosen distance apart. if dist < NUMBER and temp not in colors { colors.append(temp); } But this approach would significantly slow down your algorithm. Another way would be to scrap the randomness and systematically go through every 4 values and add a color to an array in the above example."
269,91.0,2008-08-01T23:49:57Z,260,4,"Yes, I thought about that, but I soon figured out that another Domain-Specific-Language (DSL) would be a bit too much. Essentially, they need to interact with my gamestate in possibly unpredictable ways. For example, a card could have a rule ""When this cards enter play, all your undead minions gain +3 attack against flying enemies, except when the enemy is blessed"". As trading card games are turn based, the GameState Manager will fire OnStageX events and let the cards modify other cards or the GameState in whatever way the card needs. If I try to create a DSL, I have to implement a rather large feature set and possibly constantly update it, which shifts the maintenance work to another part without actually removing it. That's why I wanted to stay with a ""real"" .NET language to essentially be able to just fire the event and let the card manipulate the gamestate in whatever way (within the limits of the code access security)."
307,49.0,2008-08-02T01:49:46Z,260,28,"Oleg Shilo's C# Script solution (at The Code Project ) really is a great introduction to providing script abilities in your application. A different approach would be to consider a language that is specifically built for scripting, such as IronRuby , IronPython , or Lua . IronPython and IronRuby are both available today. For a guide to embedding IronPython read How to embed IronPython script support in your existing app in 10 easy steps . Lua is a scripting language commonly used in games. There is a Lua compiler for .NET, available from CodePlex -- http://www.codeplex.com/Nua That codebase is a great read if you want to learn about building a compiler in .NET. A different angle altogether is to try PowerShell . There are numerous examples of embedding PowerShell into an application -- here's a thorough project on the topic: Powershell Tunnel"
332,59.0,2008-08-02T03:00:24Z,330,19,"I would be a bit reluctant to use nested classes here.  What if you created an abstract base class for a ""multimedia driver"" to handle the back-end stuff (workhorse), and a separate class for the front-end work?  The front-end class could take a pointer/reference to an implemented driver class (for the appropriate media type and situation) and perform the abstract operations on the workhorse structure. My philosophy would be to go ahead and make both structures accessible to the client in a polished way, just under the assumption they would be used in tandem. I would reference something like a QTextDocument in Qt.  You provide a direct interface to the bare metal data handling, but pass the authority along to an object like a QTextEdit to do the manipulation."
344,100.0,2008-08-02T04:18:15Z,260,6,"You might be able to use IronRuby for that. Otherwise I'd suggest you have a directory where you place precompiled assemblies. Then you could have a reference in the DB to the assembly and class, and use reflection to load the proper assemblies at runtime. If you really want to compile at run-time you could use the CodeDOM, then you could use reflection to load the dynamic assembly. MSDN article which might help ."
359,119.0,2008-08-02T06:16:23Z,260,5,"You could use any of the DLR languages, which provide a way to really easily host your own scripting platform. However, you don't have to use a scripting language for this. You could use C# and compile it with the C# code provider. As long as you load it in its own AppDomain, you can load and unload it to your heart's content."
473,49.0,2008-08-02T15:33:13Z,470,8,"No, what you're doing is fine. Don't let those people confuse you. If you've written the web services with .net then the reference proxies generated by .net are going to be quite suitable. The situation you describe (where you are both producer and consumer) is the ideal situation. If you need to connect to a web services that is unknown at compile time, then you would want a more dynamic approach, where you deduce the 'shape' of the web service. But start by using the auto generated proxy class, and don't worry about it until you hit a limitation. And when you do -- come back to stack overflow ;-)"
529,86.0,2008-08-02T18:16:07Z,180,3,"Isn't it also a factor which order you set up the colors? Like if you use Dillie-Os idea you need to mix the colors as much as possible. 
0 64 128 256 is from one to the next. but 0 256 64 128 in a wheel would be more ""apart"" Does this make sense?"
539,157.0,2008-08-02T19:03:52Z,180,21,"My first thought on this is ""how generate N vectors in a space that maximize distance from each other."" You can see that the RGB (or any other scale you use that forms a basis in color space) are just vectors. Take a look at Random Point Picking . Hope this is a good start for you! Once you have a set of vectors that are maximized a part, you can save them in a hash table or something for later, and just perform random rotations on them to get all the colors you desire that are maximally apart from each other! Edit: Thinking about this problem more, it would be better to map the colors in a linear manor, possibly (0,0,0) --> (255,255,255) lexicographically, and then distribute them evenly. I really don't know how well this will work, but it should since, lets say: n = 10
we know we have 16777216 colors (256^3). We can use buckles algorithm 515 to find the lexicographically indexed color. . You'll probably have to edit the algorithm to avoid overflow and probably add some minor speed improvements."
585,149.0,2008-08-02T23:40:04Z,580,13,"For my projects I alternate between SQL Compare from REd Gate and the Database Publishing Wizard from Microsoft which you can download free here . The Wizard isn't as slick as SQL Compare or SQL Data Compare but it does the trick. One issue is that the scripts it generates may need some rearranging and/or editing to flow in one shot. On the up side, it can move your schema and data which isn't bad for a free tool."
586,34.0,2008-08-02T23:51:09Z,580,17,"I've taken to hand-coding all of my DDL (creates/alter/delete) statements, adding them to my .sln as text files, and using normal versioning (using subversion, but any revision control should work). This way, I not only get the benefit of versioning, but updating live from dev/stage is the same process for code and database - tags, branches and so on work all the same. Otherwise, I agree redgate is expensive if you don't have a company buying it for you. If you can get a company to buy it for you though, it really is worth it!"
590,116.0,2008-08-03T00:22:03Z,580,2,"If you have a company buying it, Toad from Quest Software has this kind of management functionality built in.  It's basically a two-click operation to compare two schemas and generate a sync script from one to the other. They have editions for most of the popular databases, including of course Sql Server."
591,111.0,2008-08-03T00:37:03Z,580,3,"I work the same way Karl does, by keeping all of my SQL scripts for creating and altering tables in a text file that I keep in source control.  In fact, to avoid the problem of having to have a script examine the live database to determine what ALTERs to run, I usually work like this: On the first version, I place everything during testing into one SQL script, and treat all tables as a CREATE.  This means I end up dropping and readding tables a lot during testing, but that's not a big deal early into the project (since I'm usually hacking the data I'm using at that point anyway). On all subsequent versions, I do two things: I make a new text file to hold the upgrade SQL scripts, that contain just the ALTERs for that version.  And I make the changes to the original, create a fresh database script as well.  This way an upgrade just runs the upgrade script, but if we have to recreate the DB we don't need to run 100 scripts to get there. Depending on how I'm deploying the DB changes, I'll also usually put a version table in the DB that holds the version of the DB.  Then, rather than make any human decisions about which scripts to run, whatever code I have running the create/upgrade scripts uses the version to determine what to run. The one thing this will not do is help if part of what you're moving from test to production is data, but if you want to manage structure and not pay for a nice, but expensive DB management package, is really not very difficult.  I've also found it's a pretty good way of keeping mental track of your DB."
597,76.0,2008-08-03T01:38:02Z,580,1,I agree that scripting everything is the best way to go and is what I advocate at work.  You should script everything from DB and object creation to populating your lookup tables. Anything you do in UI only won't translate (especially for changes... not so much for first deployments) and will end up requiring a tools like what Redgate offers.
655,91.0,2008-08-03T11:41:38Z,650,69,"With the ""Built in"" stuff, you can't, as using 1.0.* or 1.0.0.* will replace the revision and build numbers with a coded date/timestamp, which is usually also a good way. For more info, see the Assembly Linker Documentation in the /v tag. As for automatically incrementing numbers, use the AssemblyInfo Task: AssemblyInfo Task This can be configured to automatically increment the build number. There are 2 Gotchas: Each of the 4 numbers in the Version string is limited to 65535. This is a Windows Limitation and unlikely to get fixed. Why are build numbers limited to 65535? Using with with Subversion requires a small change: Using MSBuild to generate assembly version info at build time (including SubVersion fix) Retrieving the Version number is then quite easy: Version v = Assembly.GetExecutingAssembly().GetName().Version;
string About = string.Format(CultureInfo.InvariantCulture, @""YourApp Version {0}.{1}.{2} (r{3})"", v.Major, v.Minor, v.Build, v.Revision); And, to clarify: In .net or at least in C#, the build is actually the THIRD number, not the fourth one as some people (for example Delphi Developers who are used to Major.Minor.Release.Build) might expect. In .net, it's Major.Minor.Build.Revision."
787,175.0,2008-08-03T18:46:33Z,650,4,What source control system are you using? Almost all of them have some form of $ Id $ tag that gets expanded when the file is checked in. I usually use some form of hackery to display this as the version number. The other alternative is use to use the date as the build number: 080803-1448
818,233.0,2008-08-03T20:45:27Z,810,0,"I'm partway to my solution with this entry on MSDN (don't know how I couldn't find it before). User/Machine Hive Subkeys and values entered under this hive will be installed under the HKEY_CURRENT_USER hive when a user chooses ""Just Me"" or the HKEY_USERS hive or when a user chooses ""Everyone"" during installation. Registry Editor"
820,91.0,2008-08-03T20:48:47Z,810,3,"First: Yes, this is something that belongs in the Application for the exact reson you specified: What happens after new user profiles are created? Sure, if you're using a domain it's possible to have some stuff put in the registry on creation, but this is not really a use case. The Application should check if there are seetings and use the default settings if not. That being said, it IS possible to change other users Keys through the HKEY_USERS Hive. I have no experience with the Visual Studio 2003 Setup Project, so here is a bit of (totally unrelated) VBScript code that might just give you an idea where to look: const HKEY_USERS = &H80000003 strComputer = ""."" Set objReg=GetObject(""winmgmts:{impersonationLevel=impersonate}!\\"" & strComputer & ""\root\default:StdRegProv"") strKeyPath = """" objReg.EnumKey HKEY_USERS, strKeyPath, arrSubKeys strKeyPath = ""\Software\Microsoft\Windows\CurrentVersion\WinTrust\Trust Providers\Software Publishing"" For Each subkey In arrSubKeys objReg.SetDWORDValue HKEY_USERS, subkey & strKeyPath, ""State"", 146944 Next (Code Courtesy of Jeroen Ritmeijer )"
829,234.0,2008-08-03T21:17:33Z,810,4,"I'm guessing that because you want to set it for all users, that you're on some kind of shared computer, which is probably running under a domain? HERE BE DRAGONS Let's say Joe and Jane regularly log onto the computer, then they will each have 'registries'. You'll then install your app, and the installer will employ giant hacks and disgusting things to set items under HKCU for them. THEN, bob will come along and log on (he, and 500 other people have accounts in the domain and so can do this). He's never used this computer before, so he has no registry. The first time he logs in, windows creates him one, but he won't have your setting. Your app then falls over or behaves incorrectly, and bob complains loudly about those crappy products from raynixon incorporated. The correct answer is to just have some default settings in your app, which can write them to the registry if it doesn't find them. It's general good practice that your app should never depend on the registry, and should create things as needed, for any registry entry, not just HKCU, anyway"
870,233.0,2008-08-03T22:34:06Z,810,0,"Despite what the MSDN article says about User/Machine Hive, it doesn't write to HKEY_USERS. Rather it writes to HKCU if you select Just Me and HKLM if you select Everyone. So my solution is going to be to use the User/Machine Hive, and then in the application it checks if the registry entries are in HKCU and if not, copies them from HKLM. I know this probably isn't the most ideal way of doing it, but it has the least amount of changes."
940,243.0,2008-08-04T01:07:14Z,930,15,"Very roughly and from memory since I don't have code on this laptop: using (OleDBConnection conn = new OleDbConnection())
{
  conn.ConnectionString = ""Whatever connection string"";

  using (OleDbCommand cmd = new OleDbCommand())
  {
    cmd.Connection = conn;
    cmd.CommandText = ""Select * from CoolTable"";

    using (OleDbDataReader dr = cmd.ExecuteReader())
    {
      while (dr.Read())
      {
        // do something like Console.WriteLine(dr[""column name""] as String);
      }
    }
  }
}"
951,49.0,2008-08-04T01:31:31Z,930,26,"@Goyuix -- that's excellent for something written from memory.
tested it here -- found the connection wasn't opened. Otherwise very nice. using System.Data.OleDb;
...

using (OleDbConnection conn = new OleDbConnection())
{
    conn.ConnectionString = ""Provider=sqloledb;Data Source=yourServername\\yourInstance;Initial Catalog=databaseName;Integrated Security=SSPI;"";

    using (OleDbCommand cmd = new OleDbCommand())
    {
        conn.Open();
        cmd.Connection = conn;
        cmd.CommandText = ""Select * from yourTable"";

        using (OleDbDataReader dr = cmd.ExecuteReader())
        {
            while (dr.Read())
            {
                Console.WriteLine(dr[""columnName""]);
            }
        }
    }
}"
1022,93.0,2008-08-04T04:45:05Z,930,9,"That's definitely a good way to do it.  But you if you happen to be using a database that supports LINQ to SQL, it can be a lot more fun.  It can look something like this: MyDB db = new MyDB(""Data Source=...""); var q = from db.MyTable select c; foreach (var c in q) Console.WriteLine(c.MyField.ToString());"
1043,55.0,2008-08-04T05:59:53Z,1040,2,"You can use this program, Handle , to find which process has the lock on your file. It's a command-line tool, so I guess you use the output from that... I'm not sure about finding it programmatically. If deleting the file can wait, you could specify it for deletion when your computer next starts up: Start REGEDT32 (W2K) or REGEDIT (WXP) and navigate to: HKEY_LOCAL_MACHINE\System\CurrentControlSet\Control\Session Manager W2K and WXP W2K: Edit Add Value... Data Type: REG_MULTI_SZ Value Name: PendingFileRenameOperations OK WXP: Edit New Multi-String Value enter PendingFileRenameOperations In the Data area, enter ""\??\"" + filename to be deleted. LFNs may
be entered without being embedded in quotes. To delete C:\Long Directory Name\Long File Name.exe , enter the following data: \??\C:\Long Directory Name\Long File Name.exe Then press OK . The ""destination file name"" is a null (zero) string. It is entered
as follows: W2K: Edit Binary select Data Format: Hex click at the end of the hex string enter 0000 (four zeros) OK WXP: Right-click the value choose ""Modify Binary Data"" click at the end of the hex string enter 0000 (four zeros) OK Close REGEDT32/REGEDIT and reboot to delete the file. (Shamelessly stolen from some random forum , for posterity's sake.)"
1044,227.0,2008-08-04T06:01:52Z,1040,32,"Killing other processes is not a healthy thing to do. If your scenario involves something like uninstallation, you could use the MoveFileEx API function to mark the file for deletion upon next reboot. If it appears that you really need to delete a file in use by another process, I'd recommend re-considering the actual problem before considering any solutions."
1049,234.0,2008-08-04T06:12:11Z,1040,5,"If you want to do it programatically. I'm not sure... and I'd really recommend against it.
If you're just troubleshooting stuff on your own machine, SysInternals Process Explorer can help you Run it, use the Find Handle command (I think it's either in the find or handle menu), and search for the name of your file. Once the handle(s) is found, you can forcibly close them. You can then delete the file and so on. Beware , doing this may cause the program which owns the handles to behave strangely, as you've just pulled the proverbial rug out from under it, but it works well when you are debugging your own errant code, or when visual studio / windows explorer is being crap and not releasing file handles even though you told them to close the file ages ago... sigh :-)"
1050,234.0,2008-08-04T06:14:42Z,1040,1,"Oh, one big hack I employed years ago, is that Windows won't let you delete files, but it does let you move them. Pseudo-sort-of-code: mv %WINDIR%\System32\mfc42.dll %WINDIR\System32\mfc42.dll.old
Install new mfc42.dll
Tell user to save work and restart applications When the applications restarted (note we didn't need to reboot the machine), they loaded the new mfc42.dll , and all was well. That, coupled with PendingFileOperations to delete the old one the next time the whole system restarted, worked pretty well."
1051,,2008-08-04T06:15:30Z,1040,11,"The typical method is as follows. You've said you want to do this in C# so here goes... If you don't know which process has the file locked, you'll need to examine each process's handle list, and query each handle to determine if it identifies the locked file. Doing this in C# will likely require P/Invoke or an intermediary C++/CLI to call the native APIs you'll need. Once you've figured out which process(es) have the file locked, you'll need to safely inject a small native DLL into the process (you can also inject a managed DLL, but this is messier, as you then have to start or attach to the .NET runtime). That bootstrap DLL then closes the handle using CloseHandle etc. Essentially: the way to unlock a ""locked"" file is to inject a DLL into the offending process's address space and close it yourself. You can do this using native or managed code. No matter what, you're going to need a small amount of native code or at least P/Invoke into the same. Helpful links: http://www.codeproject.com/KB/threads/winspy.aspx http://damianblog.com/2008/07/02/net-code-injection/ Good luck!"
1074,1.0,2008-08-04T08:04:41Z,1010,7,"Rex, I suspect a good place to start looking is solutions that compress the ViewState -- they're grabbing ViewState on the server before it's sent down to the client and gzipping it. That's exactly where you want to be. Scott Hanselman on ViewState Compression (2005) ViewState Compression with System.IO.Compression (2007)"
1082,116.0,2008-08-04T08:21:07Z,1070,2,"Yes, you are right to lock at the VSZ. ps u will give you the VSZ and RSS, which are the virtual memory size and resident set size.  The RSS is how much physical memory has been allocated to the process, and the VSZ is the virtual memory size of the process.  If you have several copies of a program running, a lot of the memory in the VSZ will be shared between those processes."
1093,51.0,2008-08-04T09:04:17Z,1010,2,"See this blog post where the author describes a method for overriding the default behavior for generating the ViewState and instead shows how to save it on the server Session object. In ASP.NET 2.0, ViewState is saved by
  a descendant of PageStatePersister
  class. This class is an abstract class
  for saving and loading ViewsState and
  there are two implemented descendants
  of this class in .Net Framework, named
  HiddenFieldPageStatePersister and
  SessionPageStatePersister. By default
  HiddenFieldPageStatePersister is used
  to save/load ViewState information,
  but we can easily get the
  SessionPageStatePersister to work and
  save ViewState in Session object. Although I did not test his code, it seems to show exactly what you want: a way to gain access to ViewState code while still on the server, before postback."
1164,91.0,2008-08-04T11:41:37Z,1160,2,"I am currently ""manually"" doing it through a prebuild-exec Task, using my cmdnetsvnrev tool, but if someone knows a better ccnet-integrated way of doing it, i'd be happy to hear :-)"
1168,120.0,2008-08-04T11:51:59Z,1160,3,I found this project on google code. This is CCNET plugin to generate the label in CCNET . The DLL is tested with CCNET 1.3 but it works with CCNET 1.4 for me. I'm successfully using this plugin to label my build. Now onto passing it to MSBuild ...
1169,275.0,2008-08-04T11:56:42Z,1160,13,"You have basically two options. Either you write a simple script that will start and parse output from svn.exe info --revision HEAD to obtain revision number (then generating AssemblyInfo.cs is pretty much straight forward) or just use plugin for CCNET. Here it is: SVN Revision Labeller is a plugin for
  CruiseControl.NET that allows you to
  generate CruiseControl labels for your
  builds, based upon the revision number
  of your Subversion working copy. This
  can be customised with a prefix and/or
  major/minor version numbers. http://code.google.com/p/svnrevisionlabeller/ I prefer the first option because it's only roughly 20 lines of code: using System;
using System.Diagnostics;

namespace SvnRevisionNumberParserSample
{
    class Program
    {
        static void Main()
        {
            Process p = Process.Start(new ProcessStartInfo()
                {
                    FileName = @""C:\Program Files\SlikSvn\bin\svn.exe"", // path to your svn.exe
                    UseShellExecute = false,
                    RedirectStandardOutput = true,
                    Arguments = ""info --revision HEAD"",
                    WorkingDirectory = @""C:\MyProject"" // path to your svn working copy
                });

            // command ""svn.exe info --revision HEAD"" will produce a few lines of output
            p.WaitForExit();

            // our line starts with ""Revision: ""
            while (!p.StandardOutput.EndOfStream)
            {
                string line = p.StandardOutput.ReadLine();
                if (line.StartsWith(""Revision: ""))
                {
                    string revision = line.Substring(""Revision: "".Length);
                    Console.WriteLine(revision); // show revision number on screen                       
                    break;
                }
            }

            Console.Read();
        }
    }
}"
1172,266.0,2008-08-04T12:03:03Z,1160,3,"If you prefer doing it on the MSBuild side over the CCNet config, looks like the MSBuild Community Tasks extension's SvnVersion task might do the trick."
1185,60.0,2008-08-04T12:28:03Z,1180,1,"The trick to that is to use URL rewriting so that name.domain.com transparently maps to something like domain.com/users/name on your server.  Once you start down that path, it's fairly trivial to implement."
1187,275.0,2008-08-04T12:32:24Z,1180,7,"Don't worry about DNS and URL rewriting Your DNS record will be static, something like: *.YOURDOMAIN.COM A 123.123.123.123 Ask your DNS provider to do it for you (if it's not done already) or do it by yourself if you have control over your DNS records. This will automatically point all your subdomains (current and future ones) into the same HTTP server. Once it's done, you will only need to parse HOST header on every single http request to detect what hostname was used to access your server-side scripts on your http server. Assuming you're using ASP.NET, this is kind of silly example I came up with but works and demonstrates simplicity of this approach: <%@ Language=""C#"" %> <% string subDomain = Request.Url.Host.Split('.')[0].ToUpper(); if (subDomain == ""CLIENTXXX"") Response.Write(""Hello CLIENTXXX, your secret number is 33""); else if (subDomain == ""CLIENTYYY"") Response.Write(""Hello CLIENTYYY, your secret number is 44""); else Response.Write(subDomain+"" doesn't exist""); %>"
1216,120.0,2008-08-04T13:22:31Z,1160,2,"Customizing csproj files to autogenerate AssemblyInfo.cs http://www.codeproject.com/KB/dotnet/Customizing_csproj_files.aspx Every time we create a new C# project,
  Visual Studio puts there the
  AssemblyInfo.cs file for us. The file
  defines the assembly meta-data like
  its version, configuration, or
  producer. Found the above technique to auto-gen AssemblyInfo.cs using MSBuild. Will post sample shortly."
1232,48.0,2008-08-04T13:39:39Z,1180,11,"The way we do this is to have a 'catch all' for our domain name registered in DNS so that anything.ourdomain.com will point to our server. With Apache you can set up a similar catch-all for your vhosts.  The ServerName must be a single static name but the ServerAlias directive can contain a pattern. Servername www.ourdomain.com
ServerAlias *.ourdomain.com Now all of the domains will trigger the vhost for our project.  The final part is to decode the domain name actually used so that you can work out the username in your code, something like (PHP): list( $username ) = explode( ""."", $_SERVER[ ""HTTP_HOST"" ] ); or a RewriteRule as already suggested that silently maps user.ourdomain.com/foo/bar to www.ourdomain.com/foo/bar?user=user or whatever you prefer."
1235,173.0,2008-08-04T13:43:54Z,1160,3,"I have written a NAnt build file that handles parsing SVN information and creating properties. I then use those property values for a variety of build tasks, including setting the label on the build. I use this target combined with the SVN Revision Labeller mentioned by lubos hasko with great results. <target name=""svninfo"" description=""get the svn checkout information""> <property name=""svn.infotempfile"" value=""${build.directory}\svninfo.txt"" /> <exec program=""${svn.executable}"" output=""${svn.infotempfile}""> <arg value=""info"" /> </exec> <loadfile file=""${svn.infotempfile}"" property=""svn.info"" /> <delete file=""${svn.infotempfile}"" /> <property name=""match"" value="""" /> <regex pattern=""URL: (?'match'.*)"" input=""${svn.info}"" /> <property name=""svn.info.url"" value=""${match}""/> <regex pattern=""Repository Root: (?'match'.*)"" input=""${svn.info}"" /> <property name=""svn.info.repositoryroot"" value=""${match}""/> <regex pattern=""Revision: (?'match'\d+)"" input=""${svn.info}"" /> <property name=""svn.info.revision"" value=""${match}""/> <regex pattern=""Last Changed Author: (?'match'\w+)"" input=""${svn.info}"" /> <property name=""svn.info.lastchangedauthor"" value=""${match}""/> <echo message=""URL: ${svn.info.url}"" /> <echo message=""Repository Root: ${svn.info.repositoryroot}"" /> <echo message=""Revision: ${svn.info.revision}"" /> <echo message=""Last Changed Author: ${svn.info.lastchangedauthor}"" /> </target>"
1320,312.0,2008-08-04T15:12:37Z,1300,4,"In my opinion it is more a question of personal preference.  nAnt is a great framework and MSBuild is almost as capable.  With the ability to easily develop custom tasks (in both frameworks) you can accomplish almost anything that you need to do. I cannot answer the ""still supported"" portion of your questions, but I would say if you are already comfortable with nAnt then it's probably viable.  If you (or someone in your group) is familiar with MSBuild then that is a fine way to go as well."
1347,17.0,2008-08-04T15:37:31Z,1300,0,"Honestly it depends on what fits in to your environment better.  If you are using a lot of Non-Microsoft tools, nunit, ccnet, ncover.  You will probably find better support with nant.  Alternatively if you are using MSTest, TFSBuild, you will probably find MSBuild a better environment.  I would learn both and use which every fits more smoothly with your environment."
1364,35.0,2008-08-04T15:52:42Z,1300,2,"If you've already got a bunch of custom tasks you use with nAnt, stick with it - you don't gain much with MSBuild.  That said, there doesn't seem to be anything that nAnt can do that MSBuild can't at its core.  Both can call external tools, both can run .Net-based custom tasks, and both have a bunch of community tasks out there. We're using MSBuild here for the same reason you are - it's the default build system for VS now, and we didn't have any nAnt-specific stuff to worry about. The MSBuildCommunityTasks are a good third-party task base to start with, and covers most of the custom stuff I ever did in nAnt, including VSS and Subversion support."
1394,91.0,2008-08-04T16:38:03Z,1390,16,"Not sure how credible this source is , but: The Windows Server 2008 Core edition can: Run the file server role. Run the Hyper-V virtualization server role. Run the Directory Services role. Run the DHCP server role. Run the IIS Web server role. Run the DNS server role. Run Active Directory Lightweight Directory Services. Run the print server role. The Windows Server 2008 Core edition cannot: Run a SQL Server. Run an Exchange Server. Run Internet Explorer. Run Windows Explorer. Host a remote desktop session. Run MMC snap-in consoles locally."
1446,307.0,2008-08-04T17:38:59Z,580,2,"Using SMO/DMO, it isn't too difficult to generate a script of your schema.  Data is a little more fun, but still doable. In general, I take ""Script It"" approach, but you might want to consider something along these lines: Distinguish between Development and Staging, such that you can Develop with a subset of data ... this I would create a tool to simply pull down some production data, or generate fake data where security is concerned. For team development, each change to the database will have to be coordinated amongst your team members.  Schema and data changes can be intermingled, but a single script should enable a given feature.  Once all your features are ready, you bundle these up in a single SQL file and run that against a restore of production. Once your staging has cleared acceptance, you run the single SQL file again on the production machine. I have used the Red Gate tools and they are great tools, but if you can't afford it, building the tools and working this way isn't too far from the ideal."
1464,26.0,2008-08-04T18:00:50Z,580,5,"Like Rob Allen, I use SQL Compare / Data Compare by Redgate. I also use the Database publishing wizard by Microsoft. I also have a console app I wrote in C# that takes a sql script and runs it on a server. This way you can run large scripts with 'GO' commands in it from a command line or in a batch script. I use Microsoft.SqlServer.BatchParser.dll and Microsoft.SqlServer.ConnectionInfo.dll libraries in the console application."
1550,313.0,2008-08-04T19:51:49Z,650,0,"Some time ago I wrote a quick and dirty exe that would update the version #'s in an assemblyinfo.{cs/vb} - I also have used rxfind.exe (a simple and powerful regex-based search replace tool) to do the update from a command line as part of the build process.  A couple of other helpfule hints: separate the assemblyinfo into product parts (company name, version, etc.) and assembly specific parts (assembly name etc.).  See here Also - i use subversion, so I found it helpful to set the build number to subversion revision number thereby making it really easy to always get back to the codebase that generated the assembly (e.g. 1.4.100.1502 was built from revision 1502)."
1617,173.0,2008-08-04T21:46:01Z,1600,6,"It is possible to skip the step of creating the empty database. You can create the new database as part of the restore process. This is actually the easiest and best way I know of to clone a database. You can eliminate errors by scripting the backup and restore process rather than running it through the SQL Server Management Studio There are two other options you could explore: Detach the database, copy the .mdf file and re-attach. Use SQL Server Integration Services (SSIS) to copy all the objects over I suggest sticking with backup and restore and automating if necessary."
1621,224.0,2008-08-04T21:54:18Z,1610,51,"You can not do this programatically (in a safe way that is) without creating a new table. What Enterprise Manager does when you commit a reordering is to create a new table, move the data and then delete the old table and rename the new table to the existing name. If you want your columns in a particular order/grouping without altering their physical order, you can create a view which can be whatever you desire."
1624,192.0,2008-08-04T22:00:53Z,1610,1,"When Management Studio does it, it's creating a temporary table, copying everything across, dropping your original table and renaming the temporary table.  There's no simple equivalent T-SQL statement. If you don't fancy doing that, you could always create a view of the table with the columns in the order you'd like and use that? Edit: beaten!"
1646,225.0,2008-08-04T22:18:05Z,1600,4,"Here's a dynamic sql script I've used in the past.  It can be further modified but it will give you the basics.  I prefer scripting it to avoid the mistakes you can make using the Management Studio: Declare @OldDB varchar(100) Declare @NewDB varchar(100) Declare @vchBackupPath varchar(255) Declare @query varchar(8000) /*Test code to implement Select @OldDB = 'Pubs' Select @NewDB = 'Pubs2' Select @vchBackupPath = '\\dbserver\C$\Program Files\Microsoft SQL Server\MSSQL.1\MSSQL\Backup\pubs.bak' */ SET NOCOUNT ON; Select @query = 'Create Database ' + @NewDB exec(@query) Select @query = ' Declare @vBAKPath varchar(256) declare @oldMDFName varchar(100) declare @oldLDFName varchar(100) declare @newMDFPath varchar(100) declare @newLDFPath varchar(100) declare @restQuery varchar(800) select @vBAKPath = ''' + @vchBackupPath + ''' select @oldLDFName = name from ' + @OldDB +'.dbo.sysfiles where filename like ''%.ldf%'' select @oldMDFName = name from  ' + @OldDB +'.dbo.sysfiles where filename like ''%.mdf%'' select @newMDFPath = physical_name from ' + @NewDB +'.sys.database_files where type_desc = ''ROWS'' select @newLDFPath = physical_name from ' + @NewDB +'.sys.database_files where type_desc = ''LOG'' select @restQuery = ''RESTORE DATABASE ' + @NewDB + ' FROM DISK = N'' + '''''''' + @vBAKpath + '''''''' + '' WITH MOVE N'' + '''''''' + @oldMDFName + '''''''' + '' TO N'' + '''''''' + @newMDFPath + '''''''' + '', MOVE N'' + '''''''' + @oldLDFName + '''''''' + '' TO N'' + '''''''' + @newLDFPath + '''''''' + '', NOUNLOAD, REPLACE, STATS = 10'' exec(@restQuery) --print @restQuery' exec(@query)"
1673,308.0,2008-08-04T22:52:31Z,1300,13,"If you are quite happy with MSBuild, then I would stick with MSBuild.  This may be one of those cases where the tool you learn first is the one you will prefer.  I started with NAnt and can't quite get used to MSBuild.  I'm sure they will both be around for quite some time. There are some fundamental differences between the two, probably best highlighted by this conversation between some NAnt fans and a Microsoftie . Interestingly, Jeremy Miller asked the exact opposite question on his blog last year."
1765,307.0,2008-08-05T00:57:48Z,1760,1,"I like MbUnit , er, Gallio .  Most importantly to me is having good tools support inside Visual Studio.  For that I use Resharper , which has an MbUnit test runner .  A lot of folks seem to like TestDriven.NET as their test runner as well."
1773,164.0,2008-08-05T01:07:50Z,1070,13,"The exact definitions of ""vsize,"" ""rss,"" ""rprvt,"" ""rshrd,"" and other obscure-looking abbreviations vary from OS to OS.  The manual pages for the ""top"" and ""ps"" commands will have some sort of description, but all such descriptions are simplified greatly (or are based on long-extinct kernel implementations).  ""Process size"" as a concept is fiendishly difficult to pin down in the general case.  Answers in specific instances depend heavily on the actual memory management implementation in the OS, and are rarely as satisfying as the tidy ""process size"" concept that exists in the minds of most users (and most developers). For example, none of those numbers (nor, likely, any combination of them) can be used to tell you exactly how many such processes can run at once in a given amount of free memory.  But really, your best bet is to come at it from that end: why do you want this number, and what will you use it for?  Given that information, I think you'll get more useful answers."
1776,313.0,2008-08-05T01:11:30Z,1600,0,"The Publish to Provider functionality has worked great for me.  See Scott Gu's Blog Entry . If you need something really robust look  at redgate software's tools here ...if you are doing much SQL at all, these are worth the $$."
1793,307.0,2008-08-05T01:31:16Z,1600,2,Backup and Restore is the most straight-forward way I know.  You have to be careful between servers as security credentials don't come with the restored database.
1794,117.0,2008-08-05T01:32:34Z,1790,2,Sounds like you might like Git . There's a Google Talk explaining all about it .
1806,292.0,2008-08-05T01:44:13Z,1790,0,"Its probably not exactly what your looking for, but you may be able to implement OS level clustering."
1820,358.0,2008-08-05T02:07:50Z,1760,2,I like TestDriven.NET (even though I use ReSharper) and I'm pretty happy with XUnit.net . It uses Facts instead of Tests which many people dislike but I like the difference in terminology. It's useful to think of a collection of automatically provable Facts about your software and see which ones you violate when you make a change. Be aware that Visual Studio 2008 Professional (and above) now comes with integrated Unit Testing (it used to be available only with the Team System Editions) and may be suitable for your needs.
1821,308.0,2008-08-05T02:15:24Z,1760,47,"There are so many it's crazy.  Crazy good, I guess. For the conservative types (me), NUnit is still available and still more than capable. For the Microsoft-types, MSTest is adequate, but slow and clunky compared to Nunit.  It also lacks code coverage without paying the big bucks for the pricey versions of Visual Studio. There's also MbUnit .  It's like NUnit, but has nifty features like RowTest (run the same test with different parameters) and Rollback (put the database back like you found it after a test) And finally, xUnit.net is the trendy option with some attitude. Oh, and TestDriven.NET will give you IDE integration for both Nunit and MBunit. I'm sure they're all just fine.  I'd steer away from MSTest though, unless you just enjoy the convenience of having everything in one IDE out of the box. Scott Hanselman has a podcast on this very topic."
1971,269.0,2008-08-05T06:43:30Z,1970,3,When I wrote my first trigger it was in Perl because that was the language used in the example I found but the last triggers I wrote where in Python because I'm more comfortable with that. I think programmer comfort is my main reason to choose python but I think Perl is still better supported.
2017,48.0,2008-08-05T08:22:05Z,1790,4,"This is more or less the perfect use case for SVK .  SVK is a command line front end for subversion that works with an entire local copy of the repository.  So your commits, updates, etc. work on the local repository and you can then sync with a master.  I would generally recommend SVK over plain subversion anyway as it makes a lot of things nicer.  No .svn folders, better branching and merging, better conflict resolution."
2019,342.0,2008-08-05T08:29:13Z,330,3,"One way of deciding whether or not to use nested classes is to think whether or not this class plays a supporting role or it's own part. If it exists solely for the purpose of helping another class then I generally make it a nested class. There are a whole load of caveats to that, some of which seem contradictory but it all comes down to experience and gut-feeling."
2023,370.0,2008-08-05T08:37:19Z,330,2,sounds like a case where you could use the strategy pattern
2058,371.0,2008-08-05T10:07:51Z,1010,1,I enabled compression following similar articles to those posted above. The key to accessing the ViewState before the application sends it was overriding this method; protected override void SavePageStateToPersistenceMedium(object viewState) You can call the base method within this override and then add whatever additional logic you require to handle the ViewState.
2254,34.0,2008-08-05T13:10:26Z,2250,5,"in 1.x there used to be things DataTables couldn't do which DataSets could (don't remember exactly what). All that was changed in 2.x.  My guess is that's why a lot of examples still use DataSets.  DataTables should be quicker as they are more lightweight. If you're only pulling a single resultset, its your best choice between the two."
2258,377.0,2008-08-05T13:12:15Z,2250,69,"It really depends on the sort of data you're bringing back.  Since a DataSet is (in effect) just a collection of DataTable objects, you can return multiple distinct sets of data into a single, and therefore more manageable, object. Performance-wise, you're more likely to get inefficiency from unoptimized queries than from the ""wrong"" choice of .NET construct.  At least, that's been my experience."
2264,26.0,2008-08-05T13:15:53Z,2250,3,"One feature of the DataSet is that if you can call multiple select statements in your stored procedures, the DataSet will have one DataTable for each."
2312,307.0,2008-08-05T13:42:15Z,2300,3,"Whatever your [things] are need to be written outside of VBScript. In VB6, you can write a Custom Collection class , then you'll need to compile to an ActiveX DLL and register it on your webserver to access it."
2369,95.0,2008-08-05T14:17:22Z,930,6,"This is an alternative way (DataReader is faster than this one): string s = """"; SqlConnection conn = new SqlConnection(""Server=192.168.1.1;Database=master;Connect Timeout=30;User ID=foobar;Password=raboof;""); SqlDataAdapter da = new SqlDataAdapter(""SELECT TOP 5 name, dbid FROM sysdatabases"", conn); DataTable dt = new DataTable(); da.Fill(dt); for (int i = 0; i < dt.Rows.Count; i++) { s += dt.Rows[i][""name""].ToString() + "" -- "" + dt.Rows[i][""dbid""].ToString() + ""\n""; } MessageBox.Show(s);"
2382,383.0,2008-08-05T14:26:41Z,2120,118,"I have found the solution else where: SELECT SUBSTRING(master.dbo.fn_varbintohexstr(HashBytes('MD5', 'HelloWorld')), 3, 32)"
2531,157.0,2008-08-05T16:24:53Z,2530,1637,"Firefox 30 ignores autocomplete=""off"" for passwords, opting to prompt the user instead whether the password should be stored on the client. Note the following commentary from May 5, 2014: The password manager always prompts if it wants to save a password. Passwords are not saved without permission from the user. We are the third browser to implement this change, after IE and Chrome. According to Mozilla developer documentation the form element attribute autocomplete prevents form data from being cached in older browsers. <input type=""text"" name=""foo"" autocomplete=""off"" />"
2535,269.0,2008-08-05T16:26:26Z,2530,11,"<form autocomplete=""off"" ... was a none standard way to do this (I think mozilla and IE still support it) but messing with the users expectations is normally a bad idea. If the user enters their credit card details into a form and then let's someone else use that browser it's not your concern :)"
2536,1384652.0,2008-08-05T16:27:39Z,2530,6,"Use a non-standard name and id for the fields, so rather than ""name"" have ""name_"". Browsers will then not see it as being the name field. The best part about it is that you can do this to some but not all fields and it will autocomplete some but not all fields."
2537,225.0,2008-08-05T16:27:40Z,2530,77,"<form name=""form1"" id=""form1"" method=""post"" 
      autocomplete=""off"" action=""http://www.example.com/form.cgi""> This will work in IE and FF, the downside is that it is not XHTML standard."
2542,106.0,2008-08-05T16:32:11Z,2530,12,"Just set autocomplete=""off"" . There is a very good reason for doing this: You want to provide your own autocomplete functionality!"
2558,414.0,2008-08-05T16:43:11Z,2540,11,"As always, Google is your friend: http://nixbit.com/cat/programming/libraries/c-generic-library/ specifically: http://nixbit.com/cat/programming/libraries/generic-data-structures-library/"
2560,72.0,2008-08-05T16:44:01Z,2550,19,"Flash is certainly the most ubiquitous and portable solution. 98% of browsers have Flash installed.  Other alternatives are Quicktime , Windows Media Player , or even Silverlight (Microsoft's Flash competitor, which can be used to embed several video formats). I would recommend using Flash (and it's FLV video file format) for embedding your video unless you have very specific requirements as far as video quality or DRM."
2562,91.0,2008-08-05T16:44:43Z,2550,5,"Flash is usually the product of choice: Everyone has it, and using the JW FLV Player makes it relatively easy on your side. As for other Video Formats, there are WMV and QuickTime, but the players are rather ""heavy"", not everyone might have them and they feel so 1990ish... Real Player... Don't let me even start ranting about that pile of ... The only other alternative of Flash that I would personally consider is Silverlight, which allows streaming WMV Videos. I found the production of WMV much better and easier than FLV because all Windows FLV Encoders I tried are not really good and stable, whereas pretty much every tool can natively output WMV. The problem with Silverlight is that no one has that Browser Plugin (yet?). There is also a player from JW ."
2566,46.0,2008-08-05T16:49:31Z,2550,0,"I have worked for a company that developed a system for distributing media content to dedicated ""players"". It was web based and used ASP.NET technology and have tried almost every possible media format you can think of and your choice really comes down to asking yourself: does it needs to play directly out of the box, or can I make sure that the components required to play the videos can be installed beforehand? If your answer is that it needs to play out of the box then really your only option is flash (I know that it is not installed by default, but most will already have it installed) If it is not a big issue that extra components are needed then you can go with formats that are supported by windows media player The reason why windows media player falls into the second option is because for some browsers and some formats extra components must be installed. We had the luxury that the ""players"" were provided by us, so we could go for the second option, however even we tried to convert as much as possible back to flash because it handles way better than windows media player"
2697,199.0,2008-08-05T18:50:09Z,2540,33,The Glib library used on the Gnome project may also be some use. Moreover it is pretty well tested. IBM developer works has a good tutorial on its use: Manage C data using the GLib collections
2699,36.0,2008-08-05T18:53:44Z,2630,0,"While semi-related to your question, it does not entirely fit the Powershell NetCmdlets motif. But I wanted to post it anyhow as I use it daily and it may help others. Simply making shift-control-c key combo into displaying the visual studio command prompt."
2754,92.0,2008-08-05T19:55:52Z,2750,1,"Validation should be captured separately from getters or setters in a validation method.  That way if the validation needs to be reused across multiple components, it is available. When the setter is called, such a validation service should be utilized to sanitize input into the object.  That way you know all information stored in an object is valid at all times. You don't need any kind of validation for the getter, because information on the object is already trusted to be valid. Don't save your validation until a database update!! It is better to fail fast ."
2757,357.0,2008-08-05T19:59:05Z,2750,3,"From the perspective of having the most maintainable code, I think you should do as much validation as you can in the setter of a property. This way you won't be caching or otherwise dealing with invalid data. After all, this is what properties are meant for. If all you have is a bunch of properties like... public string Name
{
    get
    {
        return _name;
    }
    set
    {
        _name = value;
    }
} ... they might as well be fields"
2760,304.0,2008-08-05T19:59:13Z,2750,3,"It depends. Generally, code should fail fast. If the value can be set by multiple points in the code and you validate only on after retrieving the value, the bug appears to be in the code that does the update. If the setters validate the input, you know what code is trying to set invalid values."
2761,91.0,2008-08-05T19:59:39Z,2750,12,"Well, one of the reaons why classes usually contain private members with public getters/setters is exactly because they can verify data. If you have a Number than can be between 1 and 100, i would definitely put something in the setter that validates that and then maybe throw an exception that is being caught by the code. The reason is simple: If you don't do it in the setter, you have to remember that 1 to 100 limitation every time you set it, which leads to duplicated code or when you forget it, it leads to an invalid state. As for performance, i'm with Knuth here: ""We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil."""
2768,198.0,2008-08-05T20:03:46Z,2750,1,"You might wanna check out Domain Driven Design , by Eric Evans. DDD has this notion of  a Specification: ... explicit predicate-like VALUE
  OBJECTS for specialized purposes. A
  SPECIFICATION is a predicate that
  determines if an object does or does
  not satisfy some criteria. I think failing fast is one thing, the other is where to keep the logic for validation. The domain is the right place to keep the logic and I think a Specification Object or a validate method on your Domain objects would be a good place."
2779,289.0,2008-08-05T20:11:45Z,2770,21,You need to handle the System.Windows.Forms.Application.ThreadException event for Windows Forms. This article really helped me: http://bytes.com/forum/thread236199.html .
2789,2089740.0,2008-08-05T20:16:03Z,2780,15,"It's called alpha blending . In psuedocode, assuming the background color (blend) always has 255 alpha. Also assumes alpha is 0-255. alpha=argb.alpha() r = (alpha/255)*argb.r() + (1 - alpha/255)*blend.r() g = (alpha/255)*argb.g() + (1 - alpha/255)*blend.g() b = (alpha/255)*argb.b() + (1 - alpha/255)*blend.b() note: you probably need to be a bit (more) careful about floating-point/int math and rounding issues, depending on language. Cast intermediates accordingly Edited to add: If you don't have a background color with an alpha of 255, the algebra gets alot more complicated. I've done it before and it's a fun exercise left to the reader (if you really need to know, ask another question :). In other words, what color C blends into some background the same as blending A, then blending B. This is sort of like calculating A+B (which isn't the same as B+A)."
2790,308.0,2008-08-05T20:16:26Z,2770,6,"If you're using VB.NET, you can tap into the very convenient ApplicationEvents.vb.  This file comes for free with a VB.NET WinForms project and contains a method for handling unhandled exceptions . To get to this nifty file, it's ""Project Properties >> Application >> Application Events"" If you're not using VB.NET, then yeah, it's handling Application.ThreadException ."
2817,234.0,2008-08-05T20:42:31Z,2770,8,"Currently in my winforms app I have handlers for Application.ThreadException , as above, but also AppDomain.CurrentDomain.UnhandledException Most exceptions arrive via the ThreadException handler, but the AppDomain one has also caught a few in my experience"
2831,234.0,2008-08-05T20:53:00Z,1760,4,"xUnit.net looks like it provides a slightly different approach to N/MB/MS/Unit, which is interesting. In my search for an rspec-like solution (because I LOVE the rspec), I also came across NSpec , which looks a bit wordy, but combined with the NSpec Extensions addon to use C#3 extension methods, it looks pretty nice."
2843,36.0,2008-08-05T20:59:21Z,2840,32,"You can use the Row_Number() function.
Its used as follows: SELECT Row_Number() OVER(ORDER BY UserName) As RowID, UserFirstName, UserLastName
FROM Users From which it will yield a result set with a RowID field which you can use to page between. SELECT * 
FROM 
    ( SELECT Row_Number() OVER(ORDER BY UserName) As RowID, UserFirstName, UserLastName
      FROM Users 
    ) As RowResults
WHERE RowID Between 5 AND 10 etc"
2855,36.0,2008-08-05T21:05:10Z,2840,0,I believe you'd need to perform a separate query to accomplish that unfortionately. I was able to accomplish this at my previous position using some help from this page: Paging in DotNet 2.0 They also have it pulling a row count seperately.
2908,1384652.0,2008-08-05T21:52:44Z,2900,1,"Just to check, if you use just this part you get an error? <?php
include(""../includes/connect.php"");

$query = ""SELECT * from story"";
$result = mysql_query($query) or die(mysql_error()); If so, do you still get an error if you copy and paste one of those Inserts into this page, I am trying to see if it's local to the page or that actual line. Also, can you post a copy of the connection calls (minus passwords), unless the inserts use exactly the same syntax as this example."
2911,271.0,2008-08-05T21:56:20Z,2900,1,"Does the apache user require a password to connect to the database?  If so, then the fact that it says ""using password: NO"" would lead me to believe that the code is trying to connect without a password. If, however, the apache user doesn't require a password, a double-check of the permissions may be a good idea (which you mentioned you already checked).  It may still be beneficial to try executing something like this at a mysql prompt: GRANT ALL PRIVILEGES ON databasename .* to 'apache'@'localhost'; That syntax should be correct. Other than that, I'm just as stumped as you are."
2915,457.0,2008-08-05T22:02:23Z,2900,0,"If indeed you are able to insert using the same connection calls, your problem most likely lies in the user ""apache"" not having SELECT permissions on the database. If you have phpMyAdmin installed you can look at the permissions for the user in the Privileges pane. phpMyAdmin also makes it very easy to modify the permissions. If you only have access to the command line, you can check the permissions from the mysql database. You'll probably need to do something like: GRANT SELECT ON myDatabase.myTable TO 'apache'@'localhost';"
2943,454.0,2008-08-05T22:40:40Z,2900,0,"Just to check, if you use just this part you get an error? If so, do you still get an error if you copy and paste one of those Inserts into this >page, I am trying to see if it's local to the page or that actual line. Also, can you post a copy of the connection calls (minus passwords), unless the inserts >use exactly the same syntax as this example. Here is what is in the connection.php file.  I linked to the file through an include in the same fashion as where I execute the INSERT queries elsewhere in the code. $conn = mysql_connect(""localhost"", ******, ******) or die(""Could not connect"");
mysql_select_db(""adbay_com_-_cms"") or die(""Could not select database""); I will try the working INSERT query in this area to check that out. As to the others posting about the password access.  I did, as stated in my first posting, check permissions.  I used phpMyAdmin to verify that the permissions for the user account I was using were correct.  And if it matters at all, apache@localhost is not the name of the user account that I use to get into the database.  I don't have any user accounts with the name apache in them at all for that matter."
2973,92.0,2008-08-06T00:00:22Z,2970,3,"With a six word character password, he may have been brute forced.  That is more likely than his ftp being intercepted, but it could be that too. Start with a stronger password. (8 characters is still fairly weak) See if this link to an internet security blog is helpful."
2975,204.0,2008-08-06T00:05:34Z,2900,6,"And if it matters at all, apache@localhost is not the name of the user account that I use to get into the database. I don't have any user accounts with the name apache in them at all for that matter. If it is saying 'apache@localhost' the username is not getting passed correctly to the MySQL connection. 'apache' is normally the user that runs the httpd process (at least on Redhat-based systems) and if no username is passed during the connection MySQL uses whomever is calling for the connection. If you do the connection right in your script, not in a called file, do you get the same error?"
2976,204.0,2008-08-06T00:16:07Z,2970,13,"Try and gather as much information as you can. See if the host can give you a log showing all the FTP connections that were made to your account. You can use those to see if it was even an FTP connection that was used to make the change and possibly get an IP address. If you're using a prepacked software like Wordpress, Drupal, or anything else that you didn't code there may be vulnerabilities in upload code that allows for this sort of modification. If it is custom built, double check any places where you allow users to upload files or modify existing files. The second thing would be to take a dump of the site as-is and check everything for other modifications. It may just be one single modification they made, but if they got in via FTP who knows what else is up there. Revert your site back to a known good status and, if need be, upgrade to the latest version. There is a level of return you have to take into account too. Is the damage worth trying to track the person down or is this something where you just live and learn and use stronger passwords?"
2978,419.0,2008-08-06T00:24:23Z,2970,2,"Is the site just plain static HTML? i.e. he hasn't managed to code himself an upload page that permits anyone driving by to upload compromised scripts/pages? Why not ask webhost4life if they have any FTP logs available and report the issue to them. You never know, they may be quite receptive and find out for you exactly what happened? I work for a shared hoster and we always welcome reports such as these and can usually pinpoint the exact vector of attack based and advise as to where the customer went wrong."
3055,116.0,2008-08-06T03:31:22Z,2970,5,"You mention your Dad was using a website publishing tool. If the publishing tool publishes from his computer to the server, it may be the case that his local files are clean, and that he just needs to republish to the server. He should see if there's a different login method to his server than plain FTP, though... that's not very secure because it sends his password as clear-text over the internet."
3129,232.0,2008-08-06T06:39:12Z,2780,1,"if you don't need to know this pre-render, you could always use the win32 method of getpixel, I believe. Note: typing on iPhone in the middle of Missouri with no inet access. Will look up real win32 example and see if there is a .net equivalent. In case anyone cares, and doesn't want to use the (excellent) answer posted above, you can get the color value of a pixel in .Net via this link MSDN example"
3162,342.0,2008-08-06T08:28:14Z,3150,2,I use UnitTest++ . In the years since I made this post the source has moved from SourceForge to github.  Also the example tutorial is now more agnostic - doesn't go into any configuration or project set up at all. I doubt it will still work for Visual Studio 6 as the project files are now created via CMake. If you still need the older version support you can get the last available version under the SourceForge branch.
3188,91.0,2008-08-06T08:52:51Z,3180,35,"I use it, especially since the hosted Version of FugBugz is free for up to 2 people . I found it a lot nicer than paper as I'm working on multiple projects, and my paper tends to get rather messy once you start making annotations or if you want to re-organize and shuffle tasks around, mark them as complete only to see that they are not complete after all... Plus, the Visual Studio integration is really neat, something paper just cannot compete with. Also, if you lay the project to rest for 6 months and come back, all your tasks and notes are still there, whereas with paper you may need to search all the old documents and notes again, if you did not discard it. But that is just the point of view from someone who is not really good at staying organized :-) If you are a really tidy and organized person, paper may work better for you than it does for me. Bonus suggestion: Run Fogbugz on a second PC (or a small Laptop like the eeePC) so that you always have it at your fingertips. The main problem with Task tracking programs - be it FogBugz, Outlook, Excel or just notepad - is that they take up screen space, and my two monitors are usually full with Visual Studio, e-Mail, Web Browsers, some Notepads etc."
3245,267.0,2008-08-06T10:05:25Z,3180,7,"I use it as well and quite frankly wouldn't want to work without it. I've always had some kind of issue tracker available for the projects I work on and thus am quite used to updating it. With FB6 the process is now even better. Since FB also integrates with Subversion, the source control tool I use for my projects, the process is really good and I have two-way links between the two systems now. I can click on a case number in the Subversion logs and go to the case in FB, or see the revisions bound to a case inside FB."
3274,493.0,2008-08-06T10:46:14Z,3180,14,"When I was working for myself doing my consulting business I signed up for a hosted account and honestly I couldn't have done without it. What I liked most about it was it took 30 seconds to sign up for an account and I was then able to integrate source control using sourcegear vault (which is an excellent source control product and free for single developers) set up projects, clients, releases and versions and monitor my progress constantly. One thing that totally blew me away was that I ended up completely abandoning outlook for all work related correspondence. I could manage all my client interactions from within fogbugz and it all just worked amazingly well. In terms of overhead, one of the nice things you could do was turn anything into a case. Anything that came up in your mind while you were coding, you simply created a new email, sent it to fogbugz and it was instantly added as an item for review later. I would strongly recommend you get yourself one of the hosted accounts and give it a whirl"
3288,199.0,2008-08-06T11:12:48Z,3260,8,"You could try signal averaging, i.e. for each point, average the value with the surrounding 3 or more points. If the noise blips are huge, then even this may not help. I realise that this was language agnostic, but guessing that you are using LabView, there are lots of pre-packaged signal processing VIs that come with LabView that you can use to do smoothing and noise reduction. The NI forums are a great place to get more specialised help on this sort of thing."
3289,383.0,2008-08-06T11:17:29Z,3260,0,You could apply some Standard Devision to your logic and take notice of peaks over x%.
3299,488.0,2008-08-06T11:38:05Z,3260,2,"I think you want to cross-correlate your signal with an expected, exemplar signal. But, it has been such a long time since I studied signal processing and even then I didn't take much notice."
3300,279.0,2008-08-06T11:38:37Z,3260,0,"I don't know very much about instrumentation, so this might be totally impractical, but then again it might be a helpful different direction.  If you know how the readings can fail, and there is a certain interval between peaks given such failures, why not do gradient descent at each interval.  If the descent brings you back to an area you've searched before, you can abandon it.  Depending upon the shape of the sampled surface, this also might help you find peaks faster than search."
3310,257.0,2008-08-06T12:10:55Z,3230,0,"You'd be looking to static link (as opposed to dynamically link)

I'm not sure how many of the MS redistributables statically link in."
3405,419.0,2008-08-06T13:57:28Z,3400,2,"The wildcard character in SQL Server is the % sign and it works just fine, leading, trailing or otherwise. That said, if you're going to be doing any kind of serious full text searching then I'd consider utilising the Full Text Index capabilities. Using % and _ wild cards will cause your database to take a serious performance hit."
3413,383.0,2008-08-06T14:00:57Z,3400,0,% Matches any number of characters _ Matches a single character I've never used Full-Text indexing but you can accomplish rather complex and fast search queries with simply using the build in T-SQL string functions.
3422,60.0,2008-08-06T14:03:59Z,3400,3,"One thing worth keeping in mind is that leading wildcard queries come at a significant performance premium, compared to other wildcard usages."
3427,91.0,2008-08-06T14:07:59Z,3400,12,"The problem with leading Wildcards: They cannot be indexed, hence you're doing a full table scan."
3451,186.0,2008-08-06T14:24:29Z,3400,0,Using the '%' character I've searched our database using something like the following: SELECT name FROM TblNames WHERE name LIKE '%overflow' Using this form or query can be slow at times but we only use it for the occasional manual search.
3454,277.0,2008-08-06T14:26:08Z,1970,5,"I have only really used PL/pgSQL, but that was because I needed a few stored procedures relatively fast and didn't want to add extra modules to the server. Longer term, I would probably use PL/Perl or PL/Python, as I use perl for quick scripting and have been looking at python for a while now. One thing I have found is that there is a lack of good documentation for that on the PostgreSQL site. The manuals were thorough as a reference, but did not work well as a tutorial to help show people how it should be done. That, combined with a very good debugging environment, meant that my first experience of writing procedures involved looking at weird syntax errors for a long time. If someone knows of a good site with tutorials etc for PostgreSQL programming, I would love to get a link to it."
3473,419.0,2008-08-06T14:39:09Z,3470,5,"You should take a look at the UNPIVOT clause. Update1 : GateKiller, strangely enough I read an article (about something unrelated) about it this morning and I'm trying to jog my memory where I saw it again, had some decent looking examples too. It'll come back to me I'm sure. Update2 : Found it: http://weblogs.sqlteam.com/jeffs/archive/2008/04/23/unpivot.aspx"
3475,91.0,2008-08-06T14:39:17Z,3470,0,UNION should be your friend: SELECT Column1 FROM table WHERE idColumn = 1 UNION ALL SELECT Column2 FROM table WHERE idColumn = 1 UNION ALL SELECT Column3 FROM table WHERE idColumn = 1 but it can also be your foe on large result sets.
3478,507.0,2008-08-06T14:41:29Z,3470,0,"If you have a fixed set of columns and you know what they are, you can basically do a series of subselects (SELECT Column1 AS ResultA FROM TableA) as R1 and join the subselects. All this in a single query."
3511,91.0,2008-08-06T14:57:01Z,3510,21,"http://www.easymaths.com/What_on_earth_is_Bodmas.htm : What do you think the answer to 2 + 3 x 5 is? Is it (2 + 3) x 5 = 5 x 5 = 25 ? or 2 + (3 x 5) = 2 + 15 = 17 ? BODMAS can come to the rescue and give us rules to follow so that we always get the right answer: (B)rackets (O)rder (D)ivision (M)ultiplication (A)ddition (S)ubtraction According to BODMAS, multiplication should always be done before addition, therefore 17 is actually the correct answer according to BODMAS and will also be the answer which your calculator will give if you type in 2 + 3 x 5 . Why it is useful in programming? No idea, but i assume it's because you can get rid of some brackets? I am a quite defensive programmer, so my lines can look like this: result = (((i + 4) - (a + b)) * MAGIC_NUMBER) - ANOTHER_MAGIC_NUMBER; with BODMAS you can make this a bit clearer: result = (i + 4 - (a + b)) * MAGIC_NUMBER - ANOTHER_MAGIC_NUMBER; I think i'd still use the first variant - more brackets, but that way i do not have to learn yet another rule and i run into less risk of forgetting it and causing those weird hard to debug errors? Just guessing at that part though. Mike Stone EDIT: Fixed math as Gaius points out"
3513,48.0,2008-08-06T14:59:36Z,3470,0,"I'm not sure of the SQL Server syntax for this but in MySQL I would do SELECT IDColumn, ( IF( Column1 >= 3, 1, 0 ) + IF( Column2 >= 3, 1, 0 ) + IF( Column3 >= 3, 1, 0 ) + ... [snip ] )
  AS NumberOfColumnsGreaterThanThree
FROM TableA; EDIT: A very (very) brief Google search tells me that the CASE statement does what I am doing with the IF statement in MySQL.  You may or may not get use out of the Google result I found FURTHER EDIT: I should also point out that this isn't an answer to your question but an alternative solution to your actual problem."
3515,448.0,2008-08-06T15:01:15Z,3510,8,"Another version of this (in middle school) was ""Please Excuse My Dear Aunt Sally"". Parentheses Exponents Multiplication Division Addition Subtraction The mnemonic device was helpful in school, and still useful in programming today."
3518,437.0,2008-08-06T15:03:35Z,3510,5,"Order of operations in an expression, such as: foo * (bar + baz^2 / foo) B rackets first O rders (ie Powers and Square Roots, etc.) D ivision and M ultiplication (left-to-right) A ddition and S ubtraction (left-to-right) source: http://www.mathsisfun.com/operation-order-bodmas.html"
3521,519.0,2008-08-06T15:08:11Z,3400,3,"From SQL Server Books Online: To write full-text queries in Microsoft SQL Server 2005, you must learn how to use the CONTAINS and FREETEXT Transact-SQL predicates, and the CONTAINSTABLE and FREETEXTTABLE rowset-valued functions. That means all of the queries written above with the % and _ are not valid full text queries. Here is a sample of what a query looks like when calling the CONTAINSTABLE function. SELECT RANK , * FROM TableName , CONTAINSTABLE (TableName, , ' "" WildCard"" ') searchTable WHERE [KEY] = TableName.pk ORDER BY searchTable.RANK DESC In order for the CONTAINSTABLE function to know that I'm using a wildcard search, I have to wrap it in double quotes. I can use the wildcard character * at the beginning or ending. There are a lot of other things you can do when you're building the search string for the CONTAINSTABLE function. You can search for a word near another word, search for inflectional words (drive = drives, drove, driving, and driven), and search for synonym of another word (metal can have synonyms such as aluminum and steel). I just created a table, put a full text index on the table and did a couple of test searches and didn't have a problem, so wildcard searching works as intended. [Update] I see that you've updated your question and know that you need to use one of the functions. You can still search with the wildcard at the beginning, but if the word is not a full word following the wildcard, you have to add another wildcard at the end. Example:  ""*ildcar"" will look for a single word as long as it ends with ""ildcar"". Example:  ""*ildcar*"" will look for a single word with ""ildcar"" in the middle, which means it will match ""wildcard"".  [Just noticed that Markdown removed the wildcard characters from the beginning and ending of my quoted string here.] [Update #2] Dave Ward - Using a wildcard with one of the functions shouldn't be a huge perf hit. If I created a search string with just ""*"", it will not return all rows, in my test case, it returned 0 records."
3523,443.0,2008-08-06T15:08:52Z,3180,12,"In addition to the benefits already mentioned, another nice feature of using FogBugz is BugzScout, which you can use to report errors from your app and log them into FogBugz automatically.  If you're a one person team, chances are there are some bugs in your code you've never seen during your own testing, so it's nice to have those bugs found ""in the wild"" automatically reported and logged for you."
3525,37.0,2008-08-06T15:11:01Z,3510,4,When I learned this in grade school (in Canada) it was referred to as BEDMAS: B rackets E xponents D ivision M ultiplication A ddition S ubtraction Just for those from this part of the world...
3527,121.0,2008-08-06T15:12:30Z,3510,1,"I'm not really sure how applicable to programming the old BODMAS mnemonic is anyways.  There is no guarantee on order of operations between languages, and while many keep the standard operations in that order, not all do.  And then there are some languages where order of operations isn't really all that meaningful (Lisp dialects, for example).  In a way, you're probably better off for programming if you forget the standard order and either use parentheses for everything(eg (a*b) + c) or specifically learn the order for each language you work in."
3533,26.0,2008-08-06T15:16:32Z,3470,1,"I had to do this for a project before. One of the major difficulties I had was explaining what I was trying to do to other people. I spent a ton of time trying to do this in SQL, but I found the pivot function woefully inadequate. I do not remember the exact reason why it was, but it is too simplistic for most applications, and it isn't full implemented in MS SQL 2000. I wound up writing a pivot function in .NET. I'll post it here in hopes it helps someone, someday. ''' <summary> ''' Pivots a data table from rows to columns ''' </summary> ''' <param name=""dtOriginal"">The data table to be transformed</param> ''' <param name=""strKeyColumn"">The name of the column that identifies each row</param> ''' <param name=""strNameColumn"">The name of the column with the values to be transformed from rows to columns</param> ''' <param name=""strValueColumn"">The name of the column with the values to pivot into the new columns</param> ''' <returns>The transformed data table</returns> ''' <remarks></remarks> Public Shared Function PivotTable(ByVal dtOriginal As DataTable, ByVal strKeyColumn As String, ByVal strNameColumn As String, ByVal strValueColumn As String) As DataTable Dim dtReturn As DataTable Dim drReturn As DataRow Dim strLastKey As String = String.Empty Dim blnFirstRow As Boolean = True ' copy the original data table and remove the name and value columns dtReturn = dtOriginal.Clone dtReturn.Columns.Remove(strNameColumn) dtReturn.Columns.Remove(strValueColumn) ' create a new row for the new data table drReturn = dtReturn.NewRow ' Fill the new data table with data from the original table For Each drOriginal As DataRow In dtOriginal.Rows ' Determine if a new row needs to be started If drOriginal(strKeyColumn).ToString <> strLastKey Then ' If this is not the first row, the previous row needs to be added to the new data table If Not blnFirstRow Then dtReturn.Rows.Add(drReturn) End If blnFirstRow = False drReturn = dtReturn.NewRow ' Add all non-pivot column values to the new row For Each dcOriginal As DataColumn In dtOriginal.Columns If dcOriginal.ColumnName <> strNameColumn AndAlso dcOriginal.ColumnName <> strValueColumn Then drReturn(dcOriginal.ColumnName.ToLower) = drOriginal(dcOriginal.ColumnName.ToLower) End If Next strLastKey = drOriginal(strKeyColumn).ToString End If ' Add new columns if needed and then assign the pivot values to the proper column If Not dtReturn.Columns.Contains(drOriginal(strNameColumn).ToString) Then dtReturn.Columns.Add(drOriginal(strNameColumn).ToString, drOriginal(strValueColumn).GetType) End If drReturn(drOriginal(strNameColumn).ToString) = drOriginal(strValueColumn) Next ' Add the final row to the new data table dtReturn.Rows.Add(drReturn) ' Return the transformed data table Return dtReturn End Function"
3543,399.0,2008-08-06T15:24:00Z,3530,37,"from timocracy.com : require 'rake'
require 'rake/rdoctask'
require 'rake/testtask'
require 'tasks/rails'

def capture_stdout
  s = StringIO.new
  oldstdout = $stdout
  $stdout = s
  yield
  s.string
ensure
  $stdout = oldstdout
end

Rake.application.rake_require '../../lib/tasks/metric_fetcher'
results = capture_stdout {Rake.application['metric_fetcher'].invoke}"
3571,519.0,2008-08-06T15:42:19Z,1610,3,"If I understand your question, you want to affect what columns are returned first, second, third, etc in existing queries, right? If all of your queries are written with SELECT * FROM TABLE - then they will show up in the output as they are layed out in SQL. If your queries are written with SELECT Field1, Field2 FROM TABLE - then the order they are layed out in SQL does not matter."
3581,91.0,2008-08-06T15:46:49Z,3180,18,"Go to http://www.fogbugz.com/ then at the bottom under ""Try It"", sign up. under Settings => Your FogBugz Hosted Account, it should either already say ""Payment Information:    Using Student and Startup Edition."" or there should be some option/link to turn on the Student and Startup Edition. And yes, it's not only for Students and Startups, I asked their support :-) Disclaimer: I'm not affiliated with FogCreek and Joel did not just deposit money in my account."
3603,536.0,2008-08-06T16:07:06Z,1760,7,"We use NUnit and MBUnit here. We use TestDriven.NET to run the unit tests from within Visual Studio. We use the excellent, highly recommended RhinoMocks as a mock framework."
3637,541.0,2008-08-06T16:28:19Z,260,5,"If you don't want to use the DLR you can use Boo (which has an interpreter) or you could consider the Script.NET (S#) project on CodePlex . With the Boo solution you can choose between compiled scripts or using the interpreter, and Boo makes a nice scripting language, has a flexible syntax and an extensible language via its open compiler architecture. Script.NET looks nice too, though, and you could easily extend that language as well as its an open source project and uses a very friendly Compiler Generator ( Irony.net )."
3833,35.0,2008-08-06T18:44:55Z,3790,1,"You didn't mention for what OS, but the WMI Redistributable Components version 1.0 definitely exists. For Windows Server 2003, the WMI SDK and redistributables are part of the Server SDK I believe that the same is true for the Server 2008 SDK"
3979,571.0,2008-08-06T21:03:00Z,1760,1,"I used to use NUnit, but I switched to MbUnit since it has more features.  I love RowTest.  It lets you parametrize your tests.  NUnit does have a litter bit better tool support though.  I am using ReSharper to run MbUnit Tests.  I've had problems with TestDriven.NET running my SetUp methods for MbUnit."
4117,571.0,2008-08-06T23:29:21Z,4110,0,"If you are talking about MS SQL Server tables, I like the diagram support in SQL Server Management Studio. You just drag the tables from the explorer onto the canvas, and they are laid out for you along with lines for relationships. You'll have to do some adjusting by hand for the best looking diagrams, but it is a decent way to get diagrams."
4126,172.0,2008-08-06T23:36:58Z,4110,2,"I am a big fan of Embarcadero's ER/Studio .  It is very powerful and produces excellent on-screen as well as printed results.  They have a free trial as well, so you should be able to get in and give it a shot without too much strife. Good luck!"
4129,116.0,2008-08-06T23:40:47Z,4110,2,"Toad Data Modeller from Quest does a nice job on this and is reasonably priced.  Embarcadero E/R studio is good too, as Bruce mentioned."
4155,419.0,2008-08-07T00:08:31Z,4110,1,"OP asked about diagramming views and view dependencies, SQL Management Studio and Enterprise Manager doesn't allow you to diagram views. I can't vouch for the other tools. The LINQ to SQL designer for Visual Studio does allow you to drop views on the design surface but there isn't a easy way to model the dependencies between the views. I'm not sure which tool has this type of diagramming functionality. You could take a look at Red Gate's SQLDoc tool but it just provides text based output."
4166,527.0,2008-08-07T00:20:41Z,4080,1,We use FindBugs and JDepend integrated with Ant. We use JUnit but we're not using any coverage tool. I'm not using it integrated to Rational Application Developer (the IDE I'm using to develop J2EE applications) because I like how neat it looks when you run javac in the Windows console. :P
4204,34.0,2008-08-07T00:37:04Z,4170,5,"There are three key components (assuming ur using SQL server): SQLConnection SqlCommand SqlDataReader (if you're using something else, replace Sql with ""Something"", like MySqlConnection , OracleCommand ) Everything else is just built on top of that. Example 1: using (SqlConnection connection = new SqlConnection(""CONNECTION STRING""))
using (SqlCommand command = new SqlCommand())
{
  command.commandText = ""SELECT Name FROM Users WHERE Status = @OnlineStatus"";
  command.Connection = connection;
  command.Parameters.Add(""@OnlineStatus"", SqlDbType.Int).Value = 1; //replace with enum
  connection.Open();

  using (SqlDataReader dr = command.ExecuteReader))
  {
      List<string> onlineUsers = new List<string>();

      while (dr.Read())
      {
         onlineUsers.Add(dr.GetString(0));
      }
  }
} Example 2: using (SqlConnection connection = new SqlConnection(""CONNECTION STRING""))
using (SqlCommand command = new SqlCommand())
{
  command.commandText = ""DELETE FROM Users where Email = @Email"";
  command.Connection = connection;
  command.Parameters.Add(""@Email"", SqlDbType.VarChar, 100).Value = ""user@host.com"";
  connection.Open();
  command.ExecuteNonQuery();
}"
4243,423.0,2008-08-07T01:05:56Z,4230,7,The DataGrid was originally in .NET 1.0.  The GridView was introduced (and replaced the DataGrid) in .NET 2.0.  They provide nearly identical functionality.
4244,419.0,2008-08-07T01:06:22Z,4230,38,"DataGrid was an ASP.NET 1.1 control, still supported. GridView arrived in 2.0, made certain tasks simpler added different databinding features: This link has a comparison of DataGrid and GridView features - https://msdn.microsoft.com/en-us/library/05yye6k9(v=vs.100).aspx"
4327,17.0,2008-08-07T02:46:16Z,4320,2,Like everything else it is environmental and depends on the use of the system.  The question you need to ask your self is: Will this be actively developed Is this going to be used over the course of many years and expanded on Is the expansion of the application unknown and thus infinite Really it comes down to laziness.  How much time to do you want to spend reworking the system from the UI?  Because having no business layer means duplication of rules in your UI across possibility many many pages. Then again if this is a proof of concept or short demo or class project.  Take the easy way out.
4331,493.0,2008-08-07T02:48:48Z,4320,4,"It's acceptable as long as you understand the consequences. The main reason you'd have a BLL is to re-use that logic elsewhere throughout your application. If you have all that validation logic in the presentation code, you're really making it difficult to re-use elsewhere within your application."
4334,598.0,2008-08-07T02:49:53Z,4320,2,"Acceptable? Depends who you ask and what your requirements are. Is this app an internal one-off used by you and a few other people? Maybe this is good enough. If it's meant to be a production ready enterprise application that will grow and be maintained over the years, then you probably want to invest more effort up-front to build a maintainable app. Separation of Concerns is a key design technique for building maintainable apps. By mixing presentation, business, and data access logic all together, you can end up with a very fragile difficult to change application architecture."
4472,259.0,2008-08-07T06:40:28Z,3230,0,"If you are looking to find out which dll's your target machine is missing then use depends.exe which used to come with MSDev, but can also be found here . Testing this on a few target machines should tell you which dll's you need to package with your application."
4491,380.0,2008-08-07T07:17:55Z,4430,2,"Well, those features are specific to a tool that you are using for development in those languages. You wouldn't have those tools if (for example) you were using notepad to write code. So, maybe you should ask the question for the tool you are using. For PHP: http://webservices.xml.com/pub/a/ws/2004/03/24/phpws.html"
4492,122.0,2008-08-07T07:18:26Z,4080,4,"Checkstyle is another one I've used at a previous company... it's mainly for style checking, but it can do some static analysis too.  Also, Clover for code coverage, though be aware it is not a free tool."
4493,198.0,2008-08-07T07:21:49Z,4080,3,"We are using FindBugs and Checkstyle as well as Clover for Code Coverage. I think it's important to have some kind of static analysis, supporting your development. Unfortunately it's still not widely spread that these tools are important."
4614,489.0,2008-08-07T12:03:42Z,4610,0,"You need to create an extension method, which requires .NET 3.5. The method needs to be static, in a static class. The first parameter of the method needs to be prefixed with ""this"" in the signature. public static string MyMethod(this string input) { // do things } You can then call it like ""asdfas"".MyMethod();"
4615,267.0,2008-08-07T12:04:11Z,4610,18,"You can't dynamically add methods to existing objects or classes in .NET, except by changing the source for that class. You can, however, in C# 3.0, use extension methods, which look like new methods, but are compile-time magic. To do this for your code: public static class StringExtensions { public static String trim(this String s) { return s.Trim(); } } To use it: String s = ""  Test  ""; s = s.trim(); This looks like a new method, but will compile the exact same way as this code: String s = ""  Test  ""; s = StringExtensions.trim(s); What exactly are you trying to accomplish? Perhaps there are better ways of doing what you want?"
4616,608.0,2008-08-07T12:04:14Z,4610,0,Using the 3.5 compiler you can use an Extension Method: public static void Trim(this string s) { // implementation } You can use this on a CLR 2.0 targeted project (3.5 compiler) by including this hack: namespace System.Runtime.CompilerServices { [AttributeUsage(AttributeTargets.Method | AttributeTargets.Class | AttributeTargets.Assembly)] public sealed class ExtensionAttribute : Attribute { } }
4619,615.0,2008-08-07T12:09:02Z,4610,4,"It sounds like you're talking about C#'s Extension Methods. You add functionality to existing classes by inserting the ""this"" keyword before the first parameter. The method has to be a static method in a static class. Strings in .NET already have a ""Trim"" method, so I'll use another example. public static class MyStringEtensions
{
    public static bool ContainsMabster(this string s)
    {
        return s.Contains(""Mabster"");
    }
} So now every string has a tremendously useful ContainsMabster method, which I can use like this: if (""Why hello there, Mabster!"".ContainsMabster()) { /* ... */ } Note that you can also add extension methods to interfaces (eg IList), which means that any class implementing that interface will also pick up that new method. Any extra parameters you declare in the extension method (after the first ""this"" parameter) are treated as normal parameters."
4682,287.0,2008-08-07T13:05:01Z,4670,1,"EDIT: Perhaps add a ""dvcs"", ""distrubutedversioncontrol"", ""distrubuted"" I've used Mercurial on Windows with no problems. You can use TortoiseHG or just use the command line. Mercurial does require Python, but that is easy to install in Windows as well. Mercurial Binary Packages"
4692,615.0,2008-08-07T13:11:06Z,2630,2,"I wrote a PowerShell provider to give me access to IE7 's RSS feed store, and had lots of fun with it. It lets me cd to a drive called feed: and navigate around folders and feeds using cd and dir. It even lets you add or remove feeds from the command line. See this post on my blog as an example: Getting the Most Prolific Authors in your Feeds It's rolled up into the PowerShell Community Extensions project nowadays, which you can find on CodePlex here ."
4696,309844.0,2008-08-07T13:13:37Z,4670,4,"I use msys-git on windows every single day. Works fast and flawlessly. Although the newer build has some problems with git-svn, this build (Git-1.5.5-preview20080413.exe) has a working git-svn."
4883,55.0,2008-08-07T15:57:16Z,4870,1,"Without doing detailed analysis, I'd guess that it's faster because of the question marks. These allow the regular expression to be ""lazy,"" and stop as soon as they have enough to match, rather than checking if the rest of the input matches. I'm not entirely happy with this answer though, because this mostly applies to question marks after * or +. If I were more familiar with the input, it might make more sense to me. (Also, for the code formatting, you can select all of your code and press Ctrl + K to have it add the four spaces required.)"
4885,271.0,2008-08-07T15:58:31Z,4880,4,"I agree with you in that it is quite an annoyance to me (I also copy and paste my address into the second input). That being said, for less savvy users, it is probably a good idea. Watching my mother type is affirmation that many users do not look at the screen when they type (when she's using her laptop she resembles Linus from Peanuts when he's playing the piano).  If it's important for you to have the user's correct email address then I would say having a confirmation input is a very good idea (one of these days I'll probably type my email address wrong in the first box and paste it wrong into the second box and then feel like a complete idiot)."
4886,106.0,2008-08-07T16:00:04Z,4880,12,"I would just use one input box. The ""Confirm"" input is a remnant form the ""Confirm Password"" method. With passwords, this is useful because they are usually typed as little circles. So, you can't just look at it to make sure that you typed it correctly. With a regular text box, you can visually check your input. So, there is no need for a confirmation input box."
4888,312.0,2008-08-07T16:02:00Z,4880,3,"I agree with Justin, while most technical folks will use the copy, paste method, for the less savvy users it is a good practice. One more thing that I would add is that the second field should have the auto-complete feature disabled.  This ensures that there is human input from either method on at least one of the fields."
4903,199.0,2008-08-07T16:16:37Z,4880,1,"I'd say that this is ok but should only be reserved for forms where the email is essential. If you mistype your email for your flight booking then you have severed the two-way link between yourself and the other party and risk not getting the confirmation number, here on StackOverflow it would only mean your Gravatar would not be loaded ... I'd consider myself fairly techie but I always fill in both fields /wo cut-paste if I regard it to be important enough."
4915,309844.0,2008-08-07T16:24:02Z,4870,3,"The reason why #1 is slower is that [\d;]+ is a greedy quantifier. Using +? or *? is going to do lazy quantifing. See MSDN - Quantifiers for more info. You may want to try: ""(\e\[(\d{1,2};)*?[mz]?)?"" That may be faster for you."
4929,673.0,2008-08-07T16:34:49Z,4880,2,"As long as a field is viewable, you do not need a confirm box. As long as you do some form validation to be sure that  it is at least in valid format for an email address let the user manage the rest of the issues."
4931,666.0,2008-08-07T16:36:19Z,4880,15,"I've seen plenty of people type their email address wrong and I've also looked through user databases full of invalid email address. The way I see it you've got two options.  Use a second box to confirm the input, or send an authentication/activation email. Both are annoyances so you get to choose which you think will annoy your users less. Most would argue that having to find an email and click on a link is more annoying, but it avoids the copy/paste a bad address issue, and it allows you to do things like delete or roll back users if they don't activate after say 48 hours."
5026,315.0,2008-08-07T17:38:05Z,4850,10,"Within your overridden ProcessCmdKey how are you determining which key has been pressed? The value of keyData (the second parameter) will change dependant on the key pressed and any modifier keys, so, for example, pressing the left arrow will return code 37, shift-left will return 65573, ctrl-left 131109 and alt-left 262181. You can extract the modifiers and the key pressed by ANDing with appropriate enum values: protected override bool ProcessCmdKey(ref Message msg, Keys keyData)
{
    bool shiftPressed = (keyData & Keys.Shift) != 0;
    Keys unmodifiedKey = (keyData & Keys.KeyCode);

    // rest of code goes here
}"
5042,235.0,2008-08-07T17:53:38Z,4630,5,"I'm not sure if this would be handled exactly the same way for webstart, but we ran into this situation in a desktop application when dealing with a set of native libraries (dlls in our case). Loading libA before libB should work, unless one of those libraries has a dependency that is unaccounted for and not in the path. My understanding is that once it gets to a system loadLibrary call (i.e. Java has found the library in its java.library.path and is now telling the OS to load it) - it is completely dependent on the operating system to find any dependent libraries, because at that point it is the operating system that is loading the library for the process, and the OS only knows how to look in the system path. That seems hard to set in the case of a Webstart app, but there is a way around this that does not involve static compiling. You may be able to shuffle where your libraries are - I am unsure If you use a custom classloader, you can override loadLibrary and findLibrary so that it can locate your libraries from within a jar in your classpath, and if you also make it aware of your native library dependencies (i.e. libB depends on libA depends on libX, then when loading libB you can catch yourself and ensure you load libA first, and in checking that notice and load libX first. Then the OS doesn't try to find a library that isn't in your path. It's klunky and a bit painful, but ensuring Java finds them and loads them all in the correct order can work."
5184,225.0,2008-08-07T19:17:11Z,5170,16,TOAD for MS SQL looks pretty good.  I've never used it personally but I have used Quest's other products and they're solid.
5201,162.0,2008-08-07T19:25:57Z,5170,4,There is an express version on SSMS that has considerably fewer features but still has the basics.
5270,667.0,2008-08-07T20:31:12Z,5170,39,"I've started using LinqPad . In addition to being more lightweight than SSMS, you can also practice writing LINQ queries- way more fun than boring old TSQL!"
5303,122.0,2008-08-07T20:57:40Z,5260,2,"You should probably write some automated tests for each condition you can think of, and then just start brainstorming more, writing the tests as you think of them.  This way, you can see for sure it will work, and will continue to work if you make further changes.  Look up Test Driven Development if you like the results."
5321,,2008-08-07T21:11:52Z,3230,10,"You need to set the run-time library (Under C/C++ -> Code Generation) for ALL projects to static linkage, which correlates to the following default building configurations: Multithreaded Debug/Release Singlethreaded Debug/Release As opposed to the ""DLL"" versions of those libraries. Even if you do that, depending on the libraries you're using, you might have to install a Merge Module/framework/etc. It depends on whether static LIB versions of your dependencies are available."
5334,414.0,2008-08-07T21:20:33Z,5260,2,"Okay, how about these? The difference between the approaches should speak for themselves. Also, this is tested about as far as I can throw it. The warranty lasts until... now. Hope it helps! Module Module1

    Public Function IsInBusinessHours(ByVal d As Date) As Boolean
        Return Not (d.Hour < 8 OrElse d.Hour > 17 OrElse d.DayOfWeek = DayOfWeek.Saturday OrElse d.DayOfWeek = DayOfWeek.Sunday)
    End Function


    Public Function AddInBusinessHours(ByVal fromDate As Date, ByVal hours As Integer) As Date
        Dim work As Date = fromDate.AddHours(hours)
        While Not IsInBusinessHours(work)
            work = work.AddHours(1)
        End While
        Return work
    End Function


    Public Function LoopInBusinessHours(ByVal fromDate As Date, ByVal hours As Integer) As Date
        Dim work As Date = fromDate
        While hours > 0
            While hours > 0 AndAlso IsInBusinessHours(work)
                work = work.AddHours(1)
                hours -= 1
            End While
            While Not IsInBusinessHours(work)
                work = work.AddHours(1)
            End While
        End While
        Return work
    End Function

    Sub Main()
        Dim test As Date = New Date(2008, 8, 8, 15, 0, 0)
        Dim hours As Integer = 5
        Console.WriteLine(""Date: "" + test.ToString() + "", "" + hours.ToString())
        Console.WriteLine(""Just skipping: "" + AddInBusinessHours(test, hours))
        Console.WriteLine(""Looping: "" + LoopInBusinessHours(test, hours))
        Console.ReadLine()
    End Sub

End Module"
5413,615.0,2008-08-07T22:24:01Z,2750,1,"I like to implement IDataErrorInfo and put my validation logic in its Error and this[columnName] properties. That way if you want to check programmatically whether there's an error you can simply test either of those properties in code, or you can hand the validation off to the data binding in Web Forms, Windows Forms or WPF. WPF's ""ValidatesOnDataError"" Binding property makes this particularly easy."
5448,71.0,2008-08-07T22:49:55Z,2970,14,"I know this is a little late in the game, but the URL mentioned for the JavaScript is mentioned in a list of sites known to have been part of the ASPRox bot resurgence that started up in June (at least that's when we were getting flagged with it). Some details about it are mentioned below: http://www.bloombit.com/Articles/2008/05/ASCII-Encoded-Binary-String-Automated-SQL-Injection.aspx The nasty thing about this is that effectively every varchar type field in the database is ""infected"" to spit out a reference to this URL, in which the the browser gets an tiny iframe that turns it into a bot. A basic SQL fix for this can be found here: http://aspadvice.com/blogs/programming_shorts/archive/2008/06/27/Asprox-Recovery.aspx The scary thing though is that the virus looks to the system tables for values to infect and a lot of shared hosting plans also share the database space for their clients. So most likely it wasn't even your dad's site that was infected, but somebody else's site within his hosting cluster that wrote some poor code and opened the door to SQL Injection attack. If he hasn't done so yet, I'd send an URGENT e-mail to their host and give them a link to that SQL code to fix the entire system. You can fix your own affected database tables, but most likely the bots that are doing the infection are going to pass right through that hole again and infect the whole lot. Hopefully this gives you some more info to work with. EDIT: One more quick thought, if he's using one of the hosts online design tools for building his website, all of that content is probably sitting in a column and was infected that way."
5462,521.0,2008-08-07T23:08:54Z,5460,9,"I've done quite a few projects using Community Server. If you're okay with the out-of-the-box functionality, or you don't mind sticking to the version you start with, I think you'll be very happy. The times I've run into headaches using CS is when the client wants functionality CS does not provide, but also insists on keeping the ability to upgrade to the latest version whenever Telligent releases an update. You can mostly support that by making all of your changes either in a separate project or by only modifying aspx/ascx files (no codebehinds). Some kind of merge is going to be required though no matter how well you plan it out."
5465,615.0,2008-08-07T23:13:21Z,5460,7,"Community Server itself has been very solid for me, but if all you need is a blogging engine then it may be overkill. Skinning it, for example, is quite a bit of work (despite their quite powerful Chameleon theme engine). I'd probably look closer at one of the dedicated blog engines out there, like BlogEngine.NET, dasBlog or SubText, if that's all you need. Go with Community Server if you think you'll want more ""community-focused"" features like forums etc."
5491,697.0,2008-08-07T23:47:57Z,5170,4,"If you are already spending time in Visual Studio, then you can always use the Server Explorer to connect to any .Net compliant database server. Provided you're using Professional or greater, you can create and edit tables and databases, run queries, etc."
5492,202.0,2008-08-07T23:49:13Z,5460,0,Have you had a look at the Shared Source blog module for Sitecore?
5498,49.0,2008-08-08T00:02:35Z,2630,8,"there's an out-twitter script i use for posting to twitter. it's nice, as it means you can send something to twitter without the risk of being distracted by a browser. i added an alias for it, ""twit"". so now you can type, for example: PS C:\>""trying out stack overflow"" | twit and if successfully lodged, it will return an integer that identifies your post."
5501,49.0,2008-08-08T00:07:08Z,2750,4,"@Terrapin, re: If all you have is a bunch of [simple
  public set/get] properties ... they
  might as well be fields Properties have other advantages over fields. They're a more explicit contract, they're  serialized, they can be debugged later, they're a nice place for extension through inheritance. The clunkier syntax is an accidental complexity -- .net 3.5 for example overcomes this. A common (and flawed) practice is to start with public fields, and turn them into properties later, on an 'as needed' basis. This breaks your contract with anyone who consumes your class, so it's best to start with properties."
5503,574.0,2008-08-08T00:15:04Z,5460,0,Expression Engine with the Multi-Site Manager works great for that kind of situation.
5588,358.0,2008-08-08T02:22:08Z,5260,1,I've worked with the following formula (pseudocode) with some success: now <- number of minutes since the work day started delay <- number of minutes in the delay day <- length of a work day in minutes x <- (now + delay) / day {integer division} y <- (now + delay) % day {modulo remainder} return startoftoday + x {in days} + y {in minutes}
5604,590.0,2008-08-08T03:00:36Z,5600,0,I too have always heard having an auto-incrementing int is good for performance even if you don't actually use it.
5607,581.0,2008-08-08T03:04:29Z,5600,31,"When dealing with indexes, you have to determine what your table is going to be used for.  If you are primarily inserting 1000 rows a second and not doing any querying, then a clustered index is a hit to performance.  If you are doing 1000 queries a second, then not having an index will lead to very bad performance.  The best thing to do when trying to tune queries/indexes is to use the Query Plan Analyzer and SQL Profiler in SQL Server.  This will show you where you are running into costly table scans or other performance blockers. As for the GUID vs ID argument, you can find people online that swear by both.  I have always been taught to use GUIDs unless I have a really good reason not to.  Jeff has a good post that talks about the reasons for using GUIDs: http://www.codinghorror.com/blog/archives/000817.html . As with most anything development related, if you are looking to improve performance there is not one, single right answer.  It really depends on what you are trying to accomplish and how you are implementing the solution.  The only true answer is to test, test, and test again against performance metrics to ensure that you are meeting your goals. [Edit]
@Matt, after doing some more research on the GUID/ID debate I came across this post.  Like I mentioned before, there is not a true right or wrong answer.  It depends on your specific implementation needs.  But these are some pretty valid reasons to use GUIDs as the primary key: For example, there is an issue known as a ""hotspot"", where certain pages of data in a table are under relatively high currency contention. Basically, what happens is most of the traffic on a table (and hence page-level locks) occurs on a small area of the table, towards the end. New records will always go to this hotspot, because IDENTITY is a sequential number generator. These inserts are troublesome because they require Exlusive page lock on the page they are added to (the hotspot). This effectively serializes all inserts to a table thanks to the page locking mechanism. NewID() on the other hand does not suffer from hotspots. Values generated using the NewID() function are only sequential for short bursts of inserts (where the function is being called very quickly, such as during a multi-row insert), which causes the inserted rows to spread randomly throughout the table's data pages instead of all at the end - thus eliminating a hotspot from inserts. Also, because the inserts are randomly distributed, the chance of page splits is greatly reduced. While a page split here and there isnt too bad, the effects do add up quickly. With IDENTITY, page Fill Factor is pretty useless as a tuning mechanism and might as well be set to 100% - rows will never be inserted in any page but the last one. With NewID(), you can actually make use of Fill Factor as a performance-enabling tool. You can set Fill Factor to a level that approximates estimated volume growth between index rebuilds, and then schedule the rebuilds during off-peak hours using dbcc reindex. This effectively delays the performance hits of page splits until off-peak times. If you even think you might need to enable replication for the table in question - then you might as well make the PK a uniqueidentifier and flag the guid field as ROWGUIDCOL. Replication will require a uniquely valued guid field with this attribute, and it will add one if none exists. If a suitable field exists, then it will just use the one thats there. Yet another huge benefit for using GUIDs for PKs is the fact that the value is indeed guaranteed unique - not just among all values generated by this server, but all values generated by all computers - whether it be your db server, web server, app server, or client machine. Pretty much every modern language has the capability of generating a valid guid now - in .NET you can use System.Guid.NewGuid. This is VERY handy when dealing with cached master-detail datasets in particular. You dont have to employ crazy temporary keying schemes just to relate your records together before they are committed. You just fetch a perfectly valid new Guid from the operating system for each new record's permanent key value at the time the record is created. http://forums.asp.net/t/264350.aspx"
5608,116.0,2008-08-08T03:04:55Z,5600,7,"The primary key serves three purposes: indicates that the column(s) should be unique indicates that the column(s) should be non-null document the intent that this is the unique identifier of the row The first two can be specified in lots of ways, as you have already done. The third reason is good: for humans, so they can easily see your intent for the computer, so a program that might compare or otherwise process your table can query the database for the table's primary key. A primary key doesn't have to be an auto-incrementing number field, so I would say that it's a good idea to specify your guid column as the primary key."
5699,653.0,2008-08-08T05:21:07Z,5690,6,What type is selectedYear? A DateTime? If so then you might need to convert to a string.
5733,188.0,2008-08-08T06:25:50Z,5600,1,"A Primary Key needn't be an autoincrementing field, in many cases this just means you are complicating your table structure. Instead, a Primary Key should be the minimum collection of attributes (note that most DBMS will allow a composite primary key) that uniquely identifies a tuple. In technical terms, it should be the field that every other field in the tuple is fully functionally dependent upon.  (If it isn't you might need to normalise). In practice, performance issues may mean that you merge tables, and use an incrementing field, but I seem to recall something about premature optimisation being evil..."
5883,34.0,2008-08-08T13:06:16Z,5880,2,"it tends to take an inexperienced team longer to build 3-tier.It's more code, so more bugs. I'm just playing the devil's advocate though."
5885,475.0,2008-08-08T13:08:40Z,5880,7,"I guess a fairly big downside is that the extra volume of code that you have to write, manage and maintain for a small project may just be overkill. It's all down to what's appropriate for the size of the project, the expected life of the final project and the budget!  Sometimes, whilst doing things 'properly' is appealing, doing something a little more 'lightweight' can be the right commercial decision!"
5899,493.0,2008-08-08T13:18:38Z,5880,1,"I would be pushing hard for the N tiered approach even if it's a small project. If you use an ORM tool like codesmith + nettiers you will be able to quickly setup the projects and be developing code that solves your business problems quickly. It kills me when you start a new project and you spend days sitting around spinning wheels talking about how the ""architecture"" should be architected. You want to be spending time solving the business problem, not solving problems that other people have solved for you. Using an ORM (it doesn't really matter which one, just pick one and stick to it) to help you get initial traction will help keep you focussed on the goals of the project and not distract you trying to solve ""architecture"" issues. If, at the end of the day, the architect wants to go the one project approach, there is no reason you can't create an app_code folder with a BLL and DAL folder to seperate the code for now which will help you move to an N-Tiered solution later."
5956,372.0,2008-08-08T13:58:59Z,5880,0,"As with anything abstraction creates complexity, and so the complexity of doing N-tiered should be properly justified, e.g., does N-tiered actually benefit the system? There will be small systems that will work best with N-tiered, although a lot of them will not. Also, even if your system is small at the moment, you might want to add more features to it later -- not going N-tiered might consitute a sort of technical debt on your part, so you have to be careful."
6022,235.0,2008-08-08T14:53:12Z,4080,0,"I am looking for many answers to learn about new tools and consolidate this knowledge in a one question/thread, so I doubt there will be 1 true answer to this question. My answer to my own question is that we use: Findbugs to look for common errors bad/coding - run from maven, and also integrates easily into Eclipse Cobertura for our coverage reports - run from maven Hudson also has a task-scanner plugin that will display a count of your TODO and FIXMEs, as well as show where they are in the source files. All are integrated with Maven 1.x in our case and tied into Hudson, which runs our builds on check-in as well as extra things nightly and weekly. Hudson trend graphs our JUnit tests, coverage, findbugs, as well as open tasks. There is also a Hudson plugin that reports and graphs our compile warnings. We also have several performance tests with their own graphs of performance and memory use over time using the Hudson plots plugin as well."
6078,729.0,2008-08-08T15:34:28Z,3400,0,"When it comes to full-text searching, for my money nothing beats Lucene .  There is a .Net port available that is compatible with indexes created with the Java version. There's a little work involved in that you have to create/maintain the indexes, but the search speed is fantastic and you can create all sorts of interesting queries.  Even indexing speed is pretty good - we just completely rebuild our indexes once a day and don't worry about updating them. As an example, this search functionality is powered by Lucene.Net."
6083,423.0,2008-08-08T15:42:21Z,6080,0,"OpenID seems to be a very good alternative to writing your own user management/authentication piece.  I'm seeing more and more sites using OpenID these days, so the barrier to entry for your users should be relatively low."
6084,676.0,2008-08-08T15:44:12Z,6080,4,"I like OpenID, but I'd still go with the email address, unless your user community is very technically savvy. It's still much easier for most people to understand and remember."
6088,302.0,2008-08-08T15:46:12Z,6080,41,"EMAIL ADDRESS Rational Users don't change emails very often Removes the step of asking for username and email address, which you'll need anyway Users don't often forget their email address (see number one) Email will be unique unless the user already registered for the site, in which case forward them to a forgot your password screen Almost everyone is using email as the primary login for access to a website, this means the rate of adoption shouldn't be affected by the fact that you're asking for an email address Update After registration, be sure to ask the user to create some kind of username, don't litter a public site with their email address! Also, another benefit of using an email address as a login: you won't need any other information (like password / password confirm), just send them a temp password through the mail, or forgo passwords altogether and send them a one-use URL to their email address every time they'd like to login (see: mugshot.org )"
6089,567.0,2008-08-08T15:46:36Z,6080,1,"I personally would say Email w/ Verification, OpenId is a great idea but I find that finding a provider that your already with is a pain, I only had an openId for here cause just 2 days before beta i decided to start a blog on blogspot. But everyone on the internet has en email address, especially when dealing with businesses, people aren't very opt to using there personal blog or whatnot for a business login."
6091,121.0,2008-08-08T15:48:06Z,6080,1,"I think that OpenID is definitely worth looking at.  Besides giving you a framework in which to provide a unified id for customers, it can also provide large businesses with the ability to manage their own logins and provide a common login across all products that they use, including your own.  This isn't that large of a benefit now when OpenId is still relatively rare, but as more products begin to use it, I suspect that the ability to use a common company OpenId login for each employee could become a good selling point. Since you're mostly catering to businesses, I don't think that it's all that unreasonable to offer to host the OpenId accounts yourself.  I just think that the extra flexibility will benefit your customers."
6093,722.0,2008-08-08T15:49:41Z,6080,7,"OpenID is very slick, and something you should seriously consider as it basically removes the requirement to save local usernames and passwords and worry about authentication. A lot of sites nowadays are using both OpenID and their own, giving users the option. If you do decide to roll your own, I'd recommend using the email address. Be careful, though, if you are creating something that groups users by an account (say, a company that has several users). In this case, the email address might be used more than once (if they do work for more than one company, for example), and you should allow that. HTH!"
6098,757.0,2008-08-08T15:54:51Z,6080,1,If most of your customers are mostly businesses then I think that using anything other than email creates problems for your customers. Most people are comfortable with email address login and since they are a business customer will likely want to use their work email rather than a personal account. OpenID creates a situation where there is a third party involved and many businesses don't like a third party involved.
6119,619.0,2008-08-08T16:40:43Z,6110,1,"I've had to do this before. The only real way to do it is to manually match up the various locations. Use your database's console interface and grouping select statements. First, add your ""Company Name"" field. Then: SELECT count(*) AS repcount, ""Location Name"" FROM mytable WHERE ""Company Name"" IS NULL GROUP BY ""Location Name"" ORDER BY repcount DESC LIMIT 5; Figure out what company the location at the top of the list belongs to and then update your company name field with an UPDATE ... WHERE ""Location Name"" = ""The Location"" statement. P.S. - You should really break your company names and location names out into separate tables and refer to them by their primary keys. Update: - Wow - no duplicates? How many records do you have?"
6120,302.0,2008-08-08T16:41:29Z,6110,0,"Please update the question, do you have a list of CompanyNames available to you? I ask because you maybe able to use Levenshtein algo to find a relationship between your list of CompanyNames and LocationNames. Update There is not a list of Company Names, I will have to generate the company name from the most descriptive or best Location Name that represents the multiple locations. Okay... try this: Build a list of candidate CompanyNames by finding LocationNames made up of mostly or all alphabetic characters. You can use regular expressions for this. Store this list in a separate table. Sort that list alphabetically and (manually) determine which entries should be CompanyNames. Compare each CompanyName to each LocationName and come up with a match score (use Levenshtein or some other string matching algo). Store the result in a separate table. Set a threshold score such that any MatchScore < Threshold will not be considered a match for a given CompanyName. Manually vet through the LocationNames by CompanyName | LocationName | MatchScore, and figure out which ones actually match. Ordering by MatchScore should make the process less painful. The whole purpose of the above actions is to automate parts and limit the scope of your problem. It's far from perfect, but will hopefully save you the trouble of going through 18K records by hand."
6124,758.0,2008-08-08T16:44:48Z,6110,0,"I was going to recommend some complicated token matching algorithm but it's really tricky to get right and if you're data does not have a lot of correlation (typos, etc) then it's not going to give very good results. I would recommend you submit a job to the Amazon Mechanical Turk and let a human sort it out."
6129,726.0,2008-08-08T16:47:35Z,6110,0,"Ideally, you'd probably want a separate table named Company and then a company_id column in this ""Location"" table that is a foreign key to the Company table's primary key, likely called id. That would avoid a fair bit of text duplication in this table (over 18,000 rows, an integer foreign key would save quite a bit of space over a varchar column). But you're still faced with a method for loading that Company table and then properly associating it with the rows in Location. There's no general solution, but you could do something along these lines: Create the Company table, with an id column that auto-increments (depends on your RDBMS). Find all of the unique company names and insert them into Company. Add a column, company_id, to Location that accepts NULLs (for now) and that is a foreign key of the Company.id column. For each row in Location, determine the corresponding company, and UPDATE that row's company_id column with that company's id. This is likely the most challenging step. If your data is like what you show in the example, you'll likely have to take many runs at this with various string matching approaches. Once all rows in Location have a company_id value, then you can ALTER the Company table to add a NOT NULL constraint to the company_id column (assuming that every location must have a company, which seems reasonable). If you can make a copy of your Location table, you can gradually build up a series of SQL statements to populate the company_id foreign key. If you make a mistake, you can just start over and rerun the script up to the point of failure."
6144,267.0,2008-08-08T16:56:13Z,6130,33,"The file in the .svn directory that keeps track of what you have checked out, when, what revision, and from where, has gotten corrupted somehow, for that particular file. This is no more dangerous or critical than the normal odd file problem, and can be because of various problems, like a subversion program dying mid-change, power-disruption, etc. Unless it happens more I wouldn't make much out of it. It can be fixed by doing what you did, make a copy of your work-files, check out a fresh copy, and add the modified files back in. Note that this might cause problems if you have a busy project where you would normally have to merge in changes. For instance, you and a collegue both check out a fresh copy, and start working on the same file. At some point, your collegue checks in his modifications. When you attempt to do the same, you get the checksum problem you have. If you now make copies of your changed files, do a fresh checkout, then subversion will lose track of how your changes should be merged back in. If you didn't get the problem in this case, when you got around to checkin in your modifications, you would need to update your working copy first, and possibly handle a conflict with your file. However, if you do a fresh checkout, complete with your collegues changes, it now looks like you removed his changes and substituted with your own. No conflicts, and no indications from subversion that something is amiss."
6212,194.0,2008-08-08T18:07:03Z,6210,0,"Are you just talking about the interface and storage, or the implementation of sending the emails as well? Yes, a SQL table with FROM, TO, Subject, Body should work for storage and, heck, a textbox or even maybe a RichText box should work for editing. Or is this a web interface? For actually sending it, check out the System.Web.Mail namespace, it's pretty self explanatory and easy to use :)"
6213,312.0,2008-08-08T18:09:55Z,6210,2,"From a high level, yes.  :D  The main thing is some place to store the templates.  A database is a great option unless you're not already using one, then file systems work fine. WSIWIG editors (such as fckeditor) work well and give you some good options regarding the features that you allow. Some sort of token replacement system is also a good idea if you need it.  For example, if someone puts %FIRSTNAME% in the email template, the code that generates the email can do some simple pattern matching to replace known tokens with other known values that may be dynamic based on user or other circumstances."
6228,380.0,2008-08-08T18:25:17Z,6210,1,"I am thinking that if these are automated notifications, then this means they are probably going out as a result of some type of event in your software. If this is a web based app, and you are going to have a number of these being sent out, then consider implementing an email queue rather than sending out an email on every event. A component can query the queue periodically and send out any pending items."
6349,341.0,2008-08-08T20:18:25Z,6340,7,"You defined the primary key twice. Try: CREATE TABLE SHIPPING_GRID(  
    id INT NOT NULL AUTO_INCREMENT PRIMARY KEY COMMENT 'Unique ID for each row',  
    shipping_vendor_no INT(6) NOT NULL COMMENT 'Foreign key to VENDOR.no for the shipping vendor (vendors_type must be 3)',  
    start_vendor_no INT(6) NOT NULL COMMENT 'Foreign key to VENDOR.no for the vendor being shipped from',  
    end_vendor_no INT(6) NOT NULL COMMENT 'Foreign key to the VENDOR.no for the vendor being shipped to',  
    shipment_duration INT(1) DEFAULT 1 COMMENT 'Duration in whole days shipment will take',  
    price FLOAT(5,5) NOT NULL COMMENT 'Price in US dollars per shipment lbs (down to 5 decimal places)',  
    is_flat_rate TINYINT(1) DEFAULT 0 COMMENT '1 if is flat rate regardless of weight, 0 if price is by lbs',  
    INDEX (shipping_vendor_no),  
    INDEX (start_vendor_no),  
    INDEX (end_vendor_no),  
    FOREIGN KEY (shipping_vendor_no) REFERENCES VENDOR (no),  
    FOREIGN KEY (start_vendor_no) REFERENCES VENDOR (no),  
    FOREIGN KEY (end_vendor_no) REFERENCES VENDOR (no)  
) TYPE = INNODB; The VENDOR primary key must be INT(6), and both tables must be of type InnoDB."
6350,431.0,2008-08-08T20:18:58Z,6340,0,"I ran the code here, and the error message showed (and it is right!) that you are setting id field twice as primary key."
6364,58.0,2008-08-08T20:44:23Z,6340,0,"Can you provide the definition of the
  VENDOR table I figured it out. The VENDOR table was MyISAM... (edited your answer to tell me to make them both INNODB ;) ) (any reason not to just switch the VENDOR type over to INNODB?)"
6428,726.0,2008-08-08T22:07:02Z,6110,0,"Yes, that step 4 from my previous post is a doozy. No matter what, you're probably going to have to do some of this by hand, but you may be able to automate the bulk of it. For the example locations you gave, a query like the following would set the appropriate company_id value: UPDATE  Location SET     Company_ID = 1 WHERE   (LOWER(Location_Name) LIKE '%to_n shop%' OR      LOWER(Location_Name) LIKE '%tts%') AND     Company_ID IS NULL; I believe that would match your examples (I added the IS NULL part to not overwrite previously set Company_ID values), but of course in 18,000 rows you're going to have to be pretty inventive to handle the various combinations. Something else that might help would be to use the names in Company to generate queries like the one above. You could do something like the following (in MySQL): SELECT  CONCAT('UPDATE Location SET Company_ID = ', Company_ID, ' WHERE LOWER(Location_Name) LIKE ', LOWER(REPLACE(Company_Name), ' ', '%'), ' AND Company_ID IS NULL;') FROM    Company; Then just run the statements that it produces. That could do a lot of the grunge work for you."
6435,303.0,2008-08-08T22:20:28Z,6430,4,"foreach(var row in DataGrid1.Rows)
{
  DoStuff(row);
}
//Or ---------------------------------------------   
foreach(DataGridRow row in DataGrid1.Rows)
{
  DoStuff(row);
}
//Or ---------------------------------------------
for(int i = 0; i< DataGrid1.Rows.Count - 1; i++)
{
  DoStuff(DataGrid1.Rows[i]);
}"
6445,91.0,2008-08-08T22:35:06Z,6440,20,"That's one of the sad reasons i'm still targeting .net 2.0 whenever possible :/ But people don't neccessarily need the full 200 MB Package. There is a 3 MB Bootstrapper which will only download the required components: .net 3.5 SP1 Bootstrapper However, the worst case scenario is still a pretty hefty download. Also, see this article for a more detailed explanation on the size and an alternative workaround to the size problem. Addition: Since answering this question, Scott Hanselman created SmallestDotNet.com , which will determine the smallest required download. Doesn't change the worst case scenario, but is still useful to know."
6451,541.0,2008-08-08T22:58:45Z,6440,5,"Have you looked at the .NET Framework Client Profile? It is much smaller than the full redistributable package and is optimized for delivering just the functionality needed for smart clients. Here is a nice overview. I don't know if this will keep the download under two minutes or not, but it should get you quite a bit closer."
6452,243.0,2008-08-08T22:59:48Z,6440,1,"Also, it is worth including (in some fashion) the Service Pack downloads as well. In fact, depending on how your executables are built, you might be forced to install the Framework and the Service Packs."
6453,489.0,2008-08-08T23:00:49Z,6440,12,"Once .NET Framework 3.5 SP1 comes out (should be fairly soon) there will be a second option of frameworks, namely the ""Client Profile"", which is a cut-down framework that only weighs in about about 30Mb from memory. It doesn't include all of the namespaces and classes of the full framework, but should be enough for most common apps in theory. It can be upgraded to the full framework if necessary (eg. if an update to your software introduces a new dependency) More more information, see here: BCL Team blog"
6509,137.0,2008-08-09T01:12:37Z,6130,5,"I occasionally get similar things, usually with files that nobody has been near in weeks. Generally, if you know you haven't been working in the directory in question, you can just delete the directory with the problem and run svn update to recreate it. If you have live changes in the directory then as lassevk and you yourself suggested, a more careful approach is required. Generally speaking I would say it's a good idea not to leave edited files uncommitted, and keep the working copy tidy - don't add a whole bunch of extra files into the working copy that you aren't going to use. Commit regularly, and then if the working copy goes tits up, you can just delete the whole thing and start over without worrying about what you might or might not be losing, and without the pain of trying to figure out what files to save."
6534,106.0,2008-08-09T02:10:12Z,6530,6,App_Data folder on the root of the project. It isn't served to web requests; so other people can't snoop for it.
6541,234.0,2008-08-09T02:17:38Z,6430,0,"Is there anything about WinForms 3.0 that is so much better than in 1.1 I don't know about 3.0, but you can write code in VS 2008 which runs on the .NET 2.0 framework. (So, you get to use the latest C# language, but you can only use the 2.0 libraries) This gets you Generics ( List<DataRow> instead of those GodAwful ArrayLists) and a ton of other stuff, you'll literally end up writing 3x less code."
6651,825.0,2008-08-09T06:14:12Z,4670,4,"There's a nice comparison between git, hg and bzr in this InfoQ article .  They all have their strengths and weaknesses.  You'll have to think about your project and your workflows and choose the best fit.  The good news is that they're all fairly good."
6702,383.0,2008-08-09T08:47:31Z,6530,1,"I would suggest putting the log file onto a seperate disk, though should give you a little performance gain so that your not trying to both read and write to the same disk as the website. If you cannot put the log file on a seperate disk, then I would simply choose a folder of your choice. In any case, you will have to give the ""Network Service"" account ""Modify"" permissions to the desired folder. If on the other hand, you have access to a databse, then log the information there. It will be much quicker than accessing the hard drive and won't be publically available. You'll also be able to report from the data quite easily."
6755,146270.0,2008-08-09T13:03:16Z,6440,1,"For the record, .Net Framework 3.5 SP1 is required for Microsoft SQL Server 2008 to install and RTM'd around the same time as the release this week. Still a hefty install but you can extract the client profile from it. Just not to sure how."
6935,419.0,2008-08-09T22:32:07Z,6890,2,"If I'm reading this properly, you're starting work items on bunch of threads but you're concerned that the threads are completing out of order, and, would prefer to wait until each thread completes in the order they were started ? If this is the case then I'm not sure why you're creating a thread per work item. Maybe you could expand? If you did need to wait for a thread to complete the you could just do: t.Join();"
7101,878.0,2008-08-10T08:37:28Z,6890,5,"How much order do you need to impose on the threads? If you just need all of the work started in the loop to finish before the code continues, but you don't care about the order the work within the loop finishes, then calling Join is the answer. To add more detail to Kevin Kenny's answer, you should call Join outside the loop. This means you will need a collection to hold references to the threads you started: // Start all of the threads.
List<Thread> startedThreads = new List<Thread>();
foreach (...) {
  Thread thread = new Thread(new ThreadStart(MyMethod));
  thread.Start();
  startedThreads.Add(thread);
}

// Wait for all of the threads to finish.
foreach (Thread thread in startedThreads) {
  thread.Join();
} In contrast, if you called Join inside the loop, the result would basically be the same as not using threads at all. Each iteration of the loop body would create and start a thread but then immediately Join it and wait for it to finish. If the individual threads produce some result (write a message in a log, for example) then the messages may still appear out of order because there's no coordination between the threads. It is possible to get the threads to output their results in order by coordinating them with a Monitor."
7182,722.0,2008-08-10T12:47:25Z,7180,0,"JavaScript is probably the way to go if you want to avoid Flash. Check this: http://www.webreference.com/programming/javascript/java_anim/ It won't work for embedded video, though, so you're stuck with Flash for that (or Silverlight, or QuickTime )."
7188,224.0,2008-08-10T13:01:37Z,7180,3,Silverlight springs to mind as an obvious choice if you want to do animation using .NET on the web. It may not cover all platforms but will work in IE and FireFox and on the Mac.
7198,922.0,2008-08-10T14:01:31Z,7190,1,"I would have a look at Team City http://www.jetbrains.com/teamcity/index.html I know some people who are looking in to this and they say good things about it. My companies build process is done in FinalBuilder so I'm going to be looking at their server soon. CC is quite good in that you can have one CC server monitor another CC server so you could set up stuff like - when a build completes on your build server, your test server would wake up, boot up a virtual machine and deploy your application.  Stuff like that."
7202,33.0,2008-08-10T14:28:08Z,7180,2,"Have a look at the jQuery cross browser JavaScript library for animation (it is what is used on Stack Overflow). The reference for it can be found at http://visualjquery.com/1.1.2.html . Unfortunately without Flash, Silverlight or another plug-in cross system video support is limited."
7217,905.0,2008-08-10T15:24:23Z,260,4,"The main application that my division sells does something very similar to provide client customisations (which means that I can't post any source). We have a C# application that loads dynamic VB.NET scripts (although any .NET language could be easily supported - VB was chosen because the customisation team came from an ASP background). Using .NET's CodeDom we compile the scripts from the database, using the VB CodeDomProvider (annoyingly it defaults to .NET 2, if you want to support 3.5 features you need to pass a dictionary with ""CompilerVersion"" = ""v3.5"" to its constructor). Use the CodeDomProvider.CompileAssemblyFromSource method to compile it (you can pass settings to force it to compile in memory only. This would result in hundreds of assemblies in memory, but you could put all the dynamic classes' code together into a single assembly, and recompile the whole lot when any change. This has the advantage that you could add a flag to compile on disk with a PDB for when you're testing, allowing you to debug through the dynamic code."
7221,905.0,2008-08-10T15:34:37Z,7190,2,"We use CruiseControl with NUnit, NCover, FxCop, SVN and some custom tools we wrote ourselves to produce the reports. In my opinion it has proven (over the last few years) to be an excellent combination. It's frustrating that MS restricts all of its integration tools to VSTS.  Its test framework is as good as NUnit, but you can't use its code coverage tools or anything else. I'd check out XNuit - it's looking pretty promising (but currently lacking UI). We automate nightly builds, and you could automate UAT and manual test builds, but I'm not sure that we'd ever want to automate the release to our production servers. Even if it were any change would be important enough that someone would have to watch over it anyway."
7299,146270.0,2008-08-10T19:35:18Z,7190,0,Microsoft loosened it's constraint on the Testing Platform by including it in Visual Studio 2008 Professional and allowing for the tests to be run from the command line with Framework 3.5 installed. We did a crossover for a client recently and so far they have been able to run all the tests without the need for NUnit.
7302,146270.0,2008-08-10T19:38:34Z,7180,0,Silverlight is the answer and Moonlight will be the linux equivalent and available shortly. We have done some beta testing on moonlight and found it fairly stable at with most of the Silverlight work we do.
7343,233.0,2008-08-10T20:52:20Z,6530,0,"I'm not in a position to modify the permissions on folders (especially outside of the virtual directory home folder), and don't already have an App_Data folder, so am a bit hesitant to go with that. So for the moment I'm going with the CommonApplicationData Folder. On Vista/Server 2008 this is C:\ProgramData\ On XP/Server 2003 this is C:\Documents and Settings\All Users\Application Data\"
7345,872.0,2008-08-10T20:57:21Z,6530,0,"I'm not in a position to modify the permissions on folders (especially outside of the virtual directory home folder), and don't already have an App_Data folder, so am a bit hesitant to go with that. If you have a website, you clearly have a folder somewhere.  Can you not add a (non-web-facing) subfolder?  It seems like that would be a more appropriate place to put your logs than dumping them into a global, shared folder."
7464,,2008-08-11T01:11:07Z,7440,2,"We use Selenium Core, but are switching gradually to Selenium RC which is much nicer and easier to manage. We have written lots of custom code to make the tests run on our Continuous Integration servers, some of them in parallel suites to run faster. One thing you'll find is that Selenium seems to restart the browser for each test (you can set it not to do this, but we got memory problems when we did that). This can be slow in Firefox, but is not too bad in IE (one time I'm thankful for Bill Gates's OS integraion)."
7465,733.0,2008-08-11T01:12:54Z,7440,3,"We are using QuickTestPro.  So far it is effective, but the browser selection is limited.  The nicest part is the ability to record your browser's activity, and convert it into a scriptable set of steps.  There is also a nice .Net addin so if you have any validation code you need to do for the different stages of your test, you can write methods in an assembly and call them from your script."
7471,150.0,2008-08-11T01:33:20Z,7470,1,"My eventual (and hopefully temporary) solution to this problem was a Python CGI script.  My script follows for anyone who might find it useful (despite the fact that this is a total hack). #!/usr/bin/python
""""""A CGI script to produce an RSS feed of top-level Gallery2 albums.""""""

#import cgi
#import cgitb; cgitb.enable()
from time import gmtime, strftime
import MySQLdb

ALBUM_QUERY = '''
    select g_id, g_title, g_originationTimestamp
    from g_Item
    where g_canContainChildren = 1 
    order by g_originationTimestamp desc
    limit 0, 20
    '''

RSS_TEMPLATE = '''Content-Type: text/xml

<?xml version=""1.0""?>
<rss version=""2.0"">
  <channel>
    <title>TITLE</title>
    <link><http://example.com/gallery2/main.php></link>
    <description>DESCRIPTION</description>
    <ttl>1440</ttl>
%s
  </channel>
</rss>
'''

ITEM_TEMPLATE = '''
    <item>
      <title>%s</title>
      <link><http://example.com/gallery2/main.php?g2_itemId=%s></link>
      <description>%s</description>
      <pubDate>%s</pubDate>
    </item>
'''

def to_item(row):
    item_id = row[0]
    title = row[1]
    date = strftime(""%a, %d %b %Y %H:%M:%S GMT"", gmtime(row[2]))
    return ITEM_TEMPLATE % (title, item_id, title, date)

conn = MySQLdb.connect(host = ""HOST"",
                       user = ""USER"",
                       passwd = ""PASSWORD"",
                       db = ""DATABASE"")
curs = conn.cursor()
curs.execute(ALBUM_QUERY)
print RSS_TEMPLATE % ''.join([ to_item(row) for row in curs.fetchall() ])
curs.close()"
7478,206.0,2008-08-11T01:45:10Z,6530,0,"You could also log to the Windows Event log or to a table in a database.  How often are people looking at the event log?  If it's being examined on a regualr basis, writing to a table amkes the reporting back much easier as it's trivial to reverse the order and only show the last X events for the current time period.  The Windows Event log you can also query the Windows Event Log through PowerShell or with LogParser ."
7481,716.0,2008-08-11T02:01:00Z,7260,5,For windows this is a good introduction and guide Here are some good ssh-agents for systems other than linux. Windows - pageant OS X - SSHKeychain
7490,1975282.0,2008-08-11T02:52:41Z,7440,4,"Well, if you've designed your application properly, you won't have scads of logic inside the UI anyway. It makes much more sense to separate the actual work getting done into units separate from the UI, and then test those. If you do that, then the only code in the UI will be code that invokes the backend, so simply testing the backend is sufficient. I have used NUnit ASP in the past (at my job), and if you insist on unit testing your UI, I would strongly advise you to use ANYTHING but NUnit ASP. It's a pain to work with, and tests tend to be invalidated (needing to be revised) after even the most minor UI changes (even if the subjects of the tests don't actually change)."
7491,785.0,2008-08-11T02:59:12Z,6430,1,"object cell = myDataGrid[row, col];"
7500,122.0,2008-08-11T03:43:10Z,7440,3,"We have been using JSunit for a while to do unit tests... it may not be the same kinds of tests you are talking about, but it is great for ensuring your JavaScript works as you expect. You run it in the browser, and it can be set in an Ant build to be automatically run against a bunch of browsers on a bunch of platforms remotely (so you can ensure your code is cross-browser as well as ensure the logic is correct). I don't think it replaces Selenium, but it complements it well."
7570,234.0,2008-08-11T06:35:58Z,7540,7,"It's probably the awesome firefox3 fsync ""bug"" , which is a giant pile of fail. In summary Firefox3 saves its bookmarks and history in an SQLite database Every time you load a page it writes to this database several times SQLite cares deeply that you don't lose your bookmarks, so each time it writes, instructs the kernel to flush it's database file to disk and ensure that it's fully written Many variants of linux, when told to flush like that, flush EVERY FILE. This may take up to a minute or more if you have background tasks doing any kind of disk intensive stuff. The kernel makes firefox wait while this flush happens, which locks up the UI."
7576,952.0,2008-08-11T06:55:05Z,6210,0,Adam Haile writes: check out the System.Web.Mail namespace By which you mean System.Net.Mail in .Net 2.0 and above :)
7587,,2008-08-11T07:33:18Z,6210,0,How about using the new Workflow components in .Net 3.0 (and 3.5)? That is what we use in combination with templates in my current project. The templates have the basic format and the the tokens that are replaced with user information.
7725,997.0,2008-08-11T12:39:46Z,2300,12,"Something like this? dim cars(2),x
cars(0)=""Volvo""
cars(1)=""Saab""
cars(2)=""BMW""

For Each x in cars
  response.write(x & ""<br />"")
Next See www.w3schools.com . If you want to associate keys and values use a dictionary object instead: Dim objDictionary
Set objDictionary = CreateObject(""Scripting.Dictionary"")
objDictionary.Add ""Name"", ""Scott""
objDictionary.Add ""Age"", ""20""
if objDictionary.Exists(""Name"") then
    ' Do something
else
    ' Do something else 
end if"
7747,287.0,2008-08-11T12:59:04Z,7720,1,"Install4J . Not free, but worth it. Give the trial a shot"
7759,287.0,2008-08-11T13:09:20Z,7720,0,I went through the same and found that all of the free options weren't very good. Looks like you'll be writing your own. I'd be interested to see if someone has a free/cheap option that works
7796,737.0,2008-08-11T13:48:31Z,7720,2,Have you thought about Java Web Start ? Here is a tutorial specifically for deploying an SWT application with Java Web Start.
7876,1026.0,2008-08-11T15:42:35Z,7540,1,"There's no ""process explorer"" kind of tool for Firefox; but there's http://developer.mozilla.org/en/docs/Venkman with profiling mode, which you could use to see the time spent by chrome (meaning non-content, that is not web-page) scripts. From what I've read about it, DTrace might also be useful for this sort of thing, but it requires creating a custom build and possibly adding additional probes to the source. I haven't played with it myself yet."
7886,1028.0,2008-08-11T15:58:06Z,7880,8,"There are three ways to do this, depending on your needs. You could use the old-school C way and call fopen/fread/fclose, or you could use the C++ fstream facilities (ifstream/ofstream), or if you're using MFC, use the CFile class, which provides functions to accomplish actual file operations. All of these are suitable for both text and binary, though none have a specific readline functionality. What you'd most likely do instead in that case is use the fstream classes (fstream.h) and use the stream operators (<< and >>) or the read function to read/write blocks of text: int nsize = 10;
char *somedata;
ifstream myfile;
myfile.open(""<path to file>"");
myfile.read(somedata,nsize);
myfile.close(); Note that, if you're using Visual Studio 2005 or higher, traditional fstream may not be available (there's a new Microsoft implementation, which is slightly different, but accomplishes the same thing)."
7888,872.0,2008-08-11T15:59:48Z,7880,24,"You need to use an ifstream if you just want to read (use an ofstream to write, or an fstream for both). To open a file in text mode, do the following: ifstream in(""filename.ext"", ios_base::in); // the in flag is optional To open a file in binary mode, you just need to add the ""binary"" flag. ifstream in2(""filename2.ext"", ios_base::in | ios_base::binary ); Use the ifstream.read() function to read a block of characters (in binary or text mode).  Use the getline() function (it's global) to read an entire line."
7896,268.0,2008-08-11T16:07:31Z,7880,-2,"fstream are great but I will go a little deeper and tell you about RAII . The problem with a classic example is that you are forced to close the file by yourself, meaning that you will have to bend your architecture to this need. RAII makes use of the automatic destructor call in C++ to close the file for you. Update : seems that std::fstream already implements RAII so the code below is useless. I'll keep it here for posterity and as an example of RAII. class FileOpener
{
public:
    FileOpener(std::fstream& file, const char* fileName): m_file(file)
    { 
        m_file.open(fileName); 
    }
    ~FileOpeneer()
    { 
        file.close(); 
    }

private:
    std::fstream& m_file;
}; You can now use this class in your code like this: int nsize = 10;
char *somedata;
ifstream myfile;
FileOpener opener(myfile, ""<path to file>"");
myfile.read(somedata,nsize);
// myfile is closed automatically when opener destructor is called Learning how RAII works can save you some headaches and some major memory management bugs."
7945,1975282.0,2008-08-11T16:50:32Z,7940,9,"a) Must look the same b) As standards-compliant as possible, but not so anal that it blocks finishing work In a situation where you have perpetual access to the code, I don't think standards-compliance is all that important, since you can always make changes to the code if something breaks. If you don't have perpetual access (ie, you sign off on the code and it becomes someone else's responsibility), it's probably best to be as standards-compliant as possible to minimize maintenance headaches later... even if you never have to deal with the code again, your reputation persists and can be transmitted to other potential clients, and many teams like to blame the previous developer(s) for problems that come up."
7947,72.0,2008-08-11T16:52:01Z,7940,0,"I think validation is a good litmus test of whether you've done things properly, so if there are only a few minor problems, why not fix them and ensure your site will at least be understood correctly by browsers in the future (even if they do render things differently for other reasons)? OTOH, for most projects, validation seems like a huge headache and if you can get things working across browsers, it's not worth spending an extra day/week+ on just validation."
7948,1035.0,2008-08-11T16:52:57Z,7940,4,"I think this is an area in which you should strive to use the Robustness principle as far as is practical (which is good advice for any area of coding). Just because something works today doesn't mean it will work tomorrow: if you're relying on a particular HTML/CSS hack or even if you've just been a little lax in emitting strictly valid code, the next iteration of browsers could well break. Doing it once the right way minimises this problem (though does not entirely mitigate it). There is a certain element of pragmatism to take here, though. I'd certainly do all I could for a client's site to be valid, but I would be willing to take more risks in my own space."
7951,999.0,2008-08-11T16:55:02Z,7940,3,"I think it's only ""tech"" guys that really care for ""100% standard compliance"". My usual page consumers (= users) don't care if there's no alt-attribute for a ""menu border picture element"". I usually just make sure that I don't see any obvious errors (all tags closed, all lower case, attributes in quotes, ...), but if it looks good on IE and FF, that's all I care for. I don't really care if I use a non-standard attribute in any HTML tag, so that the page doesn't validate against an DTD - as long as I get the visual results that I intended to get."
7952,840.0,2008-08-11T16:55:52Z,7940,1,"I know this isn't answering your whole question, but it is worth considering that by using completely valid html you can be sure that your website should work properly in future web browsers that haven't been released yet."
8006,1975282.0,2008-08-11T17:46:52Z,7990,1,"This may not be what you're looking for, but if I needed to do this quick&dirty, I would: Create a separate WPF application (so I could use the built-in document handling) Give the service the ability to interact with the desktop (note that you don't actually have to show anything on the desktop, or be logged in for this to work) Have the service run the application, and give it the data to print. You could probably also jigger this to print from a web browser that you run from the service (though I'd recommend building your own shell IE, rather than using a full browser). For a more detailed (also free) solution, your best bet is probably to manually format the document yourself (using GDI+ to do the layout for you). This is tedious, error prone, time consuming, and wastes a lot of paper during development, but also gives you the most control over what's going to the printer."
8058,1975282.0,2008-08-11T18:23:55Z,8050,1,Here you go. I started with ScottGu's explanation/examples and went from there: http://weblogs.asp.net/scottgu/archive/2007/05/19/using-linq-to-sql-part-1.aspx
8061,509.0,2008-08-11T18:28:12Z,8050,37,"LINQ stands for Language Integrated Query and is a set of extensions for .NET that allow you to query data the same way from code and isn't tied to a specific data source.  You can use the same LINQ code for SQL Server, XML, objects, DataSets, and Entities. Here is a good intro from Scott Guthrie This is a nice set of 101 LINQ Samples"
8062,439.0,2008-08-11T18:28:19Z,8050,7,"Start with everything Scott Guthrie has on linq Get LINQ Pocket Reference , which is an excerpt from C# 3.0 in a Nutshell"
8072,373.0,2008-08-11T18:38:39Z,8050,7,Here are a couple of good tutorials (video) from OakLeaf Systems: http://oakleafblog.blogspot.com/2007/04/two-new-linq-to-sql-video-segments-from.html http://oakleafblog.blogspot.com/2007/05/mike-taulty-posts-six-new-linq-to-xml.html EDIT: I just ran into this great tool created by the author of C# in a Nutshell: http://www.linqpad.net/ It includes lots of great easy to follow samples.
8080,785.0,2008-08-11T18:43:06Z,8050,11,"Two books you should consider for learning about LINQ, both from Manning: C# in Depth LINQ in Action The former was by far the better written, and taught me almost as much about LINQ in a single chapter than the latter did in a whole book.  LINQ is built on a lot of foundation, and C# in Depth builds it up from the ground. The second book is a whole lot better than nothing, and you will learn things specifically about LINQ that you won't learn in the first.  But the first book will give you much better foundation, and puts up at least a token perspective instead of more or less blindly following the MS line.  So, I'm recommending C# in Depth first and foremost for learning LINQ. Mike"
8081,667.0,2008-08-11T18:43:51Z,8050,4,"Linq is short for ""Language integrated query."" It's a set of language enhancements built into C# and VB. Basically, what you get is a bunch of  standard query operators that can be applied to any IEnumerable of type T. There's a lot of different linq providers for specific types of data- for example, there's linq to xml, linq to entities, even linq to sharepoint. To get started with linq, in all its many forms, I suggest the book Pro Linq by Joseph C. Rattz. It's an excellent overview of Linq. He takes a ground-up approach, first describing all the language features (like Lambda Expressions and Expression Trees) that Linq is built on, and then moving on to some standard linq provider implementations. Additionally, here's a pretty good MSDN article describing Linq: LINQ: .NET Language-Integrated Query Now, Linq to Sql is a linq provider written specifically for SQL Server. Included in this provider is an OR/M, that gives you some handy-dandy functionality (like typing out all your sql tables, so you get a robust design-time view of your database schema.) It's totally awesome, and for me, has greatly speed up development time when working with a sql database.
The book I recommended above also has a great section about using Linq To Sql. Also,
here's a good ""beginner's guide"" article from MSDN: Linq To SQL: .NET Language-Integrated Query for Relational Data"
8084,905.0,2008-08-11T18:46:50Z,8050,1,I think this book: C# in Depth By Jon Skeet is an excellent programmers' guide that matches your exact needs (moving from earlier C# to C#3.5). Also if you order it you get the electronic copy too - something more publishers should do (excellent for both Kindles and searching).
8094,563.0,2008-08-11T19:01:49Z,7990,8,"Printing from a Windows service is really painful. It seems to work... sometimes... but finally it craches or throws an exception from time to time, without any clear reason. It's really hopeless. Officially, it's even not supported , without any explanation, nor any proposal for an alternate solution. Recently, I have been confronted to the problem and after several unsuccessful trials and experimentations, I came finally with two viable solutions: Write your own printing DLL using the Win32 API (in C/C++ for instance), then use it from your service with P/Invoke (works fine) Write your own printing COM+ component, then uses it from your service. I have chosen this solution with success recently (but it was third party COM+ component, not own written) It works absolutely fine too."
8112,380.0,2008-08-11T19:20:29Z,7990,13,"Trust me, you will spend more money trying to search/develop a solution for this as compared to buying a third party component. Do not reinvent the wheel and go for the paid solution. Printing is a complex problem and I would love to see the day when better framework support is added for this."
8155,960.0,2008-08-11T19:59:34Z,8140,1,"Find out what minimum requrements you want to put on a plugin writer. Then make one or more Interfaces that the writer must implement for your code to know when and where to execute the code. Make an API the writer can use to access some of the functionality in your code. You could also make a base class the writer must inherit. This will make wiring up the API easier. Then use some kind of reflection to scan a directory, and load the classes you find that matches your requirements. Some people also make a scripting language for their system, or implements an interpreter for a subset of an existing language. This is also a possible route to go. Bottom line is: When you get the code to load, only your imagination should be able to stop you. Good luck."
8157,872.0,2008-08-11T20:00:09Z,8140,3,"This is generally something thatyou'll have to expose yourself, so yes, it will be dependent on the language your system is written in (though often it's possible to write wrappers for other languages as well). If, for example, you had a program written in C, for Windows, plugins would be written for your program as DLLs.  At runtime, you would manually load these DLLs, and expose some interface to them.  For example, the DLLs might expose a gimme_the_interface() function which could accept a structure filled with function pointers.  These function pointers would allow the DLL to make calls, register callbacks, etc. If you were in C++, you would use the DLL system, except you would probably pass an object pointer instead of a struct, and the object would implement an interface which provided functionality (accomplishing the same thing as the struct, but less ugly).  For Java, you would load class files on-demand instead of DLLs, but the basic idea would be the same. In all cases, you'll need to define a standard interface between your code and the plugins, so that you can initialize the plugins, and so the plugins can interact with you. P.S. If you'd like to see a good example of a C++ plugin system, check out the foobar2000 SDK .  I haven't used it in quite a while, but it used to be really well done.  I assume it still is."
8164,521.0,2008-08-11T20:06:53Z,8140,4,"I've used event-based APIs for plugins in the past. You can insert hooks for plugins by dispatching events and providing access to the application state. For example, if you were writing a blogging application, you might want to raise an event just before a new post is saved to the database, and provide the post HTML to the plugin to alter as needed."
8173,1026.0,2008-08-11T20:15:06Z,8140,2,"I'm tempted to point you to the Design Patterns book for this generic question :p Seriously, I think the answer is no. You can't write extensible code by default, it will be both hard to write/extend and awfully inefficient (Mozilla started with the idea of being very extensible, used XPCOM everywhere, and now they realized it was a mistake and started to remove it where it doesn't make sense). what makes sense to do is to identify the pieces of your system that can be meaningfully extended and support a proper API for these cases (e.g. language support plug-ins in an editor). You'd use the relevant patterns, but the specific implementation depends on your platform/language choice. IMO, it also helps to use a dynamic language - makes it possible to tweak the core code at run time (when absolutely necessary). I appreciated that Mozilla's extensibility works that way when writing Firefox extensions."
8195,96.0,2008-08-11T20:38:25Z,7990,0,"I think we are going to go the third party route.  I like the XSL -> HTML -> PDF -> Printer flow... Winnovative's HTML to PDF looks good for the first part, but I'm running into a block finding a good PDF printing solution... any suggestions?  Ideally the license would be on a developer basis, not on a deployed runtime basis."
8196,121.0,2008-08-11T20:40:44Z,8140,1,"If you are using a compiled language such as C or C++, it may be a good idea to look at plugin support via scripting languages.  Both Python and Lua are excellent languages that are used to script a large number of applications (Civ4 and blender use Python, Supreme Commander uses Lua, etc). If you are using C++, check out the boost python library.  Otherwise, python ships with headers that can be used in C, and does a fairly good job documenting the C/python API.  The documentation seemed less complete for Lua, but I may not have been looking hard enough.  Either way, you can offer a fairly solid scripting platform without a terrible amount of work.  It still isn't trivial, but it provides you with a very good base to work from."
8272,645.0,2008-08-11T22:08:57Z,7990,4,"Printing from a service is a bad idea. Network printers are connected ""per-user"". You can mark the service to be run as a particular user, but I'd consider that a bad security practice. You might be able to connect to a local printer, but I'd still hesitate before going this route. The best option is to have the service store the data and have a user-launched application do the printing by asking the service for the data. Or a common location that the data is stored, like a database. If you need to have the data printed as regular intervals, setup a Task event thru the Task Scheduler. Launching a process from a service will require knowing the user name and password, which again is bad security practice. As for the printing itself, use a third-party tool to generate the report will be the easiest."
8330,1059.0,2008-08-11T23:13:28Z,3150,1,I've used CppUnit with VS2005 and Eclipse. The wiki is very thorough (especially if you are familiar with JUnit).
8345,234.0,2008-08-11T23:45:25Z,3150,1,"I'm not 100% sure about VS2008, but I know that the Unit Testing framework that microsoft shipped in VS2005 as part of their Team Suite was only for .NET, not C++ I've used CppUnit also and it was alright. Much the same as NUnit/JUnit/so on. If you've used boost, they also have a unit testing library The guys behind boost have some serious coding chops, so I'd say their framework should be pretty good, but it might not be the most user friendly :-)"
8421,785.0,2008-08-12T02:37:15Z,6430,-2,"Aha, I was really just testing everyone once again!  :)  The real answer is, you rarely need to iterate the datagrid.  Because even when binding to an ArrayList, the binding is 2 way.  Still, it is handy to know how to itereate the grid directly, it can save a few lines of code now and then. But NotMyself and Orion gave the better answers:  Convince the stakeholders to move up to a higher version of C#, to save development costs and increase maintainability and extensability."
8598,834.0,2008-08-12T08:05:53Z,7720,7,"Maybe you should take a look at IzPack . I created a very nice installer some years ago and I'd bet that they are still improving it. It allows the installation of docs, binaries and a clickable link to start the application IIRC ."
8602,872.0,2008-08-12T08:15:51Z,7720,3,Have you considered writing a small program in C/C++ that just calls CreateProcess to start up the java VM with the jar (or class) file? You could get Visual C++ Express and put together the startup program pretty easily.  This would make it easy to add a friendly icon as well.
8620,998.0,2008-08-12T08:55:33Z,7720,0,"Another option I was considering: rather than writing a native launcher from scratch, Eclipse comes with the source code for its own launcher, and this could perhaps be repurposed for my app. It's a shame that Sun never included anything similar in the JDK."
8805,1109.0,2008-08-12T13:14:50Z,8790,33,"I've got something similar set up: I have a main Ant build.xml which calls a separate build.xml that takes care of building my tests. This is how I do it: <target name=""build-tests"">
    <subant target=""build"">
      <fileset dir=""${test.home}"" includes=""build.xml""/>
    </subant>
</target> The trick is to use subant instead of antcall . You don't have to import the other build file."
8810,207.0,2008-08-12T13:20:39Z,8790,6,"Try using the ""ant"" task instead of the ""antcall"" task, which runs the imported build directly instead of importing it into the current build file. It has a ""dir"" parameter: the directory to use as a basedir
  for the new Ant project. Defaults to
  the current project's basedir, unless
  inheritall has been set to false, in
  which case it doesn't have a default
  value. This will override the basedir
  setting of the called project. So you could do: <ant antfile=""${baseDirUpOne}/utils/build/build.xml"" dir=""../utils/build"" /> or something like that."
8811,194.0,2008-08-12T13:20:55Z,8800,127,"There is an actual Data Type called KeyValuePair, use like this KeyValuePair<string, string> myKeyValuePair = new KeyValuePair<string,string>(""defaultkey"", ""defaultvalue"");"
8813,267.0,2008-08-12T13:22:01Z,8800,1,"Dictionary Class is exactly what you want, correct. You can declare the field directly as Dictionary, instead of IDictionary, but that's up to you."
8814,42.0,2008-08-12T13:23:27Z,8800,5,"There is a KeyValuePair built-in type. As a matter of fact, this is what the IDictionary is giving you access to when you iterate in it. Also, this structure is hardly a tree, finding a more representative name might be a good exercise."
8816,372.0,2008-08-12T13:25:12Z,8800,12,"One possible thing you could do is use the Dictionary object straight out of the box and then just extend it with your own modifications: public class TokenTree : Dictionary<string, string>
{
    public IDictionary<string, string> SubPairs;
} This gives you the advantage of not having to enforce the rules of IDictionary for your Key (e.g., key uniqueness, etc). And yup you got the concept of the constructor right :)"
8818,267.0,2008-08-12T13:25:56Z,8800,2,"@ Jay Mooney : A generic Dictionary class in .NET is actually a hash table, just with fixed types. The code you've shown shouldn't convince anyone to use Hashtable instead of Dictionary, since both code pieces can be used for both types. For hashtable: foreach(object key in h.keys)
{
     string keyAsString = key.ToString(); // btw, this is unnecessary
     string valAsString = h[key].ToString();

     System.Diagnostics.Debug.WriteLine(keyAsString + "" "" + valAsString);
} For dictionary: foreach(string key in d.keys)
{
     string valAsString = d[key].ToString();

     System.Diagnostics.Debug.WriteLine(key + "" "" + valAsString);
} And just the same for the other one with KeyValuePair, just use the non-generic version for Hashtable, and the generic version for Dictionary. So it's just as easy both ways, but Hashtable uses Object for both key and value, which means you will box all value types, and you don't have type safety, and Dictionary uses generic types and is thus better."
8826,1065.0,2008-08-12T13:38:47Z,8800,1,"Use something like this: class Tree < T > : Dictionary < T, IList< Tree < T > > >  
{  
} It's ugly, but I think it will give you what you want. Too bad KeyValuePair is sealed."
8833,832.0,2008-08-12T13:45:50Z,8800,3,"Just one thing to add to this (although I do think you have already had your question answered by others). In the interests of extensibility (since we all know it will happen at some point) you may want to check out the Composite Pattern This is ideal for working with ""Tree-Like Structures"".. Like I said, I know you are only expecting one sub-level, but this could really be useful for you if you later need to extend ^_^"
8834,1120.0,2008-08-12T13:45:53Z,8800,7,"I think what you might be after (as a literal implementation of you question) is: pubic class TokenTree
{
    public TokenTree()
    {
        tree = new Dictionary<string, IDictionary<string,string>>();
    }

    IDictionary<string, IDictionary<string, string>> tree; 
} You did actually say a ""list"" of key-values in your question so you might want to swap the inner IDictionary with a: IList<KeyValuePair<string, string>>"
8881,672.0,2008-08-12T14:27:10Z,8880,3,"Answered my own question: Use the NetServerEnum function, passing in the SV_TYPE_DOMAIN_ENUM constant for the ""servertype"" argument. In Delphi, the code looks like this: <snip>
type
  NET_API_STATUS = DWORD;
  PSERVER_INFO_100 = ^SERVER_INFO_100;
  SERVER_INFO_100 = packed record
    sv100_platform_id : DWORD;
    sv100_name        : PWideChar;
end;

function NetServerEnum(  //get a list of pcs on the network (same as DOS cmd ""net view"")
  const servername    : PWideChar;
  const level         : DWORD;
  const bufptr        : Pointer;
  const prefmaxlen    : DWORD;
  const entriesread   : PDWORD;
  const totalentries  : PDWORD;
  const servertype    : DWORD;
  const domain        : PWideChar;
  const resume_handle : PDWORD
) : NET_API_STATUS; stdcall; external 'netapi32.dll';

function NetApiBufferFree(  //memory mgmt routine
  const Buffer : Pointer
) : NET_API_STATUS; stdcall; external 'netapi32.dll';

const
  MAX_PREFERRED_LENGTH = DWORD(-1);
  NERR_Success = 0;
  SV_TYPE_ALL  = $FFFFFFFF;
  SV_TYPE_DOMAIN_ENUM = $80000000;


function TNetwork.ComputersInDomain: TStringList;
var
  pBuffer        : PSERVER_INFO_100;
  pWork          : PSERVER_INFO_100;
  dwEntriesRead  : DWORD;
  dwTotalEntries : DWORD;
  i              : integer;
  dwResult       : NET_API_STATUS;
begin
  Result := TStringList.Create;
  Result.Clear;

  dwResult := NetServerEnum(nil,100,@pBuffer,MAX_PREFERRED_LENGTH,
                            @dwEntriesRead,@dwTotalEntries,SV_TYPE_DOMAIN_ENUM,
                            PWideChar(FDomainName),nil);

  if dwResult = NERR_SUCCESS then begin
    try
      pWork := pBuffer;
      for i := 1 to dwEntriesRead do begin
        Result.Add(pWork.sv100_name);
        inc(pWork);
      end;  //for i
    finally
      NetApiBufferFree(pBuffer);
    end;  //try-finally
  end  //if no error
  else begin
    raise Exception.Create('Error while retrieving computer list from domain ' +
                           FDomainName + #13#10 +
                           SysErrorMessage(dwResult));
  end;
end;
<snip>"
8888,1115.0,2008-08-12T14:33:20Z,8880,1,"You will need to use some LDAP queries Here is some code I have used in a previous script (it was taken off the net somewhere, and I've left in the copyright notices) ' This VBScript code gets the list of the domains contained in the 
' forest that the user running the script is logged into

' ---------------------------------------------------------------
' From the book ""Active Directory Cookbook"" by Robbie Allen
' Publisher: O'Reilly and Associates
' ISBN: 0-596-00466-4
' Book web site: http://rallenhome.com/books/adcookbook/code.html
' ---------------------------------------------------------------

set objRootDSE = GetObject(""LDAP://RootDSE"")
strADsPath =  ""<GC://"" & objRootDSE.Get(""rootDomainNamingContext"") & "">;""
strFilter  = ""(objectcategory=domainDNS);""
strAttrs   = ""name;""
strScope   = ""SubTree""

set objConn = CreateObject(""ADODB.Connection"")
objConn.Provider = ""ADsDSOObject""
objConn.Open ""Active Directory Provider""
set objRS = objConn.Execute(strADsPath & strFilter & strAttrs & strScope)
objRS.MoveFirst
while Not objRS.EOF
    Wscript.Echo objRS.Fields(0).Value
    objRS.MoveNext
wend Also a C# version"
8935,79.0,2008-08-12T15:01:21Z,8830,1,"Copy any applicable preferences files in ~/Library/Preferences from a machine that you have checked ""Don't show again"" on."
8964,91.0,2008-08-12T15:27:35Z,8950,1,"Unfortunately, the ""full"" Sharepoint Experience is limited to running Internet Explorer 6/7 and Office 2007. On the Mac, I recommend using Firefox (Camino?) which seems to work a bit better than Safari. Edit: When you say ""Similar experience"", what exactly are you missing? I don't have any Mac here, but I was under the impression that Office 2008 will have a working integration with Sharepoint as well."
8977,91.0,2008-08-12T15:39:32Z,8970,4,"I don't have a Mac here, but there is some Documentation up here: http://developer.apple.com/documentation/Hardware/Conceptual/iSightProgGuide/01introduction/chapter_1_section_1.html It looks like you have to go through the QuickTime API. There is supposed to be a Sample Project called ""MungGrab"" which could be worth a look according to this thread ."
8996,17.0,2008-08-12T15:56:13Z,8940,2,"I believe you can set the desired solution up by rerunning the vmware configuration script.  And doing a custom network setup, so that both NIC's are mapped to your vmware instance.  I would recommend making eth0 the 2nd NIC since it will be easier for Linux to use by default.  Then make eth1 the 1st NIC."
9095,745.0,2008-08-12T18:13:37Z,7260,87,"If you have SSH installed, you should be able to run.. ssh-keygen Then go through the steps, you'll have two files, id_rsa and id_rsa.pub (the first is your private key, the second is your public key - the one you copy to remote machines) Then, connect to the remote machine you want to login to, to the file ~/.ssh/authorized_keys add the contents of your that id_rsa.pub file. Oh, and chmod 600 all the id_rsa* files (both locally and remote), so no other users can read them: chmod 600 ~/.ssh/id_rsa* Similarly, ensure the remote ~/.ssh/authorized_keys file is chmod 600 also: chmod 600 ~/.ssh/authorized_keys Then, when you do ssh remote.machine , it should ask you for the key's password, not the remote machine. To make it nicer to use, you can use ssh-agent to hold the decrypted keys in memory - this means you don't have to type your keypair's password every single time. To launch the agent, you run (including the back-tick quotes, which eval the output of the ssh-agent command) `ssh-agent` On some distros, ssh-agent is started automatically. If you run echo $SSH_AUTH_SOCK and it shows a path (probably in /tmp/) it's already setup, so you can skip the previous command. Then to add your key, you do ssh-add ~/.ssh/id_rsa and enter your passphrase. It's stored until you remove it (using the ssh-add -D command, which removes all keys from the agent)"
9128,745.0,2008-08-12T18:53:01Z,8970,0,"Aside from ObjC, you can use the PyObjC or RubyCocoa bindings to access it also. If you're not picky about which language, I'd say use Ruby, as PyObjC is horribly badly documented (even the official Apple page on it refers to the old version, not the one that came with OS X Leopard) Quartz Composer is probably the easiest way to access it, and .quartz files can be embed in applications pretty easily (and the data piped out to ObjC or such) Also, I suppose there should be an example or two of this in the /Developer/Examples/"
9139,925.0,2008-08-12T19:05:03Z,4670,1,"I agree with basszero. I'm using mercurial under windows and it's as easy and reliable as it can get. My development team is spread over Europe (well Dublin and Vienna :-).
We use VPN to commit or sometime the built in webserver (hgserve). Both work fine with no problems out of the box. Also diff3 open source tool works perfect with mercurial and TortoiseHG out of the box."
9190,146270.0,2008-08-12T19:49:14Z,8950,1,"Office 2008 allows limited connectivity to MOSS. However there is no Mac OS browser yet that is completely compatible to MOSS. I do have it on good authority the Microsoft Mac BU team is working with the MOSS team to see this changing in future versions of the platform, specifically around the Safari support."
9208,428190.0,2008-08-12T20:08:39Z,7940,1,"My approach tends to be to ensure I can completely validate on all pages, however I still send the page as text/html instead of application/xhtml+xml so there are no ugly XML errors in the event I have missed something."
9230,1063.0,2008-08-12T20:49:23Z,7940,1,"For me, I feel like I've done a good job if my code validates. Seeing the green check box on the w3c pages just makes me slightly giddy. As for group b, They usually only care that it looks and works the same across browsers. They only place I've found that this is not true is the government sector. They require complete validation not only with the w3c but also passing ADA tests (basically how does it sound with a screen reader). p.s. when I say government sector, I mean specifically the state of California and a few counties inside it. I have had no ther experience with other government groups besides them."
9250,960.0,2008-08-12T21:16:50Z,9240,1,"I always create a separate assembly that contains: A lot of small Interfaces (think ICreateRepository, IReadRepository, IReadListRepsitory.. the list goes on and most of them relies heavily on generics) A lot of concrete Interfaces, like an IPersonRepository, that inherits from IReadRepository, you get the point.. Anything you cannot describe with just the smaller interfaces, you put into the concrete interface. As long as you use the IPersonRepository to declare your object, you get a clean, consistent interface to work with. But the kicker is, you can also make a class that takes f.x. a ICreateRepository in its constructor, so the code will end up being very easy to do some really funky stuff with. There are also interfaces for the Services in the business tier here. At last i stick all the domain objects into the extra assembly, just to make the code base itself a bit cleaner and more loosely coupled. These objects dont have any logic, they are just a common way to describe the data for all 3+ layers. Btw. Why would you define methods in the business logic tier to accommodate the data tier? The data tier should have no reason to even know there is a business tier.."
9251,1144.0,2008-08-12T21:18:44Z,9240,0,"It could be a solution, as it would not erode the interface. I guess you could have a class like this: public class BusinessObjectRecord : BusinessObject
{
}"
9257,1144.0,2008-08-12T21:28:05Z,9240,0,"What do you mean by that the data tier should not be aware of the business logic tier? How would you fill an business object with data? I often do this: namespace Data
{
    public class BusinessObjectDataManager
    {
         public void SaveObject(BusinessObject object)
         {
                // Exec stored procedure
         {
    }
}"
9258,230.0,2008-08-12T21:28:09Z,9240,5,"This is a classic problem - separating your domain model from your database model. There are several ways to attack it, it really depends on the size of your project in my opinion. You could use the repository pattern as others have said. If you are using .net or java you could use NHibernate or Hibernate . What I do is use Test Driven Development so I write my UI and Model layers first and the Data layer is mocked, so the UI and model is build around domain specific objects, then later I map these object to what ever technology I'm using the the Data Layer. Is a very bad idea to let the database determine the design of your app, write the app first and think about the data later. ps the title of the question is a little mis-leading"
9264,960.0,2008-08-12T21:34:47Z,9240,1,"@Ice^^Heat: What do you mean by that the data tier should not be aware of the business logic tier? How would you fill an business object with data? The UI asks the ServiceClass in the business tier for a service, namely getting a list of objects filtered by an object with the needed parameter data. Then the ServiceClass creates an instance of one of the repository classes in the data tier, and calls the GetList(ParameterType filters). Then the data tier accesses the database, pulls up the data, and maps it to the common format defined in the ""domain"" assembly. The BL has no more work to do with this data, so it outputs it to the UI. Then the UI wants to edit Item X. It sends the item (or business object) to the service in the Business Tier. The business tier validates the object, and if it is OK, it sends it to the data tier for storage. The UI knows the service in the business tier which again knows about the data tier. The UI is responsible for mapping the users data input to and from the objects, and the data tier is responsible for mapping the data in the db to and from the objects. The Business tier stays purely business. :)"
9277,872.0,2008-08-12T21:52:01Z,9240,0,"So the problem is that the business layer needs to expose more functionality to the data layer, and adding this functionality means exposing too much to the UI layer?  If I'm understanding your problem correctly, it sounds like you're trying to satisfy too much with a single interface, and that's just causing it to become cluttered.  Why not have two interfaces into the business layer?  One would be a simple, safe interface for the UI layer.  The other would be a lower-level interface for the data layer. You can apply this two-interface approach to any objects which need to be passed to both the UI and the data layers, too. public class BusinessLayer : ISimpleBusiness
{}

public class Some3LayerObject : ISimpleSome3LayerObject
{}"
9369,372.0,2008-08-13T01:00:12Z,9240,0,"You may want to split your interfaces into two types, namely: View interfaces -- which are interfaces that specify your interactions with your UI, and Data interfaces -- which are interfaces that will allow you to specify interactions with your data It is possible to inherit and implement both set of interfaces such that: public class BusinessObject : IView, IData This way, in your data layer you only need to see the interface implementation of IData, while in your UI you only need to see the interface implementation of IView. Another strategy you might want to use is to compose your objects in the UI or Data layers such that they are merely consumed by these layers, e.g., public class BusinessObject : DomainObject

public class ViewManager<T> where T : DomainObject

public class DataManager<T> where T : DomainObject This in turn allows your business object to remain ignorant of both the UI/View layer and the data layer."
9411,59.0,2008-08-13T02:18:28Z,9410,1,"You need to pass a function pointer .  The syntax is a little cumbersome, but it's really powerful once you get familiar with it."
9413,184.0,2008-08-13T02:22:24Z,9410,449,"Declaration A prototype for a function which takes a function parameter looks like the following: void func ( void (*f)(int) ); This states that the parameter f will be a pointer to a function which has a void return type and which takes a single int parameter. The following function ( print ) is an example of a function which could be passed to func as a parameter because it is the proper type: void print ( int x ) {
  printf(""%d\n"", x);
} Function Call When calling a function with a function parameter, the value passed must be a pointer to a function. Use the function's name (without parentheses) for this: func(print); would call func , passing the print function to it. Function Body As with any parameter, func can now use the parameter's name in the function body to access the value of the parameter. Let's say that func will apply the function it is passed to the numbers 0-4. Consider, first, what the loop would look like to call print directly: for ( int ctr = 0 ; ctr < 5 ; ctr++ ) {
  print(ctr);
} Since func 's parameter declaration says that f is the name for a pointer to the desired function, we recall first that if f is a pointer then *f is the thing that f points to (i.e. the function print in this case). As a result, just replace every occurrence of print in the loop above with *f : void func ( void (*f)(int) ) {
  for ( int ctr = 0 ; ctr < 5 ; ctr++ ) {
    (*f)(ctr);
  }
} From http://math.hws.edu/bridgeman/courses/331/f05/handouts/c-c++-notes.html"
9421,716.0,2008-08-13T02:34:47Z,9410,81,"This question already has the answer for defining function pointers, however they can get very messy, especially if you are going to be passing them around your application. To avoid this unpleasantness I would recommend that you typedef the function pointer into something more readable. For example. typedef void (*functiontype)(); Declares a function that returns void and takes no arguments. To create a function pointer to this type you can now do: void dosomething() { }

functiontype func = &dosomething;
func(); For a function that returns an int and takes a char you would do typedef int (*functiontype2)(char); and to use it int dosomethingwithchar(char a) { return 1; }

functiontype2 func2 = &dosomethingwithchar
int result = func2('a'); There are libraries that can help with turning function pointers into nice readable types. The boost function library is great and is well worth the effort! boost::function<int (char a)> functiontype2; is so much nicer than the above."
9502,1131.0,2008-08-13T06:08:24Z,9240,7,"If I understand the question correctly, you've created a domain model and you would like to write an object-relational mapper to map between records in your database and your domain objects. However, you're concerned about polluting your domain model with the 'plumbing' code that would be necessary to read and write to your object's fields. Taking a step back, you essentially have two choices of where to put your data mapping code - within the domain class itself or in an external mapping class.
The first option is often called the Active Record pattern and has the advantage that each object knows how to persist itself and has sufficient access to its internal structure to allow it to perform the mapping without needing to expose non-business related fields. E.g public class User
{
	private string name;
	private AccountStatus status;

	private User()
	{
	}

	public string Name
	{
		get { return name; }
		set { name = value; }
	}

	public AccountStatus Status
	{
		get { return status; }
	}

	public void Activate()
	{
		status = AccountStatus.Active;
	}

	public void Suspend()
	{
		status = AccountStatus.Suspended;
	}

	public static User GetById(int id)
	{
		User fetchedUser = new User();

		// Lots of database and error-checking code
		// omitted for clarity
		// ...

		fetchedUser.name = (string) reader[""Name""];
		fetchedUser.status = (int)reader[""statusCode""] == 0 ? AccountStatus.Suspended : AccountStatus.Active;

		return fetchedUser;
	}

	public static void Save(User user)
	{
		// Code to save User's internal structure to database
		// ...
	}
} In this example, we have an object that represents a User with a Name and an AccountStatus. We don't want to allow the Status to be set directly, perhaps because we want to check that the change is a valid status transition, so we don't have a setter. Fortunately, the mapping code in the GetById and Save static methods have full access to the object's name and status fields. The second option is to have a second class that is responsible for the mapping. This has the advantage of seperating out the different concerns of business logic and persistence which can allow your design to be more testable and flexible. The challenge with this method is how to expose the name and status fields to the external class. Some options are:
  1. Use reflection (which has no qualms about digging deep into your object's private parts)
  2. Provide specially-named, public setters (e.g. prefix them with the word 'Private') and hope no one uses them accidentally
  3. If your language suports it, make the setters internal but grant your data mapper module access. E.g. use the InternalsVisibleToAttribute in .NET 2.0 onwards or friend functions in C++ For more information, I'd recommend Martin Fowler's classic book 'Patterns of Enterprise Architecture' However, as a word of warning, before going down the path of writing your own mappers I'd strongly recommend looking at using a 3rd-party object relational mapper (ORM) tool such as nHibernate or Microsoft's Entity Framework. I've worked on four different projects where, for various reasons, we wrote our own mapper and it is very easy to waste a lot of time maintaining and extending the mapper instead of writing code that provides end user value. I've used nHibernate on one project so far and, although it has quite a steep learning curve initially, the investment you put in early on pays off considerably."
9562,,2008-08-13T07:52:51Z,4860,0,Can you let us know that technology you are using as there are some intresting bits out there around this stuff and some short cuts... i.e. WSE2 is complex beast and something that I dont like getting wrong! I dont like developers doing this and there are WSE2 accelorators out there like SSL Accelorates as the processing of encryption has a hugh cost best to take it out of process from the normal code and the development arena. If this is an option for you - Try look at this - ForumSystems
9653,842.0,2008-08-13T11:11:42Z,9650,6,Checkout CLISP wiki-link that ie. was used by Paul Graham Direct link
9655,1065.0,2008-08-13T11:13:43Z,9650,1,Did you try Allegro CL from http://www.franz.com/ ?
9659,188.0,2008-08-13T11:27:55Z,9650,13,"You could also try DrScheme , which whilst not exactly a standalone interpreter, isn't emacs :) It's basically a simple IDE that has an area to type in code that can be executed as a file, and then another area that is the running interpreter that you can interact with. (Also, find the UC Berkeley CS61A podcasts and listen to them, as well as reading SICP)"
9679,279.0,2008-08-13T11:55:48Z,9650,7,It looks like Steel Bank Common Lisp (SBCL) also caters to what you want: http://www.sbcl.org/manual/Shebang-Scripts.html#Shebang-Scripts SBCL is both top rate and open source.
9752,267.0,2008-08-13T13:03:17Z,9750,4,"What specifically does that question mean? Does reverse mean setting 1's to 0's and vice versa? Or does it mean 00001100 --> 00110000 where you reverse their order in the byte? Or perhaps just reversing the part that is from the first 1 to the last 1? ie. 00110101 --> 00101011 ? Assuming it means reversing the bit order in the whole byte, here's an x86 assembler version: ; al is input register
; bl is output register

xor bl, bl      ; clear output

; first bit
rcl al, 1       ; rotate al through carry
rcr bl, 1       ; rotate carry into bl

; duplicate above 2-line statements 7 more times for the other bits not the most optimal solution, a table lookup is faster."
9756,42.0,2008-08-13T13:04:59Z,9750,4,"Reversing the order of bits in C#: byte ReverseByte(byte b)
{
    byte r = 0;
    for(int i=0; i<8; i++)
    {
        int mask = 1 << i;
        int bit = (b & mask) >> i;
        int reversedMask = bit << (7 - i);
        r |= (byte)reversedMask;
    }
    return r;
} I'm sure there are more clever ways of doing it but in that precise case, the interview question is meant to determine if you know bitwise operations so I guess this solution would work. In an interview, the interviewer usually wants to know how you find a solution, what are you problem solving skills, if it's clean or if it's a hack. So don't come up with too much of a clever solution because that will probably mean you found it somewhere on the Internet beforehand. Don't try to fake that you don't know it neither and that you just come up with the answer because you are a genius, this is will be even worst if she figures out since you are basically lying."
9758,1035.0,2008-08-13T13:05:31Z,9750,2,"The classic Bit Hacks page has several (really very clever) ways to do this, but it's all in C. Any language derived from C syntax (notably Java) will likely have similar methods. I'm sure we'll get some Haskell versions in this thread ;)"
9778,1190.0,2008-08-13T13:15:42Z,9750,2,"byte ReverseByte(byte b)
  {
      return b ^ 0xff;
  } That works if ^ is XOR in your language, but not if it's AND , which it often is."
9840,37.0,2008-08-13T13:54:10Z,4430,9,I have used NuSOAP in the past.  I liked it because it is just a set of PHP files that you can include.  There is nothing to install on the web server and no config options to change.  It has WSDL support as well which is a bonus.
9870,1219.0,2008-08-13T14:16:13Z,2840,0,"Here's what I do for paging:  All of my big queries that need to be paged are coded as inserts into a temp table.  The temp table has an identity field that will act in a similar manner to the row_number() mentioned above.  I store the number of rows in the temp table in an output parameter so the calling code knows how many total records there are.  The calling code also specifies which page it wants, and how many rows per page, which are selected out from the temp table. The cool thing about doing it this way is that I also have an ""Export"" link that allows you to get all rows from the report returned as CSV above every grid in my application.  This link uses the same stored procedure: you just return the contents of the temp table instead of doing the paging logic.  This placates users who hate paging, and want to see everything , and want to sort it in a million different ways."
9907,737.0,2008-08-13T14:44:10Z,9750,14,"What specifically does that question mean? Good question.  If reversing the ""ON"" bits means reversing only the bits that are ""ON"", then you will always get 0, no matter what the input is.  If it means reversing all the bits, i.e. changing all 1s to 0s and all 0s to 1s, which is how I initially read it, then that's just a bitwise NOT, or complement.  C-based languages have a complement operator, ~ , that does this.  For example: unsigned char b = 102;      /* 0x66, 01100110 */
unsigned char reverse = ~b; /* 0x99, 10011001 */"
9929,521.0,2008-08-13T15:11:45Z,9750,3,"If you're talking about switching 1's to 0's and 0's to 1's, using Ruby: n = 0b11001100
~n If you mean reverse the order: n = 0b11001100
eval(""0b"" + n.to_s(2).reverse) If you mean counting the on bits, as mentioned by another user: n = 123
count = 0
0.upto(8) { |i| count = count + n[i] }  Ruby"
9963,1219.0,2008-08-13T15:41:44Z,580,1,"I agree with keeping everything in source control and manually scripting all changes.  Changes to the schema for a single release go into a script file created specifically for that release.  All stored procs, views, etc should go into individual files and treated just like .cs or .aspx as far as source control goes.  I use a powershell script to generate one big .sql file for updating the programmability stuff. I don't like automating the application of schema changes, like new tables, new columns, etc.  When doing a production release, I like to go through the change script command by command to make sure each one works as expected.  There's nothing worse than running a big change script on production and getting errors because you forgot some little detail that didn't present itself in development. I have also learned that indexes need to be treated just like code files and put into source control. And you should definitely have more than 2 databases - dev and live.  You should have a dev database that everybody uses for daily dev tasks.  Then a staging database that mimics production and is used to do your integration testing.  Then maybe a complete recent copy of production (restored from a full backup), if that is feasible, so your last round of installation testing goes against something that is as close to the real thing as possible."
9975,1109.0,2008-08-13T15:49:52Z,8970,1,There's a command line utility called isightcapture that does more or less what you want to do. You could probably get the code from the developer (his e-mail address is in the readme you get when you download the utility).
10008,1109.0,2008-08-13T16:09:09Z,80,6,"The SQLite API has a function called something like sqlite_prepare which takes one statement and prepares it for execution, essentially parsing the SQL and storing it in memory. This means that the SQL only has to be sent once to the database engine even though the statement is executed many times. Anyway, a statement is a single SQL query, that's just the rule. The AIR SQL API doesn't allow sending raw SQL to SQLite, only single statements, and the reason is, likely, that AIR uses the sqlite_prepare function when it talks to SQLite."
10105,1190.0,2008-08-13T17:25:21Z,3180,6,"I think it's great that Joel et al. let people use FogBugs hosted for free on their own.  It's a great business strategy, because the users become fans (it is great software after all), and then they recommend it to their businesses or customers."
10130,1236.0,2008-08-13T17:57:13Z,9750,1,"I'm probably misremembering, but I thought that Joel's question was about counting the ""on"" bits rather than reversing them."
10138,414.0,2008-08-13T18:07:03Z,9570,1,"As a bare minimum: mscoree.lib
MSVCRT.lib
mfc90.lib (adjust version appropriately) And iterate from there."
10141,245.0,2008-08-13T18:10:34Z,3230,16,"Choose Project -> Properties Select Configuration -> General In the box for how you should link MFC, choose to statically link it. Choose Linker -> Input.  Under Additional Dependencies , add any libraries you need your app to statically link in. For more info, see this article: http://www.geekadmin.com/?p=34"
10160,1233.0,2008-08-13T18:19:07Z,2630,2,"While it is not as fun as Out-Twitter, my favorite cmdlet is Get-Member, since it allows me to examine any of the objects I'm working with and find out new properties and methods, as well as the underlying type of the object. If I did not choose Get-Member, I would have to go with Out-Clipboard from the PowerShell Community Extensions (PSCX), as it enables a whole lot of clipboard automation and makes using PowerShell for code templating much easier."
10194,91.0,2008-08-13T18:40:32Z,10190,8,"On MS SQL Server 2005 and above, ROW_NUMBER() seems to work: T-SQL: Paging with ROW_NUMBER() DECLARE @PageNum AS INT;
DECLARE @PageSize AS INT;
SET @PageNum = 2;
SET @PageSize = 10;

WITH OrdersRN AS
(
    SELECT ROW_NUMBER() OVER(ORDER BY OrderDate, OrderID) AS RowNum
          ,OrderID
          ,OrderDate
          ,CustomerID
          ,EmployeeID
      FROM dbo.Orders
)

SELECT * 
  FROM OrdersRN
 WHERE RowNum BETWEEN (@PageNum - 1) * @PageSize + 1 
                  AND @PageNum * @PageSize
 ORDER BY OrderDate
         ,OrderID;"
10197,380.0,2008-08-13T18:43:17Z,10190,1,"Actually, LINQ has Skip and Take methods which can be combined to choose which records are fetched. Check those out. For DB: Pagination In SQL Server 2005"
10227,116.0,2008-08-13T19:00:32Z,10190,1,"Oracle Solution: select * from (
    select a.*, rownum rnum from (
        YOUR_QUERY_GOES_HERE -- including the order by
    ) a
    where rownum <= MAX_ROW
 ) where rnum >= MIN_ROW"
10233,414.0,2008-08-13T19:05:42Z,10230,15,"Yes, it depends on language, since string storage differs between languages. Pascal-type strings: Length = 0 . C-style strings: [0] == 0 . .NET: .IsNullOrEmpty . Etc."
10234,905.0,2008-08-13T19:05:54Z,10230,2,"In .Net: string.IsNullOrEmpty( nystr ); strings can be null, so .Length sometimes throws a NullReferenceException"
10235,380.0,2008-08-13T19:06:38Z,10230,0,"Actually, IMO the best way to determine is the IsNullOrEmpty() method of the string class. http://msdn.microsoft.com/en-us/library/system.string.isnullorempty. Update: I assumed .Net, in other languages, this might be different."
10250,872.0,2008-08-13T19:12:19Z,10230,13,"In languages that use C-style (null-terminated) strings, comparing to """" will be faster.  That's an O(1) operation, while taking the length of a C-style string is O(n). In languages that store length as part of the string object (C#, Java, ...) checking the length is also O(1).  In this case, directly checking the length is faster, because it avoids the overhead of constructing the new empty string."
10271,398.0,2008-08-13T19:23:07Z,10260,3,"I'm still learning ASP.net so I can't tell you exactly, but if you look through http://www.asp.net/learn/ you'll probably find a few new gems, there's even a 3.5 section."
10277,1208.0,2008-08-13T19:26:42Z,10260,5,Check out the MVC framework which is built ontop of 3.5.  Big improvement over the traditional webforms model.
10281,1228.0,2008-08-13T19:28:09Z,10260,1,"Its the MVC framework .  Without 3.5, there is no MVC.  Without MVC, ASP.NET is a PITA."
10282,1016.0,2008-08-13T19:29:28Z,10190,6,"I'd recommend either using LINQ, or try to copy what it does. I've got an app where I use the LINQ Take and Skip methods to retrieve paged data. The code looks something like this: MyDataContext db = new MyDataContext();
var results = db.Products
    .Skip((pageNumber - 1) * pageSize)
    .Take(pageSize); Running SQL Server Profiler reveals that LINQ is converting this query into SQL similar to: SELECT [ProductId], [Name], [Cost], and so on...
FROM (
    SELECT [ProductId], [Name], [Cost], [ROW_NUMBER]
    FROM (
       SELECT ROW_NUMBER() OVER (ORDER BY [Name]) AS [ROW_NUMBER], 
           [ProductId], [Name], [Cost]
       FROM [Products]
    )
    WHERE [ROW_NUMBER] BETWEEN 10 AND 20
)
ORDER BY [ROW_NUMBER] In plain English: 1. Filter your rows and use the ROW_NUMBER function to add row numbers in the order you want. 2. Filter (1) to return only the row numbers you want on your page. 3. Sort (2) by the row number, which is the same as the order you wanted (in this case, by Name)."
10283,543.0,2008-08-13T19:30:28Z,10230,1,"In Java 1.6, the String class has a new method isEmpty There is also the Jakarta commons library, which has the isBlank method. Blank is defined as a string that contains only whitespace."
10294,1035.0,2008-08-13T19:35:37Z,10230,0,"In this case, directly checking the length is faster, because it avoids the overhead of constructing the new empty string. @DerekPark: That's not always true. """" is a string literal so, in Java, it will almost certainly already be interned."
10305,116.0,2008-08-13T19:51:33Z,10230,0,"For C strings, if (s[0] == 0) will be faster than either if (strlen(s) == 0) or if (strcmp(s, """") == 0) because you will avoid the overhead of a function call."
10306,1084.0,2008-08-13T19:52:16Z,10230,2,"In languages that use C-style (null-terminated) strings, comparing to """" will be faster Actually, it may be better to check if the first char in the string is '\0': char *mystring;
/* do something with the string */
if ((mystring != NULL) && (mystring[0] == '\0')) {
    /* the string is empty */
} In Perl there's a third option, that the string is undefined.  This is a bit different from a NULL pointer in C, if only because you don't get a segmentation fault for accessing an undefined string."
10311,76.0,2008-08-13T19:59:49Z,10300,2,"Have you taken a look at this? http://www.codeplex.com/MvcValidatorToolkit Quoted from the page The Validator Toolkit provides a set
  of validators for the new ASP.NET MVC
  framework to validate HTML forms on
  the client and server-side using
  validation sets. I'm afraid that someone more MVC-savvy than me would have to speak to where in the architecture you should put things."
10312,872.0,2008-08-13T20:00:55Z,10230,0,"@Nathan Actually, it may be better to check if the first char in the string is '\0': I almost mentioned that, but ended up leaving it out, since calling strcmp() with the empty string and directly checking the first character in the string are both O(1).  You basically just pay for an extra function call, which is pretty cheap.  If you really need the absolute best speed, though, definitely go with a direct first-char-to-0 comparison. Honestly, I always use strlen() == 0 , because I have never written a program where this was actually a measurable performance issue, and I think that's the most readable way to express the check."
10328,203.0,2008-08-13T20:22:25Z,10260,0,"I don't think the MVC Framework is quite ready for prime time yet, though I definitely plan to use it sometime next year.  I love the clean URLs, clean XHTML (web forms can really spew out some nasty HTML) and the ability to create controller actions with no associated view. I've been using Master Pages since they were released and they've been a big help.  I do really dislike the way the master pages add the nasty prefixes to the control IDs.  It makes for some ugly CSS.  I think the MVC Framework may eliminate this problem though. Any other killer features?"
10364,1122.0,2008-08-13T20:58:00Z,10300,0,"I'm just learning the MVC framework too so I'm not sure how off this is, but from what I understand you would have a form on a View such as Edit.aspx.  This form would then post to the controller to another action method such as Update() passing in the contents of the form that you set in Edit.aspx as parameters. Update(int id, string name, string foo) You could do the validation within that method.  If all is ok, return View(""Item"", yourObject)"
10367,337.0,2008-08-13T21:03:54Z,10260,2,"ListView and its friend DataPager are probably worth looking at, but they're hardly ""Killer"" features. Things outside of ASP.NET specifically (LINQ, for example) are probably more likely to be get the ""Killer"" commendation."
10379,1122.0,2008-08-13T21:12:48Z,10260,1,"Master Pages (of course, these are in there from
  version 2.0) Nested master pages are new in 3.5.  I haven't used them yet, but I can only imagine they could turn into a hidious nightmare if not used very carefully. You only have to look at the order in which the events are fired in a page that uses a master page to think 'urgh'."
10382,1122.0,2008-08-13T21:15:14Z,10260,0,"The split design/code view is pretty cool.  It's not perfect yet, but it's pretty cool.  Also editing in the design view now edits your css there and then."
10405,1220.0,2008-08-13T21:51:47Z,10260,1,"I don't think the MVC Framework is quite ready for prime time yet Just an FYI, this site is built in MVC.
I also have 2 apps in production on mvc, I would argue that its definitely ready for prime time."
10448,1219.0,2008-08-13T22:45:03Z,9240,0,"I'm going to continue my habit of going against the grain and say that you should question why you are building all these horribly complex object layers. I think many developers think of the database as a simple persistence layer for their objects, and are only concerned with the CRUD operations that those objects need.  Too much effort is being put into the ""impedence mismatch"" between object and relational models.  Here's an idea: stop trying. Write stored procedures to encapsulate your data.  Use results sets, DataSet, DataTable, SqlCommand (or the java/php/whatever equivalent) as needed from code to interact with the database.  You don't need those objects.  An excellent example is embedding a SqlDataSource into a .ASPX page. You shouldn't try to hide your data from anyone.  Developers need to understand exactly how and when they are interacting with the physical data store. Object-relational mappers are the devil.  Stop using them. Building enterprise applications is often an exercise in managing complexity.  You have to keep things as simple as possible, or you will have an absolutely un-maintainable system.  If you are willing to allow some coupling (which is inherent in any application anyway), you can do away with both your business logic layer and your data access layer (replacing them with stored procedures), and you won't need any of those interfaces."
10467,571.0,2008-08-13T23:15:32Z,10300,12,"Here's an overview of the flow in MVC: /new - render your ""New"" view containing a form for the user to fill out User fills out form and it is posted to /create The post is routed to the Create action on your controller In your action method, update the model with the data that was posted. Your Model should validate itself. Your Controller should read if the model is valid. If the Model is valid, save it to your db.  Redirect to /show to render the show View for your object. If the Model is invalid, save the form values and error messages in the TempData, and redirect to the New action again.  Fill your form fields with the data from TempData and show the error message(s). The validation frameworks will help you along in this process.  Also, I think the ASP.NET MVC team is planning a validation framework for the next preview."
10473,571.0,2008-08-13T23:25:05Z,10260,1,"@IainMH  Nested Master Pages were always supported by ASP.NET, just not by the designer."
10526,1042.0,2008-08-14T00:32:25Z,9570,1,Use the AppWizard to create a bare-bones MFC app in your style (SDI / MDI / dialog ) and then put on your depends .
10536,13.0,2008-08-14T00:45:14Z,9650,1,"@Nathan: I've upmodded the Common Lisp links, because you asked about Lisp (especially with reference to Emacs Lisp). However, Common Lisp is very different from Scheme. A program written for one is unlikely to run on the other. As you mentioned, SICP is for learning Scheme, not Lisp (or at least, not Common Lisp and not Emacs Lisp). There are some overlap in principles, however you can't simply cut and paste code from SICP and expect it to run on any Common Lisp or Emacs Lisp system. :-)"
10540,859.0,2008-08-14T00:49:17Z,9650,0,"Another good dialect of lisp is cmucl .  They used to love to brag about being the ""fastest"" lisp."
10557,1190.0,2008-08-14T01:29:42Z,3510,5,"I don't have the power to edit @Michael Stum's answer , but it's not quite correct.  He reduces (i + 4) - (a + b) to (i + 4 - a + b) They are not equivalent.  The best reduction I can get for the whole expression is ((i + 4) - (a + b)) * MAGIC_NUMBER - ANOTHER_MAGIC_NUMBER; or (i + 4 - a - b) * MAGIC_NUMBER - ANOTHER_MAGIC_NUMBER;"
10561,13.0,2008-08-14T01:35:32Z,9750,0,"Since the question asked for a non-C way, here's a Scheme implementation, cheerfully plagiarised from SLIB : (define (bit-reverse k n)
  (do ((m (if (negative? n) (lognot n) n) (arithmetic-shift m -1))
       (k (+ -1 k) (+ -1 k))
       (rvs 0 (logior (arithmetic-shift rvs 1) (logand 1 m))))
      ((negative? k) (if (negative? n) (lognot rvs) rvs))))

(define (reverse-bit-field n start end)
  (define width (- end start))
  (let ((mask (lognot (ash -1 width))))
    (define zn (logand mask (arithmetic-shift n (- start))))
    (logior (arithmetic-shift (bit-reverse width zn) start)
            (logand (lognot (ash mask start)) n)))) Rewritten as C (for people unfamiliar with Scheme), it'd look something like this (with the understanding that in Scheme, numbers can be arbitrarily big): int
bit_reverse(int k, int n)
{
    int m = n < 0 ? ~n : n;
    int rvs = 0;
    while (--k >= 0) {
        rvs = (rvs << 1) | (m & 1);
        m >>= 1;
    }
    return n < 0 ? ~rvs : rvs;
}

int
reverse_bit_field(int n, int start, int end)
{
    int width = end - start;
    int mask = ~(-1 << width);
    int zn = mask & (n >> start);
    return (bit_reverse(width, zn) << start) | (~(mask << start) & n);
}"
10563,13.0,2008-08-14T01:43:01Z,9750,2,"And here's a version directly cut and pasted from OpenJDK , which is interesting because it involves no loop. On the other hand, unlike the Scheme version I posted, this version only works for 32-bit and 64-bit numbers. :-) 32-bit version: public static int reverse(int i) {
    // HD, Figure 7-1
    i = (i & 0x55555555) << 1 | (i >>> 1) & 0x55555555;
    i = (i & 0x33333333) << 2 | (i >>> 2) & 0x33333333;
    i = (i & 0x0f0f0f0f) << 4 | (i >>> 4) & 0x0f0f0f0f;
    i = (i << 24) | ((i & 0xff00) << 8) |
        ((i >>> 8) & 0xff00) | (i >>> 24);
    return i;
} 64-bit version: public static long reverse(long i) {
    // HD, Figure 7-1
    i = (i & 0x5555555555555555L) << 1 | (i >>> 1) & 0x5555555555555555L;
    i = (i & 0x3333333333333333L) << 2 | (i >>> 2) & 0x3333333333333333L;
    i = (i & 0x0f0f0f0f0f0f0f0fL) << 4 | (i >>> 4) & 0x0f0f0f0f0f0f0f0fL;
    i = (i & 0x00ff00ff00ff00ffL) << 8 | (i >>> 8) & 0x00ff00ff00ff00ffL;
    i = (i << 48) | ((i & 0xffff0000L) << 16) |
        ((i >>> 16) & 0xffff0000L) | (i >>> 48);
    return i;
}"
10581,1053.0,2008-08-14T02:16:32Z,10580,4,"Taken directly from http://word.mvps.org/fAQs/InterDev/EarlyvsLateBinding.htm There are two ways to use Automation (or OLE Automation) to
  programmatically control another application. Late binding uses CreateObject to create and instance of the
  application object, which you can then control. For example, to create
  a new instance of Excel using late binding: Dim oXL As Object
 Set oXL = CreateObject(""Excel.Application"") On the other hand, to manipulate an existing instance of Excel (if
  Excel is already open) you would use GetObject (regardless whether
  you're using early or late binding): Dim oXL As Object
 Set oXL = GetObject(, ""Excel.Application"") To use early binding, you first need to set a reference in your
  project to the application you want to manipulate. In the VB Editor of
  any Office application, or in VB itself, you do this by selecting
  Tools + References, and selecting the application you want from the
  list (e.g. Microsoft Excel 8.0 Object Library). To create a new instance of Excel using early binding: Dim oXL As Excel.Application
 Set oXL = New Excel.Application In either case, incidentally, you can first try to get an existing
  instance of Excel, and if that returns an error, you can create a new
  instance in your error handler."
10582,1190.0,2008-08-14T02:16:42Z,10580,12,"In compiled languages, the difference is stark. Java: //early binding:
public create_a_foo(*args) {
 return new Foo(args)
}
my_foo = create_a_foo();

//late binding:
public create_something(Class klass, *args) {
  klass.new_instance(args)
}
my_foo = create_something(Foo); In the first example, the compiler can do all sorts of neat stuff at compile time.  In the second, you just have to hope that whoever uses the method does so responsibly.  (Of course, newer JVMs support the Class<? extends Foo> klass structure, which can greatly reduce this risk.) Another benefit is that IDEs can hotlink to the class definition, since it's declared right there in the method.  The call to create something(Foo) might be _very far from the method definition, and if you're looking at the method definition, it might be nice to see the implementation. The major advantage of late binding is that it makes things like inversion-of-control easier, as well as certain other uses of polymorphism and duck-typing (if your language supports such things)."
10584,1190.0,2008-08-14T02:21:56Z,10580,2,"In interpreted languages, the difference is a little more subtle. Ruby: # early binding:
def create_a_foo(*args)
  Foo.new(*args)
end
my_foo = create_a_foo

# late binding:
def create_something(klass, *args)
  klass.new(*args)
end
my_foo = create_something(Foo) Because Ruby is (generally) not compiled, there isn't a compiler to do the nifty up-front stuff.  The growth of JRuby means that more Ruby is compiled these days, though, making it act more like Java, above. The issue with IDEs still stands: a platform like Eclipse can look up class definitions if you hard-code them, but cannot if you leave them up to the caller. Inversion-of-control is not terribly popular in Ruby, probably because of its extreme runtime flexibility, but Rails makes great use of late binding to reduce the amount of configuration necessary to get your application going."
10587,202.0,2008-08-14T02:24:04Z,10580,39,The short answer is that early (or static) binding refers to compile time binding and late (or dynamic) binding refers to runtime binding (for example when you use reflection).
10602,1219.0,2008-08-14T02:56:51Z,10600,0,"Just to get this started, I am using a Dell MD3000 direct attached storage device, connected via redundant HBA cards.  It has 9x146Gb 15K drives, arranged in 4 RAID 1 arrays with 1 hot spare standing by.  Total data footprint is approaching 200Gb.  I'm not thrilled with the IO performance, but it's getting the job done."
10614,1.0,2008-08-14T03:10:22Z,10610,2,"Besides being pretty much the best Regex tool on the market (seriously), RegexBuddy is about the only tool I know of that lets you switch amongst different Regex rendering engines. http://www.regexbuddy.com/ See info here: http://en.wikipedia.org/wiki/RegexBuddy RegexBuddy's proprietary regular expression engine allows the software to emulate the rules and limitations of numerous popular regular expression flavors."
10619,522.0,2008-08-14T03:15:59Z,10610,5,"Boost, for c++"
10631,872.0,2008-08-14T03:25:26Z,10610,1,"Lately, I do all my text parsing in Perl.  If I needed regex's in another language, I'd go with PCRE . The PCRE library is a set of functions that implement regular
     expression pattern matching using the same syntax and semantics as
     Perl5. PCRE has its own native API, as well as a set of
     wrapper functions that correspond to the POSIX regular expression
     API. The PCRE library is free, even for building commercial
     software. PCRE was originally written for the Exim MTA ,
     but is now used by many high-profile open source projects, including Apache , PHP , KDE , Postfix , Analog , and Nmap .
     PCRE has also found its way into some well known commercial products, like Apple Safari .
     Some other interesting projects using PCRE include Chicken , Ferite , Onyx , Hypermail , Leafnode , Askemos ,
     and Wenlin . PCRE is mature, and has the support of numerous projects.  Apache and Apple both have a vested interest in making it high-quality.  I doubt that any other RE library is likely to surpass it in both functionality and quality (or possibly either) anytime soon."
10646,718.0,2008-08-14T03:53:27Z,10610,4,You can search for regular expression in regexlib .
10653,116.0,2008-08-14T04:01:10Z,10600,0,"We have a database cluster attached to a NAS, also with redundant HBA.  The NAS units are RAID-10.  From our storage-meister, for databases the higher RPM the better."
10684,872.0,2008-08-14T04:59:55Z,10680,16,"You should use atexit() if possible. on_exit() is nonstandard and less common.  For example, it's not available on OS X. Kernel.org - on_exit() : This function comes from SunOS 4, but is also present in libc4, libc5 and
  glibc.  It no longer occurs in Solaris (SunOS 5).  Avoid this function, and
  use the standard atexit(3) instead."
10686,122.0,2008-08-14T05:03:17Z,10680,8,"According to this link I found, it seems there are a few differences. on_exit will let you pass in an argument that is passed in to the on_exit function when it is called... which might let you set up some pointers to do some cleanup work on when it is time to exit. Furthermore, it appears that on_exit was a SunOS specific function that may not be compatible on all platforms... so you may want to stick with atexit, despite it being more restrictive."
10687,234.0,2008-08-14T05:03:34Z,10610,-1,"e-texteditor hilights what you're searching for as you type it. This is incredibly useful, as you can paste your 'sample text' into a file, and just type your regex into the search field, and see what it's matching right in front of you. None of these 'visual regex builder' things are substitutes for actually LEARNING regular expressions."
10697,1265.0,2008-08-14T05:26:17Z,10600,1,"The biggest performance boost you can get is by partitioning tables/indexes onto different disks. The first step would be to put indexes on one disk and data on an other. After this you should consider which tables/indexes are used together, and put them on separate disks (""spindles"") when possible."
10698,528.0,2008-08-14T05:28:59Z,8970,9,"You should check out the QTKit Capture documentation . On Leopard, you can get at all of it over the RubyCocoa bridge: require 'osx/cocoa'
OSX.require_framework(""/System/Library/Frameworks/QTKit.framework"")

OSX::QTCaptureDevice.inputDevices.each do |device|
    puts device.localizedDisplayName
end"
10714,380.0,2008-08-14T05:53:40Z,10670,0,"The idea behind using the ThreadPool is that through it you can control the amount of synchronous threads, and if those get too many, then the thread pool automatically manages the waiting of newer threads. The Asp.Net worked thread (AFAIK) doesn't come from the Thread Pool and shouldn't get affected by your call to the remoting service (unless this is a very slow processor, and your remoting function is very CPU intensive - in which case, everything on your computer will be affected). You could always host the remoting service on a different physical server. In that case, your asp.net worker thread will be totally independent of your remoting call (if the remoting call is called on a separate thread that is)."
10770,122.0,2008-08-14T06:51:23Z,10680,0,"@Nathan First, see if there is another API call to determine exit status... a quick glance and I don't see one, but I am not well versed in the standard C API. An easy alternative is to have a global variable that stores the exit status... the default being an unknown error cause (for if the program terminates abnormally).  Then, when you call exit, you can store the exit status in the global and retrieve it from any atexit functions.  This requires storing the exit status diligently before every exit call, and clearly is not ideal, but if there is no API and you don't want to risk on_exit not being on the platform... it might be the only option."
10791,1228206.0,2008-08-14T07:59:30Z,8050,1,A bit old but still relevant: http://www.developerzen.com/2007/09/17/introduction-to-linq/
10792,5.0,2008-08-14T08:03:12Z,8050,7,"I recommend the Hooked On LINQ wiki. They've got some great introductory info , as well as more in depth info and samples on all of the operators. I listed a lot of LINQ references in the show notes for Herding Code Episode 10 (on LINQ) . One of my favorites is an MSDN Magazine article which explains how LINQ works from a framework perspective in a way which really helped me understand how it works."
10795,1266.0,2008-08-14T08:09:24Z,8050,1,"From MSDN, here are some papers, written by Anders and others: LINQ: .NET Language-Integrated Query LINQ to SQL: .NET Language-Integrated Query for Relational Data PS after writing this, I see someone has already linked to these, but buried inside a paragraph, so I'll keep them pulled out here in list form as well."
10796,872.0,2008-08-14T08:09:47Z,10680,1,"@Nathan, I can't find any function that will return the exit code for the current running process.  I expect that it isn't set yet at the point when atexit() is called, anyway.  By this I mean that the runtime knows what it is, but probably hasn't reported it to the OS.  This is pretty much just conjecture, though. It looks like you will either need to use on_exit() or structure your program so that the exit code doesn't matter.  It would not be unreasonable to have the last statement in your main function flip a global exited_cleanly variable to true.  In the function you register with atexit() , you could check this variable to determine how the program exited.  This will only give you two states, but I expect that would be sufficient for most needs.  You could also expand this type of scheme to support more exit states if necessary."
10809,5.0,2008-08-14T08:35:30Z,10260,1,"As others have said, there's a good list at www.asp.net/learn . I think the biggest ASP.NET specific changes are: Official ASP.NET AJAX integration ListView (much better than the GridView / DataView in that they let you write out clean HTML) Big improvements to the IDE for CSS / HTML editing Javascript debugging Note that ASP.NET MVC is not released yet, and was definitely not included with ASP.NET 3.5."
10811,372.0,2008-08-14T08:39:11Z,10260,7,"For ASP.NET, you have a lot of improvements: split view (code and design) faster switching between code and design view embedded master pages (one master page in another) javascript debugging Anyway most of the useful stuff are really in the meat of the language, and for .NET 3.5 the new language features for C# 3.0 will be (and yes, I find ALL of them useful) anonymous objects automatic properties object initializers collection initializers (inline initialization for collections) implicit typing (var keyword) lambda expressions LINQ Extension methods I might have forgotten a few, but I think this is about most of the new cool and useful stuff."
10851,986.0,2008-08-14T09:57:18Z,10260,1,"Here's a brief list of my favourites: LINQ Extension Methods Lambda Methods And I don't actually use ASP.NET, but ASP.NET AJAX is now included in 3.5 too and ASP.NET MVC is included in 3.5 SP1."
10862,431.0,2008-08-14T10:27:50Z,10860,1,"An excellent book I have, which covers this topic, is Data Access Patterns , by Clifton Nock.  It has got many good explanations and good ideas on how to decouple your business layer from the persistence layer.  You really should give it a try.  It's one of my favorite books."
10863,615.0,2008-08-14T10:28:08Z,10860,0,"One trick I've found handy is to have my data layer be ""collection agnostic"". That is, whenever I want to return a list of objects from my data layer, I get the caller to pass in the list. So instead of this: public IList<Foo> GetFoosById(int id) { ... } I do this: public void GetFoosById(IList<Foo> foos, int id) { ... } This lets me pass in a plain old List if that's all I need, or a more intelligent implementation of IList<T> (like ObservableCollection<T>) if I plan to bind to it from the UI. This technique also lets me return stuff from the method like a ValidationResult containing an error message if one occurred. This still means that my data layer knows about my object definitions, but it gives me one extra degree of flexibility."
10864,905.0,2008-08-14T10:28:20Z,10860,0,"Check out Linq to SQL, if I were creating a new application right now I would consider relying on an entirely Linq based data layer. Other than that I think it's good practise to de-couple data and logic as much as possible, but that isn't always practical.  A pure separation between logic and data access makes joins and optimisations difficult, which is what makes Linq so powerful."
10866,31505.0,2008-08-14T10:28:43Z,10860,3,"You can have both. Let data layer not know of your bussiness objects and make it capable of working with more than one type of data sources. If you supply a common interface (or an abstract class) for interacting with data, you can have different implementations for each type of data source. Factory pattern goes well here."
10868,718.0,2008-08-14T10:30:21Z,10860,1,Jeffrey Palermo wrote a good post about this. He called it Onion Architecture .
10869,372.0,2008-08-14T10:30:51Z,10860,0,"In applications wherein we use NHibernate, the answer becomes ""somewhere in between"", in that, while the XML mapping definitions (they specify which table belongs to which object and which columns belong to which field, etc) are clearly in the business object tier. They are passed to a generic data session manager which is not aware of any of the business objects; the only requirement is that the business objects passed to it for CRUD have to have a mapping file."
10875,1075.0,2008-08-14T10:38:04Z,10860,5,"It really depends on your view of the world - I used to be in the uncoupled camp. The DAL was only there to supply data to the BAL - end of story. With emerging technologies such as Linq to SQL and Entity Framework becoming a bit more popular, then the line between DAL and BAL have been blurred a bit. In L2S especially your DAL is quite tightly coupled to the Business objects as the object model has a 1-1 mapping to your database field. Like anything in software development there is no right or wrong answer. You need to understand your requirements and future requirments and work from there. I would no more use a Ferrari on the Dakhar rally as I would a Range Rover on a track day."
10888,1163.0,2008-08-14T11:01:33Z,9570,1,"How I solved it: link with ""/FORCE:MULTIPLE /verbose"" (that links ok) and set the output aside. link with ""/NODEFAULTIB /verbose"" and trace all unresolveds in the output of the previous step and add the libraries 1 by 1. This resulted in doubles: ""AAA.lib: XXX already defined in BBB.lib"" Then I finally got it:
Recompiled managed AND unmanaged units with /MD
and link to (among others):
mscoree.lib
msvcmrt.lib
mfcm80d.lib Mixing /MT (unmanaged) and /MD (managed) turned out to be the bad idea: 
different(overlapping) libraries are needed. @ajryan: Dependcy Walker only tells me what dll's are used, not what libraries are linked to when linking.
(e.g. msvcmrt.lib ?)
I think. Thanks for the answers! Jan"
10890,1199.0,2008-08-14T11:04:13Z,10880,16,"No specific article, really, but I've found EmacsWiki to be full of useful information. Consider checking out these entries: CPlusPlus as a starting point for many C++-related articles, and CppTemplate to define a template that can give you a good skeleton when you start new files"
10902,1276.0,2008-08-14T11:26:53Z,10230,1,"String.IsNullOrEmpty() only works on .net 2.0 and above, for .net 1/1.1, I tend to use: if (inputString == null || inputString == String.Empty)
{
    // String is null or empty, do something clever here. Or just expload.
} I use String.Empty as opposed to """" because """" will create an object, whereas String.Empty wont - I know its something small and trivial, but id still rather not create objects when I dont need them! ( Source )"
10997,968.0,2008-08-14T13:26:27Z,10990,9,Or at least 766. read = 4 write = 2 execute = 1 7 = read + write + execute 6 = read + write first number: owner second number: group third number: other users
11005,75.0,2008-08-14T13:32:55Z,10990,24,You can create a new group with both the apache user and FTP user as members and then make the permission on the upload folder 775.  This should give both the apache and FTP users the ability to write to the files in the folder but keep everyone else from modifying them.
11025,312.0,2008-08-14T13:43:36Z,10990,0,I will add that if you are using SELinux that you need to make sure the type context is tmp_t  You can accomplish this by using the chcon utility chcon -t tmp_t uploads
11029,1309.0,2008-08-14T13:45:04Z,10990,10,"I would go with Ryan's answer if you really want to do this. In general on a *nix environment, you always want to err on giving away as little permissions as possible. 9 times out of 10, 755 is the ideal permission for this - as the only user with the ability to modify the files will be the webserver. Change this to 775 with your ftp user in a group if you REALLY need to change this. Since you're new to php by your own admission, here's a helpful link for improving the security of your upload service: move_uploaded_file"
11074,267.0,2008-08-14T14:04:25Z,11060,8,"I started writing up a summary of my experience with my own code generator, then went back and re-read your question and found you had already touched upon the same issues yourself, focus on the execution results instead of the code layout/look. Problem is, this is hard to test, the generated code might not be suited to actually run in the environment of the unit test system, and how do you encode the expected results? I've found that you need to break down the code generator into smaller pieces and unit test those. Unit testing a full code generator is more like integration testing than unit testing if you ask me."
11091,1319.0,2008-08-14T14:17:39Z,3150,4,"The framework included with VS9 is .NET, but you can write tests in C++/CLI, so as long as you're comfortable learning some .NET isms, you should be able to test most any C++ code. boost.test and googletest look to be fairly similar, but adapted for slightly different uses. Both of these have a binary component, so you'll need an extra project in your solution to compile and run the tests. The framework we use is CxxTest , which is much lighter; it's headers only, and uses a Perl (!) script to scrape test suite information from your headers (suites inherit from CxxTest::Base, all your test methods' names start with ""test""). Obviously, this requires that you get Perl from one source or another , which adds overhead to your build environment setup."
11097,740.0,2008-08-14T14:20:08Z,10260,1,see Scott Hanselman's post here: http://www.hanselman.com/blog/HiddenGemsNotTheSameOld35SP1Post.aspx
11122,,2008-08-14T14:34:30Z,10810,2,"For AzMan with ASP.NET, turn on impersonation in web.config ( <identity impersonate=""true"" username=""xx"" pasword=""xx"" /> ), and make sure with an AD administrator that the impersonation account has ""reader"" permissions on the AzMan store; plus, give write permissions to this account on the Temporary ASP.NET Files folder (under C:\Windows\Microsoft.NET\<framework> )."
11128,414.0,2008-08-14T14:38:19Z,11060,0,"Yes, results are the ONLY thing that matters. The real chore is writing a framework that allows your generated code to run independently... spend your time there."
11150,1114.0,2008-08-14T14:56:14Z,8970,2,"If you poke around Apple's mailing lists you can find some code to do it in Java as well. Here's a simple example suitable for capturing individual frames , and here's a more complicated one that's fast enough to display live video ."
11163,1327.0,2008-08-14T15:04:08Z,1760,1,"This is really a personal opinion on my part (I guess that's redundant since it is a forum). NUnit, MSTest, ect all do pretty mutch the same thing.  However I find NMock indispensable. NMock or any mocking package is not unit testing but it makes it so much easier to do unit testing that it mught as well be."
11170,1122.0,2008-08-14T15:09:17Z,1760,6,Stick to NUnit.  Don't go anywhere near MSTest. NUnit + ReSharper is an absolute joy to work with.
11209,22.0,2008-08-14T15:29:51Z,10870,6,"Are you using only the dragEnter method? If you are trying to reject the drag while still dragging over the same component you need to use both the dragEnter and dragOver methods. Check out this example: <?xml version=""1.0"" encoding=""utf-8""?>
<mx:Application xmlns:mx=""http://www.adobe.com/2006/mxml"" layout=""absolute"">
    <mx:Script>
    	<![CDATA[
    		import mx.core.DragSource;
    		import mx.managers.DragManager;
    		import mx.events.DragEvent;

    		private function onDragEnter(e:DragEvent):void {
    			if ( e.target == lbl ) {

    				if ( e.localX < lbl.width/2 ) {
    					trace(""accept"");
    					DragManager.acceptDragDrop(this);
    				}
    				else {
    					DragManager.acceptDragDrop(null);
    				}
    			}
    		}

    		private function doStartDrag(e:MouseEvent):void {
    			if ( e.buttonDown ) {
    				var ds:DragSource = new DragSource();
    				ds.addData(""test"", ""text"");

    				DragManager.doDrag(btn, ds, e);
    			}
    		}
    	]]>
    </mx:Script>
    <mx:Label id=""lbl"" text=""hello world!"" left=""10"" top=""10"" dragEnter=""onDragEnter(event)"" dragOver=""onDragEnter(event)"" />
    <mx:Button id=""btn"" x=""47"" y=""255"" label=""Button"" mouseMove=""doStartDrag(event)""/>
</mx:Application>"
11213,414.0,2008-08-14T15:32:00Z,11200,0,"Define ""last epoch"". Does this come close? Select Cast(lastModified As Integer)"
11216,740.0,2008-08-14T15:33:02Z,11200,2,"Last epoch is when 1970 GMT? SELECT DATEDIFF(s,'19700101 05:00:00:000',lastModified) See also Epoch Date"
11229,414.0,2008-08-14T15:41:30Z,11200,0,"If you store them as varchar, store them as YYYYMMDD. That way you CAN sort by them later if you want to."
11234,740.0,2008-08-14T15:45:00Z,11200,0,"SQL server has only 2 failsafe date formats ISO = YYYYMMDD, run this to see that select convert(varchar(10),getdate(),112) ISO8601 = yyyy-mm-dd Thh:mm:ss:mmm(no spaces) run this to see that select convert(varchar(30),getdate(),126) To learn more about how dates are stored in SQL server I wrote How Are Dates Stored In SQL Server?"
11235,1343.0,2008-08-14T15:46:01Z,11060,0,"If you are running on *nux you might consider dumping the unittest framework in favor of a bash script or makefile. on windows you might consider building a shell app/function that runs the generator and then uses the code (as another process) and unittest that. A third option would be to generate the code and then build an app from it that includes nothing but a unittest. Again you would need a shell script or whatnot to run this for each input. As to how to encode the expected behavior, it occurs to me that it could be done in much the same way as you would for the C++ code just using the generated interface rather than the C++ one."
11240,26.0,2008-08-14T15:52:17Z,11200,1,"I wound up using format 120 in MS SQL: convert(char(24), lastModified, 120) Each time I needed to a select a date in SQLite for non-display purposes I used: strftime(\""%Y-%m-%d %H:%M:%S\"", dateModified) as dateModified Now I just need a readable/friendly way to display the date to the user! edit: accept answer goes to whoever shows me how to display the date nicely from sqlite ;p"
11335,390.0,2008-08-14T16:58:47Z,11330,7,"You'd probably need to redefine the function pointer to take additional arguments. void foreachMove( void (*action)(chess_move*, int), chess_game* game )"
11352,700.0,2008-08-14T17:15:31Z,2840,13,"If you're trying to get it in one statement (the total plus the paging).  You might need to explore SQL Server support for the partition by clause (windowing functions in ANSI SQL terms).  In Oracle the syntax is just like the example above using row_number(), but I have also added a partition by clause to get the total number of rows included with each row returned in the paging (total rows is 1,262): SELECT rn, total_rows, x.OWNER, x.object_name, x.object_type
  FROM (SELECT COUNT (*) OVER (PARTITION BY owner) AS TOTAL_ROWS,
               ROW_NUMBER () OVER (ORDER BY 1) AS rn, uo.*
          FROM all_objects uo
         WHERE owner = 'CSEIS') x
 WHERE rn BETWEEN 6 AND 10 Note that I have where owner = 'CSEIS' and my partition by is on owner.  So the results are: RN  TOTAL_ROWS  OWNER   OBJECT_NAME OBJECT_TYPE
6   1262    CSEIS   CG$BDS_MODIFICATION_TYPES   TRIGGER
7   1262    CSEIS   CG$AUS_MODIFICATION_TYPES   TRIGGER
8   1262    CSEIS   CG$BDR_MODIFICATION_TYPES   TRIGGER
9   1262    CSEIS   CG$ADS_MODIFICATION_TYPES   TRIGGER
10  1262    CSEIS   CG$BIS_LANGUAGES    TRIGGER"
11363,316.0,2008-08-14T17:19:28Z,11330,10,"Ah, if only C supported closures... Antonio is right; if you need to pass extra parameters, you'll need to redefine your function pointer to accept the additional arguments. If you don't know exactly what parameters you'll need, then you have at least three choices: Have the last argument in your prototype be a void*. This gives you flexibility of passing in anything else that you need, but it definitely isn't type-safe. Use variadic parameters (...). Given my lack of experience with variadic parameters in C, I'm not sure if you can use this with a function pointer, but this gives even more flexibility than the first solution, albeit still with the lack of type safety. Upgrade to C++ and use function objects ."
11379,1179.0,2008-08-14T17:24:57Z,11330,3,"If I'm reading this right, what I'd suggest is to make your function take a pointer to a struct as an argument. Then, your struct can have can have ""game"" and ""depth"" when it needs them, and just leave them set to 0 or Null when you don't need them. What is going on in that function? Do you have a conditional that says, if (depth > -1) //some default
  {
  //do something
  } Does the function always REQUIRE ""game"" and ""depth"". Then, they should always be arguments, and that can go into your prototypes. Are you indicating the function only sometimes requires ""game"" and ""depth""? Well, maybe make two functions and use each one when you need to. But, having a structure as the argument is probably the easiest thing."
11395,112.0,2008-08-14T17:32:18Z,11330,5,"If you're willing to use some C++, you can use a ""function object"": struct MoveHandler {
    chess_game *game;
    int depth;

    MoveHandler(chess_game *g, int d): game(g), depth(d) {}

    void operator () (chess_move*) {
         // now you can use the game and the depth
    }
}; and turn your foreachMove into a template: template <typename T>
void foreachMove(T action, chess_game* game); and you can call it like this: chess_move getNextMove(chess_game* game, int depth){
    //for each valid move, determine how good the move is
    foreachMove(MoveHandler(game, depth), game);
} but it won't disrupt your other uses of MoveHandler ."
11412,432.0,2008-08-14T17:48:17Z,9750,3,"I'm probably misremembering, but I
  thought that Joel's question was about
  counting the ""on"" bits rather than
  reversing them. Here you go: #include <stdio.h>

int countBits(unsigned char byte);

int main(){
  FILE* out = fopen( ""bitcount.c"" ,""w"");

  int i;
  fprintf(out, ""#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n\n"");

  fprintf(out, ""int bitcount[256] = {"");
  for(i=0;i<256;i++){
    fprintf(out, ""%i"", countBits((unsigned char)i));
    if( i < 255 ) fprintf(out, "", "");
  }
  fprintf(out, ""};\n\n"");

  fprintf(out, ""int main(){\n"");

  fprintf(out, ""srand ( time(NULL) );\n"");
  fprintf(out, ""\tint num = rand() %% 256;\n"");
  fprintf(out, ""\tprintf(\""The byte %%i has %%i bits set to ON.\\n\"", num, bitcount[num]);\n"");

  fprintf(out, ""\treturn 0;\n"");
  fprintf(out, ""}\n"");
  fclose(out);

  return 0;
}

int countBits(unsigned char byte){
  unsigned char mask = 1;
  int count = 0;
  while(mask){
    if( mask&byte ) count++;
    mask <<= 1;
  }
  return count;
}"
11436,1341.0,2008-08-14T18:09:59Z,11430,10,"Actually it ships with .NET 3.5 SP1.  So yes, the stored procs can use 3.5 features and libraries."
11442,1084.0,2008-08-14T18:15:26Z,11330,1,"I'd suggest using an array of void*, with the last entry always void.
say you need 3 parameters you could do this: void MoveHandler (void** DataArray)
{
    // data1 is always chess_move
    chess_move data1 = DataArray[0]? (*(chess_move*)DataArray[0]) : NULL; 
    // data2 is always float
    float data1 = DataArray[1]? (*(float*)DataArray[1]) : NULL; 
    // data3 is always char
    char data1 = DataArray[2]? (*(char*)DataArray[2]) : NULL; 
    //etc
}

void foreachMove( void (*action)(void**), chess_game* game); and then chess_move getNextMove(chess_game* game, int depth){
    //for each valid move, determine how good the move is
    void* data[4];
    data[0] = &chess_move;
    float f1;
    char c1;
    data[1] = &f1;
    data[2] = &c1;
    data[3] = NULL;
    foreachMove(moveHandler, game);
} If all the parameters are the same type then you can avoid the void* array and just send a NULL-terminated array of whatever type you need."
11443,825.0,2008-08-14T18:15:42Z,11060,4,"Recall that ""unit testing"" is only one kind of testing.  You should be able to unit test the internal pieces of your code generator.  What you're really looking at here is system level testing (a.k.a. regression testing).  It's not just semantics... there are different mindsets, approaches, expectations, etc.  It's certainly more work, but you probably need to bite the bullet and set up an end-to-end regression test suite: fixed C++ files -> SWIG interfaces -> python modules -> known output.  You really want to check the known input (fixed C++ code) against expected output (what comes out of the final Python program).  Checking the code generator results directly would be like diffing object files..."
11452,1358.0,2008-08-14T18:20:02Z,2630,1,"Well it is a little bland, but I would vote for Get-Help."
11472,72.0,2008-08-14T18:38:24Z,11460,3,"Why won't any old proxy software work for this?  Why does it need to be an ASP.NET application?  There are TONS of tools out there (both Windows and *nix) that will get the job done quite easily.  Check Squid or NetProxy for starters. If you need to integrate with IIS, IISProxy looks like it would do the trick too."
11475,872.0,2008-08-14T18:39:46Z,11330,0,"+1 to Antonio.  You need to change your function pointer declaration to accept additional parameters. Also, please don't start passing around void pointers or (especially) arrays of void pointers.  That's just asking for trouble.  If you start passing void pointers, you're going to also have to pass some kind of message to indicate what the pointer type is (or types are).  This technique is rarely appropriate. If your parameters are always the same, just add them to your function pointer arguments (or possibly pack them into a struct and use that as the argument if there are a lot of parameters).  If your parameters change, then consider using multiple function pointers for the multiple call scenarios instead of passing void pointers."
11506,942.0,2008-08-14T18:55:21Z,11500,0,You could turn on compression based on your client supporting it. See this article: link text
11507,383.0,2008-08-14T18:55:58Z,11500,0,Static resources shouldn't be resent unless changed. IIS will send a response code which tells the browser to use the cached version.
11521,77.0,2008-08-14T19:03:58Z,11500,22,"Script Combining in .net 3.5 SP1 Best Practices for fast websites HTTP Compression (gzip) Compress JS / CSS (different than http compression, minify javascript) YUI Compressor .NET YUI Compressor My best advice is to check out the YUI content . They have some great articles that talk about things like CSS sprites and have some nice javascript libraries to help reduce the number of requests the browser is making."
11528,1220.0,2008-08-14T19:06:44Z,11500,3,"Turn viewstate off by default, it will be a night and day difference on even the most simple pages."
11529,162.0,2008-08-14T19:06:54Z,11520,2,The other major player would be DevExpress and their CodeRush and Refactor products. Found here.
11534,419.0,2008-08-14T19:10:06Z,11500,1,"I think you really need to be able to get some actual PerfMon data/telemetry from the app whilst running in production to be able to make an enlightened decision about what to optimise. As a throw away tip I'd make sure your app is deployed as a Release build and set debug=""false"" in the ' compilation ' section of your web.config."
11537,1311.0,2008-08-14T19:11:07Z,11500,5,"If you are using Firefox to test your website, you might want to try a nifty Firefox extension from Yahoo! called YSlow . It analyzes your web pages and provides grades from A-F (A being the Best and F being the worst) for each of the best practices, for high performance websites. It will help you to track down the elements of your website which you could optimize to gain speedups."
11539,1367.0,2008-08-14T19:11:33Z,11520,4,"Aside from trying out Visual AssistX, the only other one I've tried is ReSharper (which I highly recommend). If you do decide to go for ReSharper, you'll likely notice that it's missing a spell checker for code though - however the Agent Smith plugin fixes that."
11541,1220.0,2008-08-14T19:12:55Z,11460,1,I use apache mod proxy and mod proxy_balancer. Works awesome running 5 domains a cluster of 4 web boxes.
11570,1194.0,2008-08-14T19:28:00Z,11500,1,"You seem to be starting by assuming that your problem is download size - that may not necessarily be the case. You should do some experimentation with your ASP.NET site to determine if there are areas in your code which are causing undue delays. If it turns out that download size is not your problem, you'll need to find ways to cache your results (look into output caching, which is an ASP.NET feature) or optimize your code. In any case - the first step when looking at a performance issue is always to verify your assumptions first, then decide on a course of action."
11594,267.0,2008-08-14T19:45:43Z,11520,4,"You should take a look at Visual Studio Gallery , the one stop shop for Visual Studio extensions . Here you'll find quite a lot of extensions for Visual Studio in all categories, from intellisense and refactoring to designers and documentation builders."
11616,731.0,2008-08-14T19:53:55Z,11500,1,Have you tried these tips? http://weblogs.asp.net/haroonwaheed/archive/2008/06/30/ASP.NET-Performance-Tips.aspx
11623,1194.0,2008-08-14T19:56:56Z,11620,4,"In MS SQL Server Management Studio on the object explorer, right click on the database. In the context menu that follows select 'Tasks -> Take Offline'"
11624,740.0,2008-08-14T19:56:56Z,11620,327,"See Kill All Active Connections To A Database . The reason that the approach that Adam suggested won't work is that during the time that you are looping over the active connections new one can be established, and you'll miss those. The article I linked to uses the following approach which does not have this drawback: -- set your current connection to use master otherwise you might get an error

use master
ALTER DATABASE YourDatabase SET SINGLE_USER WITH ROLLBACK IMMEDIATE 

--do you stuff here 

ALTER DATABASE YourDatabase SET MULTI_USER"
11627,1341.0,2008-08-14T19:57:15Z,11620,94,"Script to accomplish this, replace 'DB_NAME' with the database to kill all connections to: USE master
GO

SET NOCOUNT ON
DECLARE @DBName varchar(50)
DECLARE @spidstr varchar(8000)
DECLARE @ConnKilled smallint
SET @ConnKilled=0
SET @spidstr = ''

Set @DBName = 'DB_NAME'
IF db_id(@DBName) < 4
BEGIN
PRINT 'Connections to system databases cannot be killed'
RETURN
END
SELECT @spidstr=coalesce(@spidstr,',' )+'kill '+convert(varchar, spid)+ '; '
FROM master..sysprocesses WHERE dbid=db_id(@DBName)

IF LEN(@spidstr) > 0
BEGIN
EXEC(@spidstr)
SELECT @ConnKilled = COUNT(1)
FROM master..sysprocesses WHERE dbid=db_id(@DBName)
END"
11629,507.0,2008-08-14T19:58:03Z,11620,23,"Using SQL Management Studio Express: In the Object Explorer tree drill down under Management to ""Activity Monitor"" (if you cannot find it there then right click on the database server and select ""Activity Monitor""). Opening the Activity Monitor, you can view all process info. You should be able to find the locks for the database you're interested in and kill those locks, which will also kill the connection. You should be able to rename after that."
11630,317.0,2008-08-14T19:58:13Z,11620,2,"Try this: ALTER DATABASE [DATABASE_NAME]
SET SINGLE_USER
WITH ROLLBACK IMMEDIATE"
11633,225.0,2008-08-14T20:00:23Z,11620,22,"I've always used: ALTER DATABASE DB_NAME SET SINGLE_USER WITH ROLLBACK IMMEDIATE 
GO 
SP_RENAMEDB 'DB_NAME','DB_NAME_NEW'
Go 
ALTER DATABASE DB_NAME_NEW  SET MULTI_USER -- set back to multi user 
GO"
11649,942.0,2008-08-14T20:07:46Z,11520,3,"Once you get into resharper, you really don't want to leave, its done a massive amount to improve my productivity. It depends though on what you are doing. Are you doing a lot of TDD when you write tests, write code, then refactor? Unless you are pretty intensely into refactoring then I'd suggest that you might not get the best of out R#. As a plugin for a plugin I use the RGreatX plugin for R#. It's really handy for shifting string values out to resource files for localization of the software.....saves me plenty of time!"
11661,1117.0,2008-08-14T20:22:54Z,11330,0,"If your parameters change, I would change the function pointer declaration to use the ""..."" technique to set up a variable number of arguments.  It could save you in readability and also having to make a change for each parameter you want to pass to the function.  It is definately a lot safer than passing void around. http://publications.gbdirect.co.uk/c_book/chapter9/stdarg.html Just an FYI, about the example code in the link: some places they have n args and others it is n_args with the underscore. They should all have the underscore. I thought the syntax looked a little funny until  I realized they had dropped the underscore in some places."
11681,942.0,2008-08-14T20:37:15Z,11680,1,The Strategy pattern is maybe one you want to look at. The strategy being the file parsing algorithm. Then you want a separate strategy for database insertion.
11693,444.0,2008-08-14T20:56:08Z,11690,2,"I'm not sure about the unicode issue but if you want the tooltip effect you should be using the title attribute, not alt. Alt is for text you want screenreaders to speak, and it's what gets displayed if an image can't be loaded."
11700,42.0,2008-08-14T21:04:22Z,11690,5,This is because the font used in the tooltip doesn't include the characters you are trying to display. Try installing a font pack that includes those characters. I'm affraid you can't do much for your site's visitors other than implementating a tooltip yourself using javascript.
11712,234.0,2008-08-14T21:20:18Z,11680,19,"Just write your file parser, using whatever techniques come to mind Write lots of unit tests for it to make sure all your edge cases are covered Once you've done this, you will actually have a reasonable idea of the problem/solution. Right now you just have theories floating around in your head, most of which will turn out to be misguided. Step 3: Refactor mercilessly. Your aim should be to delete about half of your code You'll find that your code at the end will either resemble an existing design pattern, or you'll have created a new one. You'll then be qualified to answer this question :-)"
11713,872.0,2008-08-14T21:21:07Z,11690,0,"Can you sanitize the alt text so that it doesn't have the characters in it, preferably by replacing the entire text with something useful (rather than just filtering the string)?  That's not ideal, but neither is displaying broken characters, or telling your users to install a new font pack."
11730,419.0,2008-08-14T22:08:31Z,11720,3,You can actually script a fair number of tasks in MS Virtual Server: http://www.microsoft.com/technet/scriptcenter/scripts/vs/default.mspx?mfr=true http://msdn.microsoft.com/en-us/library/aa368876 (VS.85).aspx Also Virtual PC guy has got a ton of stuff on his blog about scripting Virtual Server/PC and now Hyper-V here: http://blogs.msdn.com/virtual_pc_guy/default.aspx VMware has similar capabilities: http://www.vmware.com/support/developer/scripting-API/
11747,1370.0,2008-08-14T22:43:07Z,11690,1,"Where's your Japanese input coming from? It could be that it's in a non-unicode (e.g. http://en.wikipedia.org/wiki/JIS_X_0208 ) encoding, whereas your file is in unicode so the browser attempts to interpret the non-unicode characters as unicode and gets confused. I tried throwing together an example to reproduce your problem: <img src=""test.png"" alt="""" /> The tooltip displays properly under IE7 with the Japanese language pack installed."
11757,608.0,2008-08-14T23:04:50Z,11740,1,LightSpeed allows you to specify an INamingStrategy that lets you resolve table names dynamically at runtime.
11830,952.0,2008-08-15T00:36:54Z,11820,0,"I use this method for some internal corporate webservices, and I haven't noticed any major slow-downs (but that doesn't mean it's not there). You could probably use any of the numerous network traffic analysis tools to measure the size of the data, and make a judgment call based off that."
11832,9.0,2008-08-15T00:39:05Z,11820,11,"Typically a byte array is sent as a base64 encoded string, not as individual bytes in tags. http://en.wikipedia.org/wiki/Base64 The base64 encoded version is about 137% of the size of the original content."
11834,194.0,2008-08-15T00:40:14Z,11820,0,"I'm not sure about all the details (compressing, encoding, etc) but I usually just use WireShark to analyze the network traffic (while trying various methods) which then allows you to see exactly how it's sent. For example, if it's compressed the data block of the packet shouldn't be readable as plain text...however if it's uncompressed, you will just see plain old xml text...like you would see with HTTP traffic, or even FTP in certain cases."
11840,493.0,2008-08-15T00:48:55Z,11820,0,"To echo what Kevin said, in .net web services if you have a byte array it is sent as a base64 encoded string by default. You can also specify the encoding of the byte array beforehand. Obviously, once it gets to the server (or client) you need to manually decode the string back into a byte array as this isn't done automagically for you unfortunately."
11912,357.0,2008-08-15T02:53:37Z,11500,2,I wrote a blog post about improving ASP.NET page performance this a couple months back. Here are some quick & easy ways - Turn off view state Turn off event validation Implement HTTP gzip/deflate compression to reduce the response size (number of bytes the server has to send back to the client) Try to optimize/minimize your database calls for each request
11943,725.0,2008-08-15T03:49:48Z,11930,0,"That is not as easy as it sounds.  Java is platform independent, so I am not sure how to do it in Java.  I am guessing that .NET contacts some web site which reports it back.  There are a couple ways to go.  First, a deeper look into the ICMP protocol may give you the information you need.  You can also trace the IP you go through (your route).  When you encounter an IP that is not in the following ranges: 10.0.0.0  10.255.255.255 172.16.0.0  172.31.255.255 192.168.0.0  192.168.255.255 it is the IP one hop away from yours, and probably shares a few octets of information with your IP. Best of luck.  I'll be curious to hear a definitive answer to this question."
11960,528.0,2008-08-15T04:25:23Z,11930,0,"Try shelling out to traceroute if you have it. 'traceroute -m 1 www.amazon.com' will emit something like this: traceroute to www.amazon.com (72.21.203.1), 1 hops max, 40 byte packets
 1  10.0.1.1 (10.0.1.1)  0.694 ms  0.445 ms  0.398 ms Parse the second line.  Yes, it's ugly, but it'll get you going until someone posts something nicer."
11961,225.0,2008-08-15T04:26:53Z,11950,0,"We use a custom home grown logging util we wrote.  It requires you to implement logging on your own everywhere you need it.  But, it also allows your to capture alot more then just the exception. For example our code would look like this: <pre><code>
Try
  Dim p as New Person()
  p.Name = ""Joe""
  p.Age = 30
Catch ex as Exception
  Log.LogException(ex,""Err creating person and assigning name/age"")
  Throw ex
End Try
</code></pre> This way our logger will write all the info we need to a sql db.  We have email alerts set up at the db level to look for certain errors or frequently occurring errrors.  It helps us identify exactly where the errors are coming from. This might not be exactly what you're looking for.  Another approach similar to using Global.asax is to us a code injection technique like AOP with PostSharp .  This allows you to inject custom code at the beginning and end of every method or on every exception.  It' an interesting approach but I believe it may have a heavy performance overhead."
11965,423.0,2008-08-15T04:33:02Z,11950,0,"My team uses log4net from Apache.  It's pretty lightweight and easy to setup.  Best of all, it's completely configurable from the web.config file, so once you've got the hooks in your code setup, you can completely change the way logging is done just by changing the web.config file. log4net supports logging to a wide variety of locations - database, email, text file, Windows event log, etc.  My team has it configured to send detailed error information to a database, and also send an email to the entire team with enough information for us to determine in which part of the code the error originated.  Then we know who is responsible for that piece of code, and they can go to the database to get more detailed information."
11967,1117.0,2008-08-15T04:34:49Z,11950,25,I use elmah .  It has some really nice features and here is a CodeProject article on it.  I think the StackOverflow team uses elmah also!
12011,721.0,2008-08-15T06:11:57Z,11950,2,"I've been using the Enterprise Library's Logging objects. It allows you to have different types of logging (flat file, e-mail, and/or database). It's pretty customizable and has a pretty good interface for updating your web.config for the configuration of the logging. Usually I call my logging from the On Error in the Global.asax. Here's a link to the MSDN"
12013,188.0,2008-08-15T06:15:16Z,11930,1,"You may be better off using something like checkmyip.org, which will determine your public IP address - not necessarily your first hop router: at Uni I have a ""real"" IP address, whereas at home it is my local router's public IP address. You can parse the page that returns, or find another site that allows you to just get the IP address back as the only string. (I'm meaning load this URL in Java/whatever, and then get the info you need). This should be totally platform independent."
12025,775.0,2008-08-15T06:31:07Z,11950,9,"I've been using Log4net , configured to email details of fatal errors.  It's also set up to log everything to a log file, which is invaluable when trying to debug problems.  The other benefit is that if that standard functionality doesn't do what you want it to, it's fairly easy to write a custom appender which can process the logging information as required. Having said that, I'm using this in tandem with a custom error handler which sends out a html email with a bit more information than is included in the standard log4net emails - page, session variables, cookies, http server variables, etc. These are both wired up in the Application_OnError event, where the exception is logged as a fatal exception in log4net (which then causes it to be emailed to a specified email address), and also handled using the custom error handler. First heard about Elmah from the Coding Horror blog entry, Crash Responsibly , and although it looks promising I'm yet to implement it any projects."
12027,338.0,2008-08-15T06:31:38Z,11930,0,"Matthew: Yes, that is what I meant by ""I can get my internet IP using a service on a website."" Sorry about being glib. Brian/Nick: Traceroute would be fine except for the fact that lots of these routers have ICMP disabled and thus it always stalls. I think a combination of traceroute and uPnP will work out. That is what I was planning on doing, I as just hoping I was missing something obvious. Thank you everyone for your comments, so it sounds like I'm not missing anything obvious. I have begun implementing some bits of uPnP in order to discover the gateway."
12030,422.0,2008-08-15T06:38:21Z,11930,4,"Java doesn't make this as pleasant as other languages, unfortunately. Here's what I did: import java.io.*;
import java.util.*;

public class ExecTest {
    public static void main(String[] args) throws IOException {
        Process result = Runtime.getRuntime().exec(""traceroute -m 1 www.amazon.com"");

        BufferedReader output = new BufferedReader(new InputStreamReader(result.getInputStream()));
        String thisLine = output.readLine();
        StringTokenizer st = new StringTokenizer(thisLine);
        st.nextToken();
        String gateway = st.nextToken();
        System.out.printf(""The gateway is %s\n"", gateway);
    }
} This presumes that the gateway is the second token and not the third. If it is, you need to add an extra st.nextToken(); to advance the tokenizer one more spot."
12089,905.0,2008-08-15T08:56:02Z,11740,1,"Rather than use table prefixes instead have an application user that belongs to a schema (in MS Sql 2005 or above). This means that instead of: select * from dbo.clientAProduct
select * from dbo.clientBroduct You have: select * from clientA.Product
select * from clientB.Product"
12098,198.0,2008-08-15T09:24:51Z,10980,-2,I'm not a lawyer but all licenses mentioned are okay to be used in commercial products as long as you don't make any changes and claim the code is yours. I think if you don't wanna risk anything you should consult a lawyer.
12106,966.0,2008-08-15T09:44:36Z,10810,2,I found out from the event log that there was a security issue with the user making the call to AzMan from a remote computer. The user did not belong the local Users group on the computer running ADAM/AzMan. When I corrected that everything worked again.
12161,1075.0,2008-08-15T11:44:52Z,12140,0,"Usually this is handled by an ini file or XML configuration file. Then you just have a class that reads the setting when neeed. .NET has this built in with the ConfigurationManager classes, but it's quite easy to implement, just read text files, or load XML into DOM or parse them by hand in code. Having config files in the database is ok, but it does tie you to the database, and creates an extra dependancy for your app that ini/xml files solve."
12183,960.0,2008-08-15T12:20:26Z,12140,0,"I did this: public class MySettings
{
    public static double Setting1
        { get { return SettingsCache.Instance.GetDouble(""Setting1""); } }

    public static string Setting2
        { get { return SettingsCache.Instance.GetString(""Setting2""); } }
} I put this in a separate infrastructure module to remove any issues with circular dependencies. Doing this I am not tied to any specific configuration method, and have no strings running havoc in my applications code."
12185,1123.0,2008-08-15T12:25:19Z,12140,1,"You could use Martin Fowlers ServiceLocator pattern. In php it could look like this: class ServiceLocator {
  private static $soleInstance;
  private $globalSettings;

  public static function load($locator) {
    self::$soleInstance = $locator;
  }

  public static function globalSettings() {
    if (!isset(self::$soleInstance->globalSettings)) {
      self::$soleInstance->setGlobalSettings(new GlobalSettings());
    }
    return self::$soleInstance->globalSettings;
  }
} Your production code then initializes the service locator like this: ServiceLocator::load(new ServiceLocator()); In your test-code, you insert your mock-settings like this: ServiceLocator s = new ServiceLocator();
s->setGlobalSettings(new MockGlobalSettings());
ServiceLocator::load(s); It's a repository for singletons that can be exchanged for testing purposes."
12210,1117.0,2008-08-15T12:57:22Z,12140,1,"I like to model my configuration access off of the Service Locator pattern.  This gives me a single point to get any configuration value that I need and by putting it outside the application in a separate library, it allows reuse and testability.  Here is some sample code, I am not sure what language you are using, but I wrote it in C#. First I create a generic class that will models my ConfigurationItem. public class ConfigurationItem<T>
{
    private T item;

    public ConfigurationItem(T item)
    {
        this.item = item;
    }

    public T GetValue()
    {
        return item;
    }
} Then I create a class that exposes public static readonly variables for the configuration item.  Here I am just reading the ConnectionStringSettings from a config file, which is just xml.  Of course for more items, you can read the values from any source. public class ConfigurationItems
{
    public static ConfigurationItem<ConnectionStringSettings> ConnectionSettings = new ConfigurationItem<ConnectionStringSettings>(RetrieveConnectionString());

    private static ConnectionStringSettings RetrieveConnectionString()
    {
        // In .Net, we store our connection string in the application/web config file.
        // We can access those values through the ConfigurationManager class.
        return ConfigurationManager.ConnectionStrings[ConfigurationManager.AppSettings[""ConnectionKey""]];
    }
} Then when I need a ConfigurationItem for use, I call it like this: ConfigurationItems.ConnectionSettings.GetValue(); And it will return me a type safe value, which I can then cache or do whatever I want with. Here's a sample test: [TestFixture]
public class ConfigurationItemsTest
{
    [Test]
    public void ShouldBeAbleToAccessConnectionStringSettings()
    {
        ConnectionStringSettings item = ConfigurationItems.ConnectionSettings.GetValue();
        Assert.IsNotNull(item);
    }
} Hope this helps."
12381,1424.0,2008-08-15T15:33:42Z,12330,3,"I believe this is what you want. WMI Code Creator A part of this nifty utility allows you to browse namespaces/classes/properties on the local and remote PCs, not to mention generating WMI code in VBScript/C#/VB on the fly.  Very useful. Also, the source code used to create the utility is included in the download, which could provide a reference if you wanted to create your own browser like interface."
12413,648.0,2008-08-15T15:52:24Z,11620,5,"I usually run into that error when I am trying to restore a database I usually just go to the top of the tree in Management Studio and right click and restart the database server (because it's on a development machine, this might not be ideal in production).  This is close all database connections."
12552,22.0,2008-08-15T17:57:40Z,10870,0,"ok, I see the problem now. Rather than null, try setting it to the dragInitiator. Check this out. <?xml version=""1.0"" encoding=""utf-8""?>
<mx:WindowedApplication xmlns:mx=""http://www.adobe.com/2006/mxml"" layout=""absolute"">
    <mx:Script>
    	<![CDATA[
    		import mx.controls.Alert;
    		import mx.events.DragEvent;
    		import mx.managers.DragManager;
    		import mx.core.DragSource;

    		private function doStartDrag(e:MouseEvent):void {
    			if ( e.buttonDown && !DragManager.isDragging ) {
    			var ds:DragSource = new DragSource();
    			ds.addData(""test"", ""test"");

    			DragManager.doDrag(btn, ds, e);
    			}
    		}

    		private function handleDragOver(e:DragEvent):void {
    			if ( e.localX < cvs.width/2 ) {
    				//since null does nothing, lets just set to accept the drag
    				//operation, but accept it to the dragInitiator
    				DragManager.acceptDragDrop(e.dragInitiator);
    			}	
    			else {
    				//accept drag
    				DragManager.acceptDragDrop(cvs);
    				DragManager.showFeedback( DragManager.COPY );
    			}
    		}

    		private function handleDragDrop(e:DragEvent):void {
    			if ( e.dragSource.hasFormat(""test"") ) {
    				Alert.show(""Got a drag drop!"");
    			}
    		}
    	]]>
    </mx:Script>
    <mx:Canvas x=""265"" y=""66"" width=""321"" height=""245"" backgroundColor=""#FF0000"" id=""cvs"" dragOver=""handleDragOver(event)"" dragDrop=""handleDragDrop(event)"">
    </mx:Canvas>
    <mx:Button id=""btn"" x=""82"" y=""140"" label=""Drag Me"" mouseDown=""doStartDrag(event)""/>
</mx:WindowedApplication>"
12585,72.0,2008-08-15T18:36:14Z,4430,17,I've had great success with wsdl2php .  It will automatically create wrapper classes for all objects and methods used in your web service.
12728,71.0,2008-08-15T20:51:14Z,12720,3,"You'll want to setup launch condition in your deployment project to make sure version 2.0 SP1 is installed. You'll want to set a requirement based off the MsiNetAssemblySupport variable, tied to the version number of .NET 2.0 SP1 (2.0.50727.1433 and above according to this page .) Bootstrapping the project to actually download the framework if it isn't installed is a different matter, and there are plenty of articles out there on how to do that."
12799,1483.0,2008-08-15T21:44:25Z,10190,1,"There are a few solutions which I use with MS SQL 2005. One of them is ROW NUMBER(). But, personally, I don't like ROW NUMBER() because it doesn't work for big results (DB which I work on is really big -- over 1TB data running thousands of queries in second -- you know -- big social networking site). Here are my favourite solution. I will use kind of pseudo code of T-SQL. Let's find 2nd page of users sorted by forename, surname, where each page has 10 records. @page = 2 -- input parameter
@size = 10 -- can be optional input parameter

if @page < 1 then begin
    @page = 1 -- check page number
end
@start = (@page-1) * @size + 1 -- @page starts at record no @start

-- find the beginning of page @page
SELECT TOP (@start)
    @forename = forename,
    @surname = surname
    @id = id
FROM
    users
ORDER BY
    forename,
    surname,
    id -- to keep correct order in case of have two John Smith.

-- select @size records starting from @start
SELECT TOP (@size)
    id,
    forename,
    surname
FROM
    users
WHERE
    (forename = @forename and surname = @surname and id >= @id) -- the same name and surname, but bigger id
    OR (forename = @forename and surname > @surname) -- the same name, but bigger surname, id doesn't matter
    OR (forename > @forename) -- bigger forename, the rest doesn't matter
ORDER BY
    forename,
    surname,
    id"
12820,1249.0,2008-08-15T22:07:40Z,10190,3,"LINQ combined with lambda expressions and anonymous classes in .Net 3.5 hugely simplifies this sort of thing. Querying the database: var customers = from c in db.customers
                join p in db.purchases on c.CustomerID equals p.CustomerID
                where p.purchases > 5
                select c; Number of records per page: customers = customers.Skip(pageNum * pageSize).Take(pageSize); Sorting by any column: customers = customers.OrderBy(c => c.LastName); Getting only selected fields from server: var customers = from c in db.customers
                join p in db.purchases on c.CustomerID equals p.CustomerID
                where p.purchases > 5
                select new
                {
                    CustomerID = c.CustomerID,
                    FirstName = c.FirstName,
                    LastName = c.LastName
                }; This creates a statically-typed anonymous class in which you can access its properties: var firstCustomer = customer.First();
int id = firstCustomer.CustomerID; Results from queries are lazy-loaded by default, so you aren't talking to the database until you actually need the data. LINQ in .Net also greatly simplifies updates by keeping a datacontext of any changes you have made, and only updating the fields which you change."
12878,122.0,2008-08-15T23:16:52Z,12870,5,"You could do essentially the same code with Hashtables (or some other Map): Hashtable<String, Hashtable<String, String>> schedule
    = new Hashtable<String, Hashtable<String, String>>();
schedule.put(""A"", new Hashtable<String, String>());
schedule.put(""B"", new Hashtable<String, String>());
schedule.put(""C"", new Hashtable<String, String>());
schedule.put(""D"", new Hashtable<String, String>());
schedule.put(""E"", new Hashtable<String, String>());

schedule.get(""A"").put(""Winter"", ""M"");
schedule.get(""A"").put(""Spring"", ""tTS"");
// Etc... Not as elegant, but then again, Java isn't a dynamic language, and it doesn't have hashes on the language level. Note: You might be able to do a better solution, this just popped in my head as I read your question."
12882,122.0,2008-08-15T23:24:37Z,12880,10,"Check out the netflix contest .  I believe they exposed their database, or a large subset, to facilitate the contest. UPDATE: Their faq says they have 100 million entries in the subset you can download."
12885,872.0,2008-08-15T23:27:13Z,12870,-1,"There is no pretty solution.  Java just doesn't do things like this well.  Mike's solution is pretty much the way to do it if you want strings as the indices (keys).  Another option if the hash-of-hashes setup is too ugly is to append the strings together (shamelessly stolen from Mike and modified): Hashtable<String, String> schedule = new Hashtable<String, String>();
schedule.put(""A-Winter"", ""M"");
schedule.put(""A-Spring"", ""tTS""); and then lookup: String val = schedule.get(group + ""-"" + season); If you're unhappy with the general ugliness (and I don't blame you), put it all behind a method call: String whenCanIWater(String group, Date date) { /* ugliness here */ }"
12887,122.0,2008-08-15T23:30:00Z,12870,1,"@Brian Warshaw FYI, with Java 1.5, primitives are now autoboxed to the wrapped version, so you can call it with just the primitive: Hashtable<String, Integer> hash = new Hashtable<String, Integer>();
hash.put(""key"", 15); // Works from Java 1.5 on"
12892,748.0,2008-08-15T23:37:46Z,10670,0,Take a look at Asynchronous Pages in ASP.NET .
12895,5.0,2008-08-15T23:40:50Z,12880,1,"You might want to look at generating random data for Fuzz Testing . That would give you a pretty much unlimited amount of test data, and you're more likely to hit edge cases. Maybe some more information on what kind of test data you want, what format, and for what types of applications?"
12899,958.0,2008-08-15T23:56:20Z,12870,2,"I'm not a Java programmer, but getting away from Java and just thinking in terms that are more language agnostic - a cleaner way to do it might be to use either constants or enumerated types.  This should work in any langauge that supports multi-dimensional arrays. If using named constants, where, for example: int A = 0;
int B = 1;
int C = 2;
int D = 3;

int Spring = 0; 
int Summer = 1;
int Winter = 2; 
int Fall = 3;
... Then the constants serve as more readable array subscripts: schedule[A][Winter]=""M"";
schedule[A][Spring]=""tTS"";
schedule[A][Summer]=""Any"";
schedule[A][Fall]=""tTS"";
schedule[B][Winter]=""t""; Using enumerated types: enum groups
{
  A = 0,
  B = 1,
  C = 2,
  D = 3
}

enum seasons
{
  Spring = 0,
  Summer = 1,
  Fall = 2,
  Winter = 3
}
...
schedule[groups.A][seasons.Winter]=""M"";
schedule[groups.A][seasons.Spring]=""tTS"";
schedule[groups.A][seasons.Summer]=""Any"";
schedule[groups.A][seasons.Fall]=""tTS"";
schedule[groups.B][seasons.Winter]=""t"";"
12900,122.0,2008-08-15T23:58:13Z,12890,1,"I know this is a bit tangential, but have you tried seeing if there are more indexes you can add? I don't have a lot of DB background, but I am working with databases a lot recently, and I've been finding that a lot of the queries can be improved just by adding indexes. We are using DB2, and there is a command called db2expln and db2advis, the first will indicate whether table scans vs index scans are being used, and the second will recommend indexes you can add to improve performance.  I'm sure MySQL has similar tools... Anyways, if this is something you haven't considered yet, it has been helping a lot with me... but if you've already gone this route, then I guess it's not what you are looking for. Another possibility is a ""materialized view"" (or as they call it in DB2), which lets you specify a table that is essentially built of parts from multiple tables.  Thus, rather than normalizing the actual columns, you could provide this view to access the data... but I don't know if this has severe performance impacts on inserts/updates/deletes (but if it is ""materialized"", then it should help with selects since the values are physically stored separately)."
12902,72.0,2008-08-16T00:04:20Z,12890,1,"MySQL 5 does support views , which may be helpful in this scenario.  It sounds like you've already done a lot of optimizing, but if not you can use MySQL's EXPLAIN syntax to see what indexes are actually being used and what is slowing down your queries. As far as going about normalizing data (whether you're using views or just duplicating data in a more efficient manner), I think starting with the slowest queries and working your way through is a good approach to take."
12917,872.0,2008-08-16T00:20:37Z,12890,0,"You might also want to consider selecting into a temporary table and then performing queries on that temporary table.  This would avoid the need to rejoin your tables for every single query you issue (assuming that you can use the temporary table for numerous queries, of course).  This basically gives you denormalized data, but if you are only doing select calls, there's no concern about data consistency."
12918,268.0,2008-08-16T00:21:05Z,12870,8,"Don't try to be as dynamic as PHP is. You could try to first define what you need. interface Season
{
    public string getDays();
}

interface User
{
    public Season getWinter();
    public Season getSpring();
    public Season getSummer();
    public Season getFall();
}

interface UserMap
{
    public User getUser(string name);
} And please, read the documentation of Hashtable before using it. This class is synchronized which means that each call is protected against multithreading which really slows the access when you don't need the extra protection. Please use any Map implementation instead like HashMap or TreeMap ."
12923,1195.0,2008-08-16T00:28:34Z,12870,4,"It seems like everyone is trying to find the Java way to do it like you're doing it in PHP, instead of the way it ought to be done in Java. Just consider each piece of your array an object, or, at the very least, the first level of the array as an object and each sub level as variables inside the object. The build a data structure that you populate with said objects and access the objects through the data structure's given accessors. Something like: class Schedule
{
  private String group;
  private String season;
  private String rundays;
  public Schedule() { this.group = null; this.season = null; this.rundays= null; }
  public void setGroup(String g) { this.group = g; }
  public String getGroup() { return this.group; }
  ...
}

public ArrayList<Schedule> schedules = new ArrayList<Schedule>();
Schedule s = new Schedule();
s.setGroup(...);
...
schedules.add(s);
... Of course that probably isn't right either. I'd make each season an object, and maybe each weekday list as an object too. Anyway, its more easily reused, understood, and extensible than a hobbled-together Hashtable that tries to imitate your PHP code. Of course, PHP has objects too, and you should use them in a similar fashion instead of your uber-arrays, wherever possible. I do understand the temptation to cheat, though. PHP makes it so easy, and so fun!"
12932,1471.0,2008-08-16T00:53:11Z,12870,0,"I agree that you should definitely put this logic behind the clean interface of: public String lookupDays(String group, String date); but maybe you should stick the data in a properties file.  I'm not against hardcoding this data in your source files but, as you noticed, Java can be pretty wordy when it comes to nested Collections.  Your file might looks like: A.Summer=M A.Spring=tTS B.Summer=T Usually I don't like to move static data like this to an external file because it increases the ""distance"" between the data and the code that uses it.  However, whenever you're dealing with nested Collections, especially maps, things can get real ugly, real fast. If you don't like this idea, maybe you can do something like this: public class WaterScheduler
{
  private static final Map<String, String> GROUP2SEASON = new HashMap<String, String>();
  static
  {
    addEntry(""A"", ""Summer"", ""M"");
    addEntry(""A"", ""Spring"", ""tTS"");
    addEntry(""B"", ""Summer"", ""T"");
  }

  private static void addEntry(String group, String season, String value)
  {
    GROUP2SEASON.put(group + ""."" + season, value);
  }

} You lose some readability but at least the data is closer to where it's going to be used."
12941,872.0,2008-08-16T01:07:35Z,12870,1,"I'm totally at a loss as to why some of you seem to think that throwing gobs of objects at the code is the way to go.  For example, there are exactly four seasons, and they don't do or store anything.  How does it simplify anything to make them objects?  Wing is quite right that these should probably be constants (or maybe enums). What Bruce needs, at it's heart, is simply a lookup table.  He doesn't need a hierarchy of objects and interfaces; he needs a way to look up a schedule based on a season and a group identifier.  Turning things into objects only makes sense if they have responsibilities or state.  If they have neither, then they are simply identifiers, and building special objects for them just makes the codebase larger. You could build, e.g., Group objects that each contain a set of schedule strings (one for each season), but if all the Group object does is provide lookup functionality, then you've reinvented the lookup table in a much less intuitive fashion.  If he has to look up the group, and then lookup the schedule, all he has is a two-step lookup table that took longer to code, is more likely to be buggy, and will be harder to maintain."
12942,493.0,2008-08-16T01:08:54Z,12880,1,"I don't know what your target platform is, but if you're developing against a MSSQL database check out Visual Studio for Database Professionals . It has a very cool feature where it can generate data for your schema using a data plan that you can define. Redgate also has a datageneration tool, but I haven't used it. The advantage is that you can create a data generation plan and use it to populate your database with consistent, large amounts of data which can be tuned to test specific areas of your schema."
12950,1219.0,2008-08-16T01:36:31Z,12890,9,"I know more about mssql that mysql, but I don't think the number of joins or number of rows you are talking about should cause you too many problems with the correct indexes in place.  Have you analyzed the query plan to see if you are missing any? http://dev.mysql.com/doc/refman/5.0/en/explain.html That being said, once you are satisifed with your indexes and have exhausted all other avenues, de-normalization might be the right answer.  If you just have one or two queries that are problems, a manual approach is probably appropriate, whereas some sort of data warehousing tool might be better for creating a platform to develop data cubes. Here's a site I found that touches on the subject: http://www.meansandends.com/mysql-data-warehouse/?link_body%2Fbody=%7Bincl%3AAggregation%7D Here's a simple technique that you can use to keep denormalizing queries simple, if you're just doing a few at a time (and I'm not replacing your OLTP tables, just creating a new one for reporting purposes).  Let's say you have this query in your application: select a.name, b.address from tbla a 
join tblb b on b.fk_a_id = a.id where a.id=1 You could create a denormalized table and populate with almost the same query: create table tbl_ab (a_id, a_name, b_address); 
-- (types elided) Notice the underscores match the table aliases you use insert tbl_ab select a.id, a.name, b.address from tbla a
join tblb b on b.fk_a_id = a.id 
-- no where clause because you want everything Then to fix your app to use the new denormalized table, switch the dots for underscores. select a_name as name, b_address as address 
from tbl_ab where a_id = 1; For huge queries this can save a lot of time and makes it clear where the data came from, and you can re-use the queries you already have. Remember, I'm only advocating this as the last resort.  I bet there's a few indexes that would help you.  And when you de-normalize, don't forget to account for the extra space on your disks, and figure out when you will run the query to populate the new tables.  This should probably be at night, or whenever activity is low.  And the data in that table, of course, will never exactly be up to date. [Yet another edit]  Don't forget that the new tables you create need to be indexed too!  The good part is that you can index to your heart's content and not worry about update lock contention, since aside from your bulk insert the table will only see selects."
12953,737.0,2008-08-16T01:46:37Z,12870,0,"I think Ian is absolutely right: stop trying to implement your PHP code in Java.  Instead, take a step back and think about how you might design this from scratch.  In particular, why not put all that data into a database, instead of hard-coding it in your sources or using properties files?  Using a database will be much easier to maintain, and there are a variety of free database engines to choose from."
12957,872.0,2008-08-16T02:03:25Z,12870,1,"@Jason In particular, why not put all that data into a database, instead of hard-coding it in your sources or using properties files? Using a database will be much easier to maintain, and there are a variety of free database engines to choose from. Adding a database is an incredibly heavyweight way to solve a problem that fits easily into a text file (or even directly into the source).  There are 5 groups and 4 seasons.  That means there are going to be a total of 20 records in the database."
12966,1061.0,2008-08-16T02:30:31Z,12890,2,"In line with some of the other comments, i would definately have a look at your indexing. One thing i discovered earlier this year on our MySQL databases was the power of composite indexes. For example, if you are reporting on order numbers over date ranges, a composite index on the order number and order date columns could help. I believe MySQL can only use one index for the query so if you just had separate indexes on the order number and order date it would have to decide on just one of them to use. Using the EXPLAIN command can help determine this. To give an indication of the performance with good indexes (including numerous composite indexes), i can run queries joining 3 tables in our database and get almost instant results in most cases. For more complex reporting most of the queries run in under 10 seconds. These 3 tables have 33 million, 110 million and 140 millions rows respectively. Note that we had also already normalised these slightly to speed up our most common query on the database. More information regarding your tables and the types of reporting queries may allow further suggestions."
12970,737.0,2008-08-16T02:46:53Z,12870,0,"@ Derek Adding a database is an incredibly heavyweight way to solve a problem that fits easily into a text file (or even directly into the source). There are 5 groups and 4 seasons. That means there are going to be a total of 20 records in the database. Two of the database engines I linked to are implemented entirely in Java and can be embedded in an application just by including a jar file.  It's a little heavyweight, sure, but it's a lot more scalable and easier to maintain.  Just because there are 20 records today doesn't mean there won't be more later due to changing requirements or feature creep. If in a few weeks or months you decide you want to add, say, time of day watering restrictions, it will be much easier to add that functionality if you're already using a database.  Even if that never happens, then you've spent a few hours learning how to embed a database in an application."
12972,1061.0,2008-08-16T02:53:00Z,12890,0,"Further to my previous answer, another approach we have taken in some situations is to store key reporting data in separate summary tables. There are certain reporting queries which are just going to be slow even after denormalising and optimisations and we found that creating a table and storing running totals or summary information throughout the month as it came in made the end of month reporting much quicker as well. We found this approach easy to implement as it didn't break anything that was already working - it's just additional database inserts at certain points."
12974,872.0,2008-08-16T03:12:50Z,12870,1,"@Jason Two of the database engines I linked to are implemented entirely in Java and can be embedded in an application just by including a jar file. It's a little heavyweight, sure, but it's a lot more scalable and easier to maintain. Just because there are 20 records today doesn't mean there won't be more later due to changing requirements or feature creep. If in a few weeks or months you decide you want to add, say, time of day watering restrictions, it will be much easier to add that functionality if you're already using a database. Even if that never happens, then you've spent a few hours learning how to embed a database in an application. Embedding a DB in Java doesn't make it easier to maintain.  There's now an additional code dependency that did not previously exist.  Updating the set of schedules is now more difficult, as either a custom tool must be coded or a DB-specific interface must be used, whereas previous notepad.exe was sufficient.  Scalability is not a concern here, either.  The needs of this system could increase by a million and the flat file would still work just fine. Certainly, it's possible that in the future needs will evolve to the point that it's worth moving to a database.  That's when you make the change.  Trying to future-proof an application never works, because we always assume incorrectly about future needs.  We can't say it'll only take a few hours to embed the database either, because we suck at estimating schedules, too.  If there even comes a time when the database is appropriate, then refactor the code to use a database.  In the meantime, do the simplest thing that could possibly work ."
12992,543.0,2008-08-16T04:35:49Z,12870,0,"Does the ""date"" have to be a parameter? If you're just showing the current watering schedule the WateringSchedule class itself can figure out what day it is, and therefore what season it is. Then just have a method which returns a map where the Key is the group letter. Something like: public Map<String,List<String>> getGroupToScheduledDaysMap() {
  // instantiate a date or whatever to decide what Map to return
} Then in the JSP page <c:forEach var=""day"" items=""${scheduler.groupToScheduledDaysMap[""A""]}"">
   ${day}
</c:forEach> If you need to show the schedules for more than one season, you should have a method in the WateringSchedule class that returns a map where Seasons are the keys, and then Maps of groupToScheduledDays are the values."
13003,813.0,2008-08-16T05:23:09Z,13000,5,"Most PHP sites should have a file (I call it a header) that you include on every single page of the site. If you put that first line of code in the header file, then include it like this on every page: include 'header.php'; you won't have to use the global keyword or anything, the second line of code you wrote should work. Edit: Oh sorry, that won't work inside functions... now I see your problem. Edit #2: Ok, take my original advice with the header, but use a define() rather than a variable. Those work inside functions after being included."
13005,834.0,2008-08-16T05:27:22Z,13000,-1,"IIRC a common solution is a plain file that contains your declarations, that you include in every source file, something like ' constants.inc.php '. There you can define a bunch of application-wide variables that are then imported in every file. Still, you have to provide the include directive in every single source file you use. I even saw some projects using this technique to provide localizations for several languages. I'd prefer the gettext way, but maybe this variant is easier to work with for the average user. edit For your problem I recomment the use of $GLOBALS[] , see Example #2 for details. If that's still not applicable, I'd try to digg down PHP5 objects and create a static Singleton that provides needed static constants ( http://www.developer.com/lang/php/article.php/3345121 )"
13006,1448.0,2008-08-16T05:28:04Z,12880,1,"You might also want to check out theinfo by Aaron Swartz. From the site This is a site for large data sets and
  the people who love them: the scrapers
  and crawlers who collect them, the
  academics and geeks who process them,
  the designers and artists who
  visualize them. It's a place where
  they can exchange tips and tricks,
  develop and share tools together, and
  begin to integrate their particular
  projects."
13008,115.0,2008-08-16T05:34:03Z,13000,-1,"Sessions are going to be your best bet, if the data is user specific, else just use a conifg file.
config.php: <?php
$EL = ""\n<br />\n"";
?> Then on each page add require 'config.php' the you will be able to access $EL on that page."
13020,893.0,2008-08-16T05:56:03Z,12880,2,"I've done some work with the Wikimedia download sets, which are huge XML files. Unfortunately, their download server appears to be currently having disk space issues so many of the data sets aren't available. But when it's available, the entire English Wikipedia data set with full history is 2.8 TB (18 GB compressed)."
13037,795.0,2008-08-16T07:37:11Z,12870,3,"Here's one way it could look like, you can figure the rest out: A = new Group();
A.getSeason(Seasons.WINTER).addDay(Days.MONDAY);
A.getSeason(Seasons.SPRING).addDay(Days.TUESDAY).addDay(Days.THURSDAY);
A.getSeason(Seasons.SPRING).addDays(Days.MONDAY, Days.TUESDAY, ...);

schedule = new Schedule();
schedule.addWateringGroup( A );"
13058,868.0,2008-08-16T08:44:31Z,13000,3,Sounds like the job of a constant. See the function define() .
13061,905.0,2008-08-16T08:49:21Z,13060,3,"out means that the parameter will be initialised by the method: int result; //not initialised

if( int.TryParse( ""123"", out result ) )
   //result is now 123
else
   //if TryParse failed result has still be 
   // initialised to its default value (0) ref will force the underlying reference to be passed: void ChangeMyClass1( MyClass input ) {
   input.MyProperty = ""changed by 1"";
   input = null;
   //can't see input anymore ... 
   // I've only nulled my local scope's reference
}

void ChangeMyClass2( ref MyClass input ) {
   input.MyProperty = ""changed by 2"";
   input = null;
   //the passed reference is now null too.
}

MyClass tester = new MyClass { MyProperty = ""initial value"" };

ChangeMyClass1( tester );
// now tester.MyProperty is ""changed by 1""

ChangeMyClass2( ref tester );
// now tester is null"
13065,718.0,2008-08-16T09:13:36Z,13060,1,"This article "" Parameter passing in C# "" is the most complete explanation I've read so far."
13070,46.0,2008-08-16T09:21:35Z,13060,2,"One of my own questions at stackoverflow handles this topic too. It handles about ""pass by reference"" and ""pass by value"" in different types of languages, c# is included so maybe you can find some extra information there as well. Basically it comes down to: ref: the parameter with the ref keyword will be passed by reference out: the parameter with the out keyword will be treated as an output parameter but that's really the most basic answer you can give, as it is a little more complex than it is stated here"
13075,115.0,2008-08-16T09:26:18Z,13000,0,"@svec yes this will, you just have to include the file inside the function also.  This is how most of my software works. function myFunc()
 {
require 'config.php';
//Variables from config are available now.
 }"
13094,1344.0,2008-08-16T11:21:08Z,13000,1,"Are you using PHP5? If you define the __autoload() function and use a class with some constants, you can call them where you need them. The only aggravating thing about this is that you have to type something a little longer, like MyClass::MY_CONST The benefit is that if you ever decide to change the way that you handle new lines, you only have to change it in one place. Of course, a possible negative is that you're calling including an extra function (__autoload()), running that function (when you reference the class), which then loads another file (your class file). That might be more overhead than it's worth. If I may offer a suggestion, it would be avoiding this sort of echoing that requires echoing tags (like <br /> ). If you could set up something a little more template-esque, you could handle the nl's without having to explicitly type them. So instead of echo ""Blah Blah Blah\n<br />\n""; try: <?php
if($condition) {
?>
<p>Blah blah blah
<br />
</p>
<?php
}
?> It just seems to me like calling up classes or including variables within functions as well as out is a lot of work that doesn't need to be done, and, if at all possible, those sorts of situations are best avoided."
13099,304.0,2008-08-16T12:04:54Z,12870,1,"I'm with those that suggest encapsulating function in objects. import java.util.Date;
import java.util.Map;
import java.util.Set;

public class Group {

    private String groupName;

    private Map<Season, Set<Day>> schedule;

    public String getGroupName() {
    	return groupName;
    }

    public void setGroupName(String groupName) {
    	this.groupName = groupName;
    }

    public Map<Season, Set<Day>> getSchedule() {
    	return schedule;
    }

    public void setSchedule(Map<Season, Set<Day>> schedule) {
    	this.schedule = schedule;
    }

    public String getScheduleFor(Date date) {
    	Season now = Season.getSeason(date);
    	Set<Day> days = schedule.get(now);
    	return Day.getDaysForDisplay(days);
    }

} EDIT: Also, your date ranges don't take leap years into account: Our seasons look like this: Summer
  (5-1 to 8-31) Spring (3-1 to 4-30)
  Fall (9-1 to 10-31) Winter (11-1 to
  2-28)"
13100,891.0,2008-08-16T12:06:17Z,13060,19,"By default (in C#), passing an object to a function actually passes a copy of the reference to that object. Changing the parameter itself only changes the value in the parameter, and not the variable that was specified. void Test1(string param)
{
    param = ""new value"";
}

string s1 = ""initial value"";
Test1(s1);
// s1 == ""initial value"" Using out or ref passes a reference to the variable specified in the call to the function. Any changes to the value of an out or ref parameter will be passed back to the caller. Both out and ref behave identically except for one slight difference: ref parameters are required to be initialised before calling, while out parameters can be uninitialised. By extension, ref parameters are guaranteed to be initialised at the start of the method, while out parameters are treated as uninitialised. void Test2(ref string param)
{
    param = ""new value"";
}

void Test3(out string param)
{
    // Use of param here will not compile
    param = ""another value"";
}

string s2 = ""initial value"";
string s3;
Test2(ref s2);
// s2 == ""new value""
// Test2(ref s3); // Passing ref s3 will not compile
Test3(out s2);
// s2 == ""another value""
Test3(out s3);
// s3 == ""another value"" Edit : As dp points out, the difference between out and ref is only enforced by the C# compiler, not by the CLR. As far as I know, VB has no equivalent for out and implements ref (as ByRef ) only, matching the support of the CLR."
13105,748.0,2008-08-16T12:32:38Z,13060,6,"One additional note about ref vs. out: The distinction between the two is enforced by the C# compiler. The CLR does not distinguish between between out and ref. This means that you cannot have two methods whose signatures differ only by an out or ref void foo(int value) {}

// Only one of the following would be allowed

// valid to overload with ref
void foo(ref int value) {}

// OR with out
void foo(out int value) {}"
13164,423.0,2008-08-16T15:04:56Z,13160,2,"It depends on how often you are going to be calling the web service.  If you're going to be calling it almost constantly, it would probably be better to use method #2.  However, if it's not going to be getting called quite so often, you are better off using method #1, and only instantiating it when you need it."
13171,1130097.0,2008-08-16T15:24:19Z,13160,0,"Right now I made a solution for a mobile device and it turns to be used on irregular times, it could be used in 10 minutes, 1 hour, 4 hours its very variable, it seems that the better aproach is the first strategy. Last year we went on a project where we used webservices, the fact is that we instantiated our webservices at the Sub New() procedure and it run it very well, however, sometimes some users claimed at us that they woke up from their chairs and when they returned and tried to continue on the application they received a timeout error message and they had to re-login again. We thougth that maybe that was Ok because maybe the users went out for a very long time out of their seats, but once in a presentation of the application with the CEOs it happened exactly the same scenario and personally I didn't like that behaviour and that's why the question. Thanks for the answer."
13207,1554.0,2008-08-16T16:18:58Z,13170,1,"A ThreadStateException is thrown because you're trying to start a thread that's not in a startable state. The most likely situations would be that it's already running, or that it has fully exited. There are potentially a couple things that might be happening. First is, the thread might have transitioned from Running to StopRequested, which isn't fully stopped yet, so your logic doesn't create a new thread, and you're trying to start a thread which has just finished running or is about to finish running (neither of which is a valid state for restarting). The other possibility is that the thread was aborted. Threads which are aborted go to the Aborted state, not the Stopped state, and of course are also not valid for restarting. Really, the only kind of thread that is still alive that can be ""restarted"" is one that's suspended. You might want to use this conditional instead: if (this.mThread == null || this.mThread.ThreadState != ThreadState.Suspended)"
13212,905.0,2008-08-16T16:23:57Z,13200,0,"CC comes with a utility that warns you when builds fail, it probably isn't worth logging the failing build in FogBugz - you don't need to track issues that are immediately resolved (as most broken builds will be) To go the other way round (FogBugz showing checkins that fixed the issue) you need a web based repository browser - FogBugz is easy to configure so that it shows the right changes."
13214,887.0,2008-08-16T16:25:07Z,13170,6,"It's possible for a thread to be in more than one state at once therefore the ThreadState property is actually a bitmap of possible states. So testing for equality with just one state will not give you the right result. You would need to do something like: if((mThread.ThreadState & ThreadState.Running) != 0) However, checking thread state is the wrong to do anything. I'm not entirely clear what you're trying to achieve but I will guess that you're waiting for a thread to terminate before restarting it. In that case you should do: mThread.Join();
mThread = new Thread(new ParameterizedThreadStart(Monitor));
if(check)
    mThread.Start(60000);
else
    mThread.Start(0); Although if you describe the problem you're trying to solve in more detail I'm almost certain there will be a better solution. Waiting around for a thread to end just to restart it again doesn't seem that efficient to me. Perhaps you just need some kind of inter-thread communication? John."
13221,1407.0,2008-08-16T16:35:18Z,2970,0,"We had been hacked from same guys apparently! Or bots, in our case. They used SQL injection in URL on some old classic ASP sites that nobody maintain anymore. We found attacking IPs and blocked them in IIS. Now we must refactor all old ASP.
So, my advice is to take a look at IIS logs first, to find if problem is in your site's code or server configuration."
13255,267.0,2008-08-16T17:56:47Z,13170,3,"The problem is that you have code that first checks if it should create a new thread object, and another piece of code that determines wether to start the thread object. Due to race conditions and similar things, your code might end up trying to call .Start on an existing thread object. Considering you don't post the details behind the check variable, it's impossible to know what might trigger this behavior. You should reorganize your code so that .Start is guaranteed to only be called on new objects. In short, you should put the Start method into the same if-statement as the one that creates a new thread object. Personally, I would try to reorganize the entire code so that I didn't need to create another thread, but wrap the code inside the thread object inside a loop so that the thread just keeps on going."
13259,1557.0,2008-08-16T18:02:03Z,13200,4,"At my company we've recently adopted the (commercial) Atlassian stack - including JIRA for issue tracking and Bamboo for builds. Much like the Microsoft world (I'm guessing - we're a Java shop), if you get all your products from a single vendor you get the bonus of tight integration. For an example of how they've done interoperability, view their interoperability page . Enough shilling. Generally speaking, I can summarize their general approach as: Create issues in your bug tracker (ex: issue key of PROJ-123). When you commit code, add ""PROJ-123"" to your commit comment to indicate what bug this code change fixes. When your CI server checks out the code, scan the commit comments of the diffs. Record any strings matching the regex of your issue keys. When the build completes, generate a report of what issue keys were found. Specifically to your second problem: Your CI doesn't doesn't have to put anything into your bug tracker. Bamboo doesn't put anything into JIRA. Instead, the Atlassian folks have provided a plugin to JIRA that will make a remote api call into Bamboo, asking the question ""Bamboo, to what builds am I (a JIRA issue) related?"". This is probably best explained with a screenshot ."
13277,1557.0,2008-08-16T18:48:43Z,4080,11,"All of the following we use and integrate easiy in both our Maven 2.x builds and Eclipse/RAD 7: Testing - JUnit/TestNG Code analysis - FindBugs, PMD Code coverage - Clover In addition, in our Maven builds we have: JDepend Tag checker (TODO, FIXME, etc) Furthermore, if you're using Maven 2.x, CodeHaus has a collection of handy Maven plugins in their Mojo project . Note: Clover has out-of-the-box integration with the Bamboo CI server (since they're both Atlassian products). There are also Bamboo plugins for FindBugs, PMD, and CheckStyle but, as noted, the free Hudson CI server has those too."
13318,1472.0,2008-08-16T19:56:45Z,13200,3,"All the CI setups I've worked with send an email (to a list), but if you did wantespecially if your team uses FogBugz much as a todo systemyou could just open a case in FogBugz 6. It has an API that lets you open cases. For that matter, you could just configure it to send the email to your FogBugz' email submission address, but the API might let you do more, like assign the case to the last committer. Brian 's answer suggests to me, if your CI finds a failure in a commit that had a case number, you might even just reopen the existing case. Like codifying a case field for every little thing, though, there's a point where the CI automation could be ""too smart,"" get it wrong, and just be annoying. Opening a new case could be plenty. And thanks: this makes me wonder if I should try integrating our Chimps setup with our FogBugz!"
13442,1117.0,2008-08-17T01:01:49Z,13430,3,"I have never tried on Server 2008, but I have installed CruiseControl.NET on Vista which includes IIS 7.0.  I don't remember there being any problems.  You do have an admin background which should help if something does pop up. Just use the CruiseControl.NET wiki to get you thru the install and getting it setup.  That is all I did."
13463,1554.0,2008-08-17T01:39:54Z,13460,8,"Think of source control as a giant ""Undo"" button for your source code. Every time you check in, you're adding a point to which you can roll back. Even if you don't use branching/merging, this feature alone can be very valuable. Additionally, by having one 'authoritative' version of the source control, it becomes much easier to back up. Centralized vs. distributed... the difference is really that in distributed, there isn't necessarily one 'authoritative' version of the source control, although in practice people usually still do have the master tree. The big advantage to distributed source control is two-fold: When you use distributed source control, you have the whole source tree on your local machine. You can commit, create branches, and work pretty much as though you were all alone, and then when you're ready to push up your changes, you can promote them from your machine to the master copy. If you're working ""offline"" a lot, this can be a huge benefit. You don't have to ask anybody's permission to become a distributor of the source control. If person A is running the project, but person B and C want to make changes, and share those changes with each other, it becomes much easier with distributed source control."
13465,1553.0,2008-08-17T01:44:24Z,13460,6,"I recommend checking out the following from Eric Sink: http://www.ericsink.com/scm/source_control.html Having some sort of revision control system in place is probably the most important tool a programmer has for reviewing code changes and understanding who did what to whom. Even for single person projects, it is invaluable to be able to diff current code against previous known working version to understand what might have gone wrong due to a change."
13466,763.0,2008-08-17T01:47:18Z,13460,1,"Even if you don't branch, you may find it useful to use tags to mark releases. Imagine that you rolled out a new version of your software yesterday and have started making major changes for the next version. A user calls you to report a serious bug in yesterday's release. You can't just fix it and copy over the changes from your development trunk because the changes you've just made the whole thing unstable. If you had tagged the release, you could check out a working copy of it and use it to fix the bug. Then, you might choose to create a branch at the tag and check the bug fix into it. That way, you can fix more bugs on that release while you continue to upgrade the trunk. You can also merge those fixes into the trunk so that they'll be present in the next release."
13467,,2008-08-17T01:47:59Z,13460,3,"Here are two articles that are very helpful for understanding the basics. Beyond being informative, Sink's company sells a great source control product called Vault that is free for single users (I am not affiliated in any way with that company). http://www.ericsink.com/scm/source_control.html http://betterexplained.com/articles/a-visual-guide-to-version-control/ Vault info at www.vault.com."
13468,1436.0,2008-08-17T01:51:00Z,13460,1,"The common standard for setting up Subversion is to have three folders under the root of your repository: trunk, branches and tags.  The trunk folder holds your current ""main"" line of development. For many shops and situations, this is all they ever use... just a single working repository of code. The tags folder takes it one step further and allows you to ""checkpoint"" your code at certain points in time. For example, when you release a new build or sometimes even when you simply make a new build, you ""tag"" a copy into this folder. This just allows you to know exactly what your code looked like at that point in time. The branches folder holds different kinds of branches that you might need in special situations. Sometimes a branch is a place to work on experimental feature or features that might take a long time to get stable (therefore you don't want to introduce them into your main line just yet). Other times, a branch might represent the ""production"" copy of your code which can be edited and deployed independently from your main line of code which contains changes intended for a future release. Anyway, this is just one aspect of how to set up your system, but I think giving some thought to this structure is important."
13475,1585.0,2008-08-17T02:18:06Z,13470,2,"I don't think it has any purpose. But because RegEx is almost impossible to understand/decompose, people rarely point out errors. That is probably why no one else pointed it out."
13483,430.0,2008-08-17T02:46:42Z,13470,3,"@ Rob : I disagree.  To enforce what you are asking for I think you would need to use negative-look-behind, which is possible but is certainly not related to use {1}.  Neither version of the regexp address that particular issue. To let the code speak: tibook 0 /home/jj33/swap > cat text
Text this is <http://example.com> text this is
Text this is <http://http://example.com> text this is
tibook 0 /home/jj33/swap > cat p
#!/usr/bin/perl

my $re1 = '((mailto\:|(news|(ht|f)tp(s?))\://){1}\S+)';
my $re2 = '((mailto\:|(news|(ht|f)tp(s?))\://)\S+)';

while (<>) {
  print ""Evaluating: $_"";
  print ""re1 saw \$1 = $1\n"" if (/$re1/);
  print ""re2 saw \$1 = $1\n"" if (/$re2/);
}
tibook 0 /home/jj33/swap > cat text | perl p
Evaluating: Text this is <http://example.com> text this is
re1 saw $1 = <http://example.com>
re2 saw $1 = <http://example.com>
Evaluating: Text this is <http://http://example.com> text this is
re1 saw $1 = <http://http://example.com>
re2 saw $1 = <http://http://example.com>
tibook 0 /home/jj33/swap > So, if there is a difference between the two versions, it's doesn't seem to be the one you suggest."
13484,1199.0,2008-08-17T02:56:56Z,13470,1,"@ Jeff Atwood , your interpretation is a little off - the {1} means match exactly once, but has no effect on the ""capturing"" - the capturing occurs because of the parens - the braces only specify the number of times the pattern must match the source - once, as you say. I agree with @ Marius , even if his answer is a little terse and may come off as being flippant. Regular expressions are tough, if one's not used to using them, and the {1} in the question isn't quite error - in systems that support it, it does mean ""exactly one match"". In this sense, it doesn't really do anything. Unfortunately, contrary to a now-deleted post, it doesn't keep the regexp from matching http:// http://example.org , since the \S+ at the end will match one or more non-whitespace characters, including the http://example.org in http:// http://example.org (verified using Python 2.5, just in case my regexp reading was off). So, the regexp given isn't really the best. I'm not a URL expert, but probably something limiting the appearance of "":""s and ""//""s after the first one would be necessary (but hardly sufficient) to ensure good URLs."
13486,1.0,2008-08-17T02:58:18Z,13470,2,"I don't think the {1} has any valid function in that regex. ( mailto\:|(news|(ht|f)tp(s?))\:// ){1} You should read this as: ""capture the stuff in the parens exactly one time"". But we don't really care about capturing this for use later, eg $1 in the replacement. So it's pointless."
13554,990.0,2008-08-17T07:22:43Z,13540,48,"Your assumption is right, this is the optimal way to do it and it's called upsert/merge . Importance of UPSERT - from sqlservercentral.com : For every update in the case mentioned above we are removing one
  additional read from the table if we
  use the UPSERT instead of EXISTS.
  Unfortunately for an Insert, both the
  UPSERT and IF EXISTS methods use the
  same number of reads on the table.
  Therefore the check for existence
  should only be done when there is a
  very valid reason to justify the
  additional I/O. The optimized way to
  do things is to make sure that you
  have little reads as possible on the
  DB. The best strategy is to attempt the
  update. If no rows are affected by the
  update then insert. In most
  circumstances, the row will already
  exist and only one I/O will be
  required. Edit : 
Please check out this answer and the linked blog post to learn about the problems with this pattern and how to make it work safe."
13555,5.0,2008-08-17T07:24:48Z,13540,5,"MERGE is one of the new features in SQL Server 2008, by the way."
13665,1200.0,2008-08-17T15:10:24Z,13620,38,"Well, as with all ""What might be faster in real life"" questions, you can't beat a real life test. function timeFunc($function, $runs)
{
  $times = array();

  for ($i = 0; $i < $runs; $i++)
  {
    $time = microtime();
    call_user_func($function);
    $times[$i] = microtime() - $time;
  }

  return array_sum($times) / $runs;
}

function Method1()
{ 
  $foo = 'some words';
  for ($i = 0; $i < 10000; $i++)
    $t = ""these are $foo"";
}

function Method2()
{
  $foo = 'some words';
  for ($i = 0; $i < 10000; $i++)
    $t = ""these are {$foo}"";
}

function Method3()
 {
  $foo = 'some words';
  for ($i = 0; $i < 10000; $i++)
    $t = ""these are "" . $foo;
}

print timeFunc('Method1', 10) . ""\n"";
print timeFunc('Method2', 10) . ""\n"";
print timeFunc('Method3', 10) . ""\n""; Give it a few runs to page everything in, then... 0.0035568 0.0035388 0.0025394 So, as expected, the interpolation are virtually identical (noise level differences, probably due to the extra characters the interpolation engine needs to handle). Straight up concatenation is about 66% of the speed, which is no great shock. The interpolation parser will look, find nothing to do, then finish with a simple internal string concat. Even if the concat were expensive, the interpolator will still have to do it, after all the work to parse out the variable and trim/copy up the original string. Updates By Somnath: I added Method4() to above real time logic. function Method4()
 {
  $foo = 'some words';
  for ($i = 0; $i < 10000; $i++)
    $t = 'these are ' . $foo;
}

print timeFunc('Method4', 10) . ""\n"";

Results were:

0.0014739
0.0015574
0.0011955
0.001169 When you are just declaring a string only and no need to parse that string too, then why to confuse PHP debugger to parse. I hope you got my point."
13668,849.0,2008-08-17T15:21:50Z,13550,1,"We use a CASE tool at my current company for code generation and we are trying to move away from it. The benefits that it brings - a graphical representation of the code making components 'easier' to pick up for new developers - are outweighed by the disadvantges in my opinion. Those main disadvantages are: We cannot do automatic merges, making it close to impossible for parallel development on one component. Developers get dependant on the tool and 'forget' how to handcode."
13673,1614.0,2008-08-17T15:32:34Z,13550,0,"Just a couple questions for you: How much productivity do you gain compared to the control that you use?
How testable and reliant is the code you create?
How well can you implement a new pattern into your design? I can't imagine that there is a CASE out there that I could write a test first and then use a CASE to generate the code I need. I'd rather stick to resharper which can easily do my mundane tasks and retain full control of my code."
13680,1532.0,2008-08-17T15:44:57Z,13620,16,"@Adam's test used ""these are "" . $foo note that the following is even faster: 'these are ' . $foo; this is due to the fact, that a double quoted ""string"" gets evaluated, where a single quoted 'string' is just taken as is..."
13716,1430.0,2008-08-17T16:52:00Z,12890,0,I've been toying with composite indexes and have seen some real benefits...maybe I'll setup some tests to see if that can save me here..at least for a little longer.
13759,1600.0,2008-08-17T17:38:12Z,13430,4,"What Dale Ragan said; it installed flawlessly on our Windows Server 2008 machine, including the Dashboard running on IIS 7. Just give it a shot; should work fine."
13905,1635.0,2008-08-17T20:23:44Z,2630,0,"ls (Get-ChildItem)
rm (Remove-Item)
ps (Get-Process) and the rest of my familiar commands that now ""just work"" :) but seriously... New-Object would have to get my vote.  With it, powershell can do ANYTHING :)"
13978,,2008-08-17T22:17:46Z,13550,0,"The project I'm on originally went w/ the Oracle Development Suite to put together a web application. Over time (5+ years), customer requirements became more complex than originally anticipated, and the screens were not easily maintainable. So, the team informally decided to start doing custom (hand coded) screens in web PL/SQL, instead of generating them using the Oracle Development Suite CASE tools (Oracle Designer). The Oracle Report Builder component of the Development Suite is still being used by the team, as it seems to ""get the job done"" in a timely fashion. In general, the developers using the Report Builder tool are not very comfortable coding. In this case, it seems that the productivity aspect of such CASE tools is heavily dependent on customer requirements and developer skill sets/training/background."
14045,380.0,2008-08-18T00:24:24Z,14040,5,"There should always be some developer testing. If a developer is producing too many bugs, then he/she is wasting time later on fixing those bugs. It is important that the developers don't develop the attitude which says, oh well if I leave a bug, it will be caught and I will get a chance to fix it. We try to keep a threshold for bugs produced. If this threshold is crossed during testing then the developer is answerable for it. It is up to you to decide what this threshold is (for us it can vary from project to project). Also, all unit testing is done by the developers."
14047,1646.0,2008-08-18T00:26:17Z,14040,4,"I have only been in the industry for a year, but in my experience dev's are responsible for unit testing their features, while QA is responsible for testing scenarios.  QA would also be expected to test any boundry conditions."
14053,1042.0,2008-08-18T00:32:30Z,14040,0,"Here are some ways that developer testing is the most efficient / highest payoff: Developer modifies a shared library while working on a feature - dev has insight into possible side effects that QA / validation don't Developer is unsure of performance of library call and writes a unit test Developer discovers path of use case not considered in spec that code must support, writes code, updates spec, writes test It's arguable how much test duty should be carried out by the dev in the third example, but I argue that it's most efficient for the dev because all of the related minutiae from many layers of documentation and code are already in her short-term memory. This perfect storm may not be attainable by a tester after the fact. Are we talking about QA or validation? I think of QA along the lines of inspection checklists, code standards enforcement, UI guidelines, etc. If we're talking validation, it doesn't make sense for devs to spend a lot of time authoring and executing formal test cases, but devs must provide all of the rationale and design documentation needed to author good tests."
14070,958.0,2008-08-18T01:08:58Z,14040,17,"It's the difference between ""black box"" testing (where you know what the code is supposed to do, but not how it works), and ""white box"" testing (where knowing how it works drives how you test it). ""Black box"" testing is what most people think of when you mention Quality Assurance. I work for a company where the QA team are also software developers.  (That narrows the field a lot if you care to guess the company.)  I know Joel's opinion, and my experience leads me to partially disagree: for the same reason that a ""white hat"" hacker is more effective finding security holes, certain kinds of errors are more effectively found by white box testers who know how to write code (and therefore what the common mistakes are - for example, resource management issues like memory leaks). Also, since QA-oriented developers are part of the process from the initial design phase, they can theoretically help to drive higher-quality code throughout the process.  Ideally, for each developer working on the project with a mental focus on functionality, you have an opposing developer with a mental focus on breaking the code (and thus making it better). Seen in that light, it's less a matter of using developers for testers than it is kind of disconnected pair-programming where one developer has an emphasis on controlling quality. On the other hand, a lot of testing (such as basic UI functionality) frankly doesn't need that kind of skill.  That's where Joel has a point. For many businesses, I could see a system where programming teams trade off code review and testing duties for each others' code.  Members of the Business Logic team, for example, could spend an occasional tour testing and reviewing code for the UI team, and vice-versa.  That way you're not ""wasting"" developer talent on full-time testing, but you are gaining the advantages of exposing the code to (hopefully) expert scrutiny and punishment.  Then, a more traditional QA team can take up the ""black box"" testing."
14218,1659.0,2008-08-18T05:35:21Z,4170,-1,Another way of getting a command object is to call connection.CreateCommand() . That way you shouldn't have to set the Connection property on the command object.
14259,1695.0,2008-08-18T06:46:35Z,14040,3,"I'm pasting my answer to a question on our internal forum. If you have an hour or so.. take a listen to Mary Poppendieck's Competing on the basis of Speed video. Recommended Note(By Testers - I refer to the QA Team) Developer / Unit tests _ = __ Usability testing & Exploratory testing '================================================================== Acceptance / Customer tests = __ Property testing Imagine that to be a square with four quadrants. :) The left half should be automated. Developer tests verify that the code works as the coder wanted it to.
  Tools: NUnit / xUnit / whatever home-made tool Customer tests verify that the code works as the customer wanted it to.
The tests should be very easy to write, shouldn't require the customer to learn .NET/Java. Else the customer wont write those tests (although he may require some help of a developer). Fit for example uses HTML tables that can be written in Word.
Tools: FIT
Regression tools also lie here. Record-replay. The right half better utilizes the time & effort of good testers. e.g. No automated test can tell you whether X dialog is usable. Humans are better at this than machines. Usability. Try to Break the System down,( catch unhandled failure scenarios, enter null values ). Basically catch things that the developer missed. Property testing again requires humans. Here you check customer mandated properties that are required by your system. e.g. Performance - does your search dialog meet the 2 sec response time ? Security- can someone hack into this System ? etc. Availability - is your system online 99.99% of the time ? Testers shouldn't be spending time executing test-plans on the left half. That is the developers responsibility to ensure that the code works as the customer and the developer intended it to. The testers can infact help the customer formulate the acceptance tests.."
14311,1630.0,2008-08-18T08:09:23Z,14310,4,"There's a simple solution. Assuming your (non-display) textures are 1024x1024 and you are restricted to a 256x256 window/display. unsigned int WIN_WIDTH = 256;
unsigned int WIN_HEIGHT = WIN_WIDTH;
unsigned int TEX_WIDTH = 1024;
unsigned int TEX_HEIGHT = TEX_WIDTH; Use the window size to create your OpenGL window: glutInitWindowSize(WIN_WIDTH, WIN_HEIGHT); But, use the texture size for everything else: glViewport(0, 0, TEX_WIDTH, TEX_HEIGHT);
gluOrtho2D(0.0, TEX_WIDTH, 0.0, TEX_HEIGHT);
glTexCoord2i(TEX_WIDTH, TEX_HEIGHT);"
14313,615.0,2008-08-18T08:10:30Z,14300,23,"Drop into a cmd instance (or indeed PowerShell itself) and type this: powershell -? You'll see that powershell.exe has a ""-noexit"" parameter which tells it not to exit after executing a ""startup command""."
14331,1630.0,2008-08-18T08:27:43Z,14330,14,"I found one possible solution in the Color FAQ . The luminance component Y (from the CIE XYZ system ) captures what is most perceived by humans as color in one channel. So, use those coefficients: mono = (0.2125 * color.r) + (0.7154 * color.g) + (0.0721 * color.b);"
14339,1659.0,2008-08-18T08:40:50Z,14330,4,This MSDN article uses (0.299 * color.R + 0.587 * color.G + 0.114 * color.B); This Wikipedia article uses (0.3* color.R + 0.59 * color.G + 0.11 * color.B);
14371,1630.0,2008-08-18T09:23:11Z,14370,12,"Creating and using pop-up menus with GLUT is very simple. Here is a code sample that creates a pop-up menu with 4 options: // Menu items
enum MENU_TYPE
{
        MENU_FRONT,
        MENU_SPOT,
        MENU_BACK,
        MENU_BACK_FRONT,
};

// Assign a default value
MENU_TYPE show = MENU_BACK_FRONT;

// Menu handling function declaration
void menu(int);

int main()
{
        // ...

        // Create a menu
        glutCreateMenu(menu);

        // Add menu items
        glutAddMenuEntry(""Show Front"", MENU_FRONT);
        glutAddMenuEntry(""Show Back"", MENU_BACK);
        glutAddMenuEntry(""Spotlight"", MENU_SPOT);
        glutAddMenuEntry(""Blend 'em all"", MENU_BACK_FRONT);

        // Associate a mouse button with menu
        glutAttachMenu(GLUT_RIGHT_BUTTON);

        // ...

        return;
}

// Menu handling function definition
void menu(int item)
{
        switch (item)
        {
        case MENU_FRONT:
        case MENU_SPOT:
        case MENU_DEPTH:
        case MENU_BACK:
        case MENU_BACK_FRONT:
                {
                        show = (MENU_TYPE) item;
                }
                break;
        default:
                {       /* Nothing */       }
                break;
        }

        glutPostRedisplay();

        return;
}"
14384,266.0,2008-08-18T09:36:26Z,14350,0,Mayhaps System.Security.allowDomain is what you need?
14404,1109.0,2008-08-18T09:59:50Z,14350,6,"This is all described in The Adobe Flex 3 Programming ActionScript 3 PDF on page 550 (Chapter 27: Flash Player Security / Cross-scripting): If two SWF files written with ActionScript 3.0 are served from different domainsfor example, http://siteA.com/swfA.swf and http://siteB.com/swfB.swf then, by default, Flash Player does not allow swfA.swf to script swfB.swf, nor swfB.swf to script swfA.swf. A SWF file gives permission to SWF files from other domains by calling Security.allowDomain(). By calling Security.allowDomain(""siteA.com""), swfB.swf gives SWF files from siteA.com permission to script it. It goes on in some more detail, with diagrams and all."
14409,914.0,2008-08-18T10:03:36Z,14350,2,"You'll need a crossdomain.xml policy file on the server that has the file you load, it should look a something like this: <?xml version=""1.0""?>
<!-- <http://www.foo.com/crossdomain.xml> -->
<cross-domain-policy>
  <allow-access-from domain=""www.friendOfFoo.com"" />
  <allow-access-from domain=""*.foo.com"" />
  <allow-access-from domain=""105.216.0.40"" />
</cross-domain-policy> Put it as crossdomain.xml in the root of the domain you're loading from. Also you need to set the loader to read this file as such: var loaderContext:LoaderContext = new LoaderContext();
loaderContext.checkPolicyFile = true;

var loader:Loader = new Loader();
loader.contentLoaderInfo.addEventListener( Event.COMPLETE, onComplete );
loader.load( new URLRequest( ""http://my.domain.com/image.png"" ), loaderContext ); code sample yoinked from http://blog.log2e.com/2008/08/15/when-a-cross-domain-policy-file-is-not-enough/"
14415,266.0,2008-08-18T10:12:49Z,14410,12,"Um... maybe there isn't much of a need for one, given that Vi/Vim is pretty much available everywhere and got the whole modal thing right? :)"
14416,1633.0,2008-08-18T10:13:17Z,14410,2,"I believe Eclipse has Vi bindings and there is a Visual Studio plugin/extension, too (which is called Vi-Emu, or something)."
14447,1073.0,2008-08-18T10:47:50Z,580,6,"Don't forget Microsoft's solution to the problem: Visual Studio 2008 Database Edition .  Includes tools for deploying changes to databases, producing a diff between databases for schema and/or data changes, unit tests, test data generation. It's pretty expensive but I used the trial edition for a while and thought it was brilliant.  It makes the database as easy to work with as any other piece of code."
14489,49.0,2008-08-18T12:03:03Z,14410,39,"Early software was often modal, but usability took a turn at some point, away from this style. VI-based editors are total enigmas -- they're the only real surviving members of that order of software. Modes are a no-no in usability and interaction design because we humans are fickle mammals who cannot be trusted to remember what mode the application is in. If you think you are in one ""mode"" when you are actually in another, then all sorts of badness can ensue. What you believe to be a series of harmless keystrokes can (in the wrong mode) cause unlimited catastrophe. This is known as a ""mode error"". To learn more, search for the term ""modeless"" (and ""usability"") As mentioned in the comments below, a Modal interface in the hands of an experienced and non-fickle person can be extremely efficient."
14524,446497.0,2008-08-18T12:30:07Z,14410,0,"Though not really answering your question, there used to be a ""modal like"" way to write Japanese on cell phones before :
The first letter you hit was a conson let's say K, and then, and then the next key you would hit would have the role of a conson. (Having two conson in a row is impossible in Japanese) Though it was main a few years ago, today it's only used by people who really want to hit fast."
14531,905.0,2008-08-18T12:40:52Z,14530,64,"Linq to Sql. Sql server will cache the query plans, so there's no performance gain for sprocs. Your linq statements, on the other hand, will be logically part of and tested with your application.  Sprocs are always a bit separated and are harder to maintain and test. If I was working on a new application from scratch right now I would just use Linq, no sprocs."
14533,372.0,2008-08-18T12:44:12Z,14530,3,"I'm assuming you mean Linq To Sql For any CRUD command it's easy to profile the performance of a stored procedure vs. any technology. In this case any difference between the two will be negligible. Try profiling for a 5 (simple types) field object over 100,000 select queries to find out if there's a real difference. On the other hand the real deal-breaker will be the question on whether you feel comfortable putting your business logic on your database or not, which is an argument against stored procedures."
14537,1659.0,2008-08-18T12:45:13Z,14530,12,"The best code is no code, and with stored procedures you have to write at least some code in the database and code in the application to call it , whereas with LINQ to SQL or LINQ to Entities, you don't have to write any additional code beyond any other LINQ query aside from instantiating a context object."
14549,1219.0,2008-08-18T12:59:51Z,14530,71,"I am generally a proponent of putting everything in stored procedures, for all of the reasons DBAs have been harping on for years.  In the case of Linq, it is true that there will be no performance difference with simple CRUD queries. But keep a few things in mind when making this decision: using any ORM couples you tightly to your data model.  A DBA has no freedom to make changes to the data model without forcing you to change your compiled code.  With stored procedures, you can hide these sorts of changes to an extent, since the parameter list and results set(s) returned from a procedure represent its contract, and the innards can be changed around, just so long as that contract is still met. And also, if Linq is used for more complex queries, tuning the database becomes a much more difficult task.  When a stored procedure is running slow, the DBA can totally focus on the code in isolation, and has lots of options, just so that contract is still satisfied when he/she is done. I have seen many, many cases where serious problems in an application were addressed by changes to the schema and code in stored procedures without any change to deployed, compiled code. Perhaps a hybird approach would be nice with Linq?  Linq can, of course, be used to call stored procedures."
14556,493.0,2008-08-18T13:08:31Z,14530,35,"For basic data retrieval I would be going for Linq without hesitation. Since moving to Linq I've found the following advantages: Debugging my DAL has never been easier. Compile time safety when your schema changes is priceless. Deployment is easier because everything is compiled into DLL's. No more managing deployment scripts. Because Linq can support querying anything that implements the IQueryable interface, you will be able to use the same syntax to query XML, Objects and any other datasource without having to learn a new syntax"
14595,51.0,2008-08-18T13:32:51Z,14570,2,"Should I just do it when I'm going to use it? I would recommend that you only retrieve the data when you are going to use it. If you are not going to need it, there is no reason to waste resources by retrieving it in Page_Load. If you are going to need it multiple times throughout the page load, consider saving the query results to a private variable or collection so that the same data can be reused multiple times throughout the page load. Am I opening a new connection when I create a new instance? Asp.net handles connection pooling , and opens and closes connections in an efficient way. You shouldn't have to worry about this. One other thing to consider from a performance perspective is to avoid using Datasets and TableAdapters. In many cases, they add extra overhead into data retrieval that does not exist when using Linq to Sql, Stored Procedures or DataReaders."
14759,1633.0,2008-08-18T15:03:36Z,12890,1,"For MySQL I like this talk: Real World Web: Performance & Scalability, MySQL Edition . This contains a lot of different pieces of advice for getting more speed out of MySQL."
14776,1975282.0,2008-08-18T15:11:51Z,14770,0,"I keep them the same. But then, I don't have multifile assemblies, which is when the AssemblyVersion number becomes important. I use Microsoft-style date encoding for my build numbers, rather than auto-incrementing (I don't find the number of times that something has been built to be all that important)."
14787,194.0,2008-08-18T15:20:59Z,14770,14,"In a scenario where I have multiple file assemblies (i.e. 1 exe and 5 dlls) I will use a different file version for each, but the same assembly version for all of them, allowing you to know which exe each of the dlls go with."
14809,1796.0,2008-08-18T15:42:29Z,4230,1,The key difference is in the ViewState management IIRC. The DataGrid requires ViewState turned on in order to have edit and sort capabilities.
14819,5.0,2008-08-18T15:52:36Z,4230,4,"If you're working in Visual Studio 2008 / .NET 3.5, you probably shouldn't use either. Use the ListView - it gives you the features of the GridView combined with the styling flexibility of a repeater."
14826,1752.0,2008-08-18T15:55:13Z,14770,3,"@Adam: Are you changing the file version with each build?  Are you using version control (SYN or VSS) and using that information to link source back to the binaries? Seems to make sense that the Assembly version stays the same. i.e. ""2.0.0.0"".  That corresponds to the deployment of the product. The file version changes to match the revision from the source control.  ""2.0.??.revision""  This would provide a link from a specific dll (or exe) to the source that built it."
14835,1365.0,2008-08-18T16:03:33Z,14770,68,"In solutions with multiple projects, one thing I've found very helpful is to have all the AssemblyInfo files point to a single project that governs the versioning. So my AssemblyInfos have a line: [assembly: AssemblyVersion(Foo.StaticVersion.Bar)] I have a project with a single file that declares the string: namespace Foo
{
    public static class StaticVersion
    {
         public const string Bar= ""3.0.216.0""; // 08/01/2008 17:28:35
    }
} My automated build process then just changes that string by pulling the most recent version from the database and incrementing the second last number. I only change the Major build number when the featureset changes dramatically. I don't change the file version at all."
14871,71.0,2008-08-18T16:33:01Z,4110,0,"I upmodded Mark's post about Toad Data Modeler and wanted to point out that they have a beta version that is fully functional and free. The only downsides are the occasional bug and built in expiration (typically around the time a new beta is available), but for this poor bloke it does wonders until I can get my boss to chip in for a license."
14956,1719.0,2008-08-18T17:25:54Z,14770,20,"The KB article mentions the most important distinction: File versions are only used for display purposes, whereas the assembly version plays an important part in the .NET loading behaviour. If you change the assembly version number, then the identity of your assembly as a whole has changed. Developers will need to rebuild to reference your new version (unless you put some auto-versioning ""policy"" in place) and at runtime only assemblies with matching version numbers will be loaded. This is important in my environment, where we need an incrementing, highly visible version number for audit purposes, but we don't want to force developers to rebuild or have many versions concurrently in production. In this case for backwardly-compatible minor changes we update the file version, but not the assembly version."
15044,1771.0,2008-08-18T18:29:40Z,15040,5,"Assuming you have VMware workstation, VMware player or anything that can run vmware appliance, you just need to: Download, unzip Ubuntu 8.04 Server and start the virtual machine. Update ubuntu and set the layout and the timezone: sudo apt-get update
sudo apt-get upgrade
sudo dpkg-reconfigure console-setup
sudo dpkg-reconfigure tzdata
sudo vim /etc/network/interfaces set a fixed IP (Optional). install apache+mysql+php: sudo tasksel install lamp-server"
15054,50.0,2008-08-18T18:36:19Z,15040,0,"I don't really understand your question because i really didn't see one. But i'll do my best to infer two: to change your keyboard layout, check this forum post on ubuntu forums and to change the timezone, check this forum post ."
15119,1347.0,2008-08-18T19:38:49Z,14330,5,"This depends on what your motivations are.  If you just want to turn an arbitrary image to grayscale and have it look pretty good, the conversions in other answers to this question will do. If you are converting color photographs to black and white, the process can be both very complicated and subjective, requiring specific tweaking for each image.  For an idea what might be involved, take a look at this tutorial from Adobe for Photoshop. Replicating this in code would be fairly involved, and would still require user intervention to get the resulting image aesthetically ""perfect"" (whatever that means!)."
15128,1818.0,2008-08-18T19:45:09Z,11430,3,"I swear this isn't being pedantic, but is an important distinction -- I don't know what specifically you need when you say "".NET 3.5 CLR"" -- probably the .NET 3.5 Framework?  Possibly C# 3.0 language features?  But the CLR that .NET 3.5 runs on is still CLR 2.0. (the link is to the same explanation re: .NET 3.0; I couldn't immediately find this info on 3.5.  Actually, the best explanation of CLR vs. Framework vs. language version numbers I've yet found is on page 12 of Teach Yourself WPF in 24 Hours *) So, my point is that you can even use the features of .NET 3.5 and C# 3.0 on SQL 2005 CLR stored procedures -- we do, at my company -- and there's not even really any trickery to it.  All you have to do is have the free 3.5 framework on your server.  Obviously the SQL 2005 answer isn't that relevant for your specific question, but hopefully this will be helpful to the person who eventually comes across this page via Google. *disclosure: I'm friends with the authors"
15209,295.0,2008-08-18T20:52:22Z,1390,2,"No. For some things you will need the .net Framework (like reporting services), and you can't install it (in a supported way) in a server core."
15217,611.0,2008-08-18T21:00:57Z,14410,4,"@Leon: Great answer. @dbr: Modal editing is something that takes a while to get used to. If you were to build a new editor that fits this paradigm, how would you improve on VI/VIM/Emacs? I think that is, in part, an answer to the question. Getting it ""right"" is hard enough, competing agains the likes of VI/VIM/Emacs would be extremely tough -- most people who use these editors are ""die hard"" fans, and you'd have to give them a compelling reason to move to another editor. Those people who don't use them already are most likely going to stay in a non-modal editor. IMHO of course ;)"
15248,414.0,2008-08-18T21:27:18Z,15240,0,"What platforms are they not available on? stdarg is part of the standard library: http://www.opengroup.org/onlinepubs/009695399/basedefs/stdarg.h.html Any platform not providing it is not a standard C implementation (or very, very old). For those, you will have to use varargs: http://opengroup.org/onlinepubs/007908775/xsh/varargs.h.html"
15262,1821.0,2008-08-18T21:35:49Z,15240,9,"Here's something that I do in C/C++. First off, you write a function that uses the varargs stuff (see the link in Stu's posting). Then do something like this: int debug_printf( const char *fmt, ... );
 #if defined( DEBUG )
  #define DEBUG_PRINTF(x) debug_printf x
 #else
   #define DEBUG_PRINTF(x)
 #endif

 DEBUG_PRINTF(( ""Format string that takes %s %s\n"", ""any number"", ""of args"" )); All you have to remember is to use double-parens when calling the debug function, and the whole line will get removed in non-DEBUG code."
15269,,2008-08-18T21:42:09Z,15240,20,"I still do it the old way, by defining a macro (XTRACE, below) which correlates to either a no-op or a function call with a variable argument list. Internally, call vsnprintf so you can keep the printf syntax: #include <stdio.h>

void XTrace0(LPCTSTR lpszText)
{
   ::OutputDebugString(lpszText);
}

void XTrace(LPCTSTR lpszFormat, ...)
{
    va_list args;
    va_start(args, lpszFormat);
    int nBuf;
    TCHAR szBuffer[512]; // get rid of this hard-coded buffer
    nBuf = _vsnprintf(szBuffer, 511, lpszFormat, args);
    ::OutputDebugString(szBuffer);
    va_end(args);
} Then a typical #ifdef switch: #ifdef _DEBUG
#define XTRACE XTrace
#else
#define XTRACE
#endif Well that can be cleaned up quite a bit but it's the basic idea."
15281,1841.0,2008-08-18T21:54:15Z,15240,1,"Ah, vsprintf() was the thing I was missing.  I can use this to pass the variable argument list directly to printf(): #include <stdarg.h>
#include <stdio.h>

void DBG_PrintImpl(char * format, ...)
{
    char buffer[256];
    va_list args;
    va_start(args, format);
    vsprintf(buffer, format, args);
    printf(""%s"", buffer);
    va_end(args);
} Then wrap the whole thing in a macro."
15282,234.0,2008-08-18T21:55:43Z,14760,1,"you can enter commands into the search box by prefixing them with a > symbol. Wow, I didn't know that. Where do I find the list of possible commands? I never actually use the search box, I've remapped ctrl + F to incremental search, which is usually ctrl + I I find this much cooler than the normal search - give it a go, you might end up not caring about the search box anymore."
15301,1739.0,2008-08-18T22:13:33Z,14760,0,"Wow, I didn't know that. Where do I
  find the list of possible commands? The commands are the same as those you can enter in the command window, so you can pretty much drive the entire IDE and debugger using it. There are a load of predefined aliases for common commands. Open up the command window and enter alias for a list, to get you started."
15335,1370.0,2008-08-18T22:42:12Z,14760,2,"This is a really cool feature. I've poked through the feature documentation , and the accompanying command list , and not a heck of a lot is showing up in terms of turning it off. If you want to search for >exit , you could always type >Edit.Find >exit in the search box; that seems to do the trick. A bit verbose, though, but it really is an edge case."
15402,1693.0,2008-08-19T00:02:42Z,15390,11,"The approach that I've found works for me is having seperate JS files for each class (just as you would in Java, C# and others). Alternatively you can group your JS into application functional areas if that's easier for you to navigate. If you put all your JS files into one directory, you can have your server-side environment (PHP for instance) loop through each file in that directory and output a <script src='/path/to/js/$file.js' type='text/javascript'> in some header file that is included by all your UI pages. You'll find this auto-loading especially handy if you're regularly creating and removing JS files. When deploying to production, you should have a script that combines them all into one JS file and ""minifies"" it to keep the size down."
15411,392.0,2008-08-19T00:09:03Z,15390,2,"For server efficiency's sake, it is best to combine all of your javascript into one minified file. Determine the order in which code is required and then place the minified code in the order it is required in a single file. The key is to reduce the number of requests required to load your page, which is why you should have all javascript in a single file for production. I'd recommend keeping files split up for development and then create a build script to combine/compile everything. Also, as a good rule of thumb, make sure you include your JavaScript toward the end of your page. If JavaScript is included in the header (or anywhere early in the page), it will stop all other requests from being made until it is loaded, even if pipelining is turned on. If it is at the end of the page, you won't have this problem."
15440,1870.0,2008-08-19T00:42:51Z,15390,3,"Just a sidenode - Steve already pointed out, you should really ""minify"" your JS files. In JS, whitespaces actually matter. If you have thousand lines of JS and you strip only the unrequired newlines you have already saved about 1K. I think you get the point. There are tools, for this job. And you should never modify the ""minified""/stripped/obfuscated JS by hand! Never!"
15474,17.0,2008-08-19T01:07:34Z,15470,17,"Just change ""Home"" to an empty string. routes.MapRoute(
    ""Home"",
    """",
    new { action = Index, controller = Home }
);"
15488,49.0,2008-08-19T01:24:09Z,12290,3,Maybe try this question at http://www.codeplex.com/n2/Thread/List.aspx They might be able to tell you about performance limitations or bottlenecks.
15683,115.0,2008-08-19T04:18:02Z,15040,5,"This is my install scrpt, I use it on debian servers, but it will work in Ubuntu (Ubuntu is built on Debian) apt-get -yq update
apt-get -yq upgrade
apt-get -yq install sudo
apt-get -yq install gcc
apt-get -yq install g++
apt-get -yq install make
apt-get -yq install apache2
apt-get -yq install php5
apt-get -yq install php5-curl
apt-get -yq install php5-mysql
apt-get -yq install php5-gd
apt-get -yq install mysql-common
apt-get -yq install mysql-client
apt-get -yq install mysql-server
apt-get -yq install phpmyadmin
apt-get -yq install samba
echo '[global]
   workgroup = workgroup
   server string = %h server
   dns proxy = no
   log file = /var/log/samba/log.%m
   max log size = 1000
   syslog = 0
   panic action = /usr/share/samba/panic-action %d
   encrypt passwords = true
   passdb backend = tdbsam
   obey pam restrictions = yes
   ;invalid users = root
   unix password sync = no
   passwd program = /usr/bin/passwd %u
   passwd chat = *Enter\snew\sUNIX\spassword:* %n\n *Retype\snew\sUNIX\spassword:* %n\n *password\supdated\ssuccessfully* .
   socket options = TCP_NODELAY
[homes]
   comment = Home Directories
   browseable = no
   writable = no
   create mask = 0700
   directory mask = 0700
   valid users = %S
[www]
   comment = WWW
   writable = yes
   locking = no
   path = /var/www
   public = yes' > /etc/samba/smb.conf
(echo SAMBAPASSWORD; echo SAMBAPASSWORD) | smbpasswd -sa root
echo 'NameVirtualHost *
<VirtualHost *>
        ServerAdmin webmaster@localhost
        DocumentRoot /var/www/
        <Directory />
                Options FollowSymLinks
                AllowOverride None
        </Directory>
        <Directory /var/www/>
                Options Indexes FollowSymLinks MultiViews
                AllowOverride None
                Order allow,deny
                allow from all
        </Directory>
        ErrorLog /var/log/apache2/error.log
        LogLevel warn
        CustomLog /var/log/apache2/access.log combined
        ServerSignature On
</VirtualHost>' > /etc/apache2/sites-enabled/000-default
/etc/init.d/apache2 stop
/etc/init.d/samba stop
/etc/init.d/apache2 start
/etc/init.d/samba start edit: add this to set your MySQL password /etc/init.d/mysql stop
echo ""UPDATE mysql.user SET Password=PASSWORD('MySQLPasswrod') WHERE User='root'; FLUSH PRIVILEGES;"" > /root/MySQLPassword
mysqld_safe --init-file=/root/MySQLPassword &
sleep 1
/etc/init.d/mysql stop
sleep 1
/etc/init.d/mysql start end edit This is a bit specailised but you get the idea, if you save this to a file ('install' for example) all you have to do is: chmod +x install
./install Some of my apt-get commands are not necessary, because apt will automatically get the dependencies but I prefer to be specific, for my installs."
15698,658.0,2008-08-19T04:31:27Z,15690,2,"Before you start coding, plan out your database schema - everything else will flow from that. Getting the database reasonably correct early on will save you time and headaches later."
15702,307.0,2008-08-19T04:32:30Z,15690,10,"Do you know much about OOP?  If so, look into Spring and Hibernate to keep your implementation clean and orthogonal .  If you get that, you should find TDD a good way to keep your design compact and lean, especially since you have ""automated testing"" up and running. UPDATE:
Looking at the first slew of answers, I couldn't disagree more.  Particularly in the Java space, you should find plenty of mentors/resources on working out your application with Objects, not a database-centric approach .  Database design is typically the first step for Microsoft folks (which I do daily, but am in a recovery program, er, Alt.Net).  If you keep the focus on what you need to deliver to a customer and let your ORM figure out how to persist your objects, your design should be better."
15704,493.0,2008-08-19T04:35:14Z,15700,1,If you're using a winforms app you could try using UserProperties to store this info. Another possible solution could be custom configuration sections .
15710,372.0,2008-08-19T04:39:11Z,15690,1,"I do it the other way around. I find that doing it database-schema-first gets the system stuck in a data-driven-design that is difficult to abstract from persistence. We try to do domain model designs first and then base the database schema on those. And then there's the infrastructure design: the team should settle on conventions on how to structure the program first and foremost. And then we work together to agree first on a design for the common functionality of the system (e.g., things everyone needs like persistence, logging, etc.). This becomes the framework of the system. We all work on that together first before we split the rest of the functionalities amongst ourselves."
15714,372.0,2008-08-19T04:41:27Z,15700,0,"If you don't want it saved, you do not need to execute the cfg.Save command. The Configuration object will store your changes until it isn't needed anymore."
15782,1886.0,2008-08-19T06:00:46Z,100420,2,The most important feature I can't live without is Visual Studio 2008. :P
15783,380.0,2008-08-19T06:03:57Z,15690,2,"The main thing is being able to abstract the complexity of the system so that you don't get bogged down by it as soon as you start off. First read the spec like a story (skimming through it). Don't stop at every requirement to analyze it right there and then. This will allow you to get an overall picture of the system without too many details. At this point you would start identifying the major functional components of the system. Start putting these down (use a mindmap tool if you like). Then take each component and start exploding it (and tying each detail with requirements in the spec document). Do this for all components, till you have covered all requirements. Now, you should start looking at relationships between the components, and whether there are repetitions of features or functions across the various components (which you can then pull out to create utility components, or such). Around now, you would have a good detailed map of your requirements in your mind. NOW, you should think of designing the database, ER diagrams, Class Design, DFDs, deployment, etc. The problem with doing the last step first is that you can get bogged down in the complexity of your system without really gaining an overall understanding in the first place."
15785,1886.0,2008-08-19T06:08:31Z,15690,3,"I also disagree about starting with the database.  The DB is simply an artifact of how your business objects are persisted.  I don't know of an equivalent in Java, but .Net has stellar tools such as SubSonic that allow your DB design to stay fluid as you iterate through your business objects design.  I'd say first and foremost (even before deciding on what technologies to introduce) focus on the process and identify your nouns and verbs ... then build out from those abstractions.  Hey, it really does work in the ""real world"", just like OOP 101 taught you!"
15786,91.0,2008-08-19T06:10:47Z,100420,2,The Debugger :-) Beats Notepad by miles.
15821,1075.0,2008-08-19T07:08:00Z,100420,14,"A lot of people don't know or use the debugger to it's fullest - I.E. just use it to stop code, but right click on the red circle and there are a lot more options such as break on condition, run code on break. Also you can change variable values at runtime using the debugger which is a great feature - saves rerunning code to fix a silly logic error etc."
15882,1812.0,2008-08-19T08:08:55Z,8050,3,"To practice without the need to write wrap code just to execute linq-queries you could use
linqpad.net"
15893,1061.0,2008-08-19T08:24:37Z,15880,0,"If you open the .MSG file in a text editor, i believe you will find that the information you are after is stored as plain text inside the file. (It is on all the messages i have checked at least) It would be pretty easy to write some code to parse the file looking for lines beginning with ""From:"" or ""To:"" etc. and then extracting the information you need. If you need the body of the email as well, that may be a bit more complicated."
15951,,2008-08-19T10:24:53Z,100420,21,"Sara Ford has this market cornered. http://blogs.msdn.com/saraford/default.aspx More Visual Studio tips and tricks than you can shake a stick at. Some others: The Visual Studio 2005 and 2008 3-month trial editions are fully-functional, and can be used indefinitely (forever) by setting the system clock back prior to opening VS. Then, when VS is opened, set the system clock forward again so your datetimes aren't screwed up. But that's really piracy and I can't recommend it, especially when anybody with a .edu address can get a fully-functional Pro version of VS2008 through Microsoft Dreamspark . You can use Visual Studio to open 3rd-party executables, and browse embedded resources (dialogs, string tables, images, etc) stored within. Debugging visualizers are not exactly a ""hidden"" feature but they are somewhat neglected, and super-useful, since in addition to using the provided visualizers you can roll your own for specific data sets. Debugger's ""Set Instruction Pointer"" or ""Set Next Statement"" command. Conditional breakpoints (as KiwiBastard noted). You can use Quickwatch etc. to evaluate not only the value of a variable, but runtime expressions around that variable."
15956,1000.0,2008-08-19T10:34:35Z,15690,4,"This sounds very much like my first job. Straight out of university, I was asked to design the database and business logic layer, while other people would take care of the UI. Meanwhile the boss was looking over my shoulder, unwilling to let go of what used to be his baby and was now mine, and poking his finger in it. Three years later, developers were fleeing the company and we were still X months away from actually selling anything. The big mistake was in being too ambitious. If this is your first job, you will make mistakes and you will need to change how things work long after you've written them. We had all sorts of features that made the system more complicated than it needed to be, both on the database level and in the API that it presented to other developers. In the end, the whole thing was just far too complicated to support all at once and just died. So my advice: If you're not sure about taking on such a big job single-handed, don't. Tell your employers, and get them to find or hire somebody for you to work with who can help you out. If people need to be added to the project, then it should be done near the start rather than after stuff starts going wrong. Think very carefully about what the product is for, and to boil it down to the simplest set of requirements you can think of. If the people giving you the spec aren't technical, try to see past what they've written to what will actually work and make money. Talk to customers and salespeople, and understand the market. There's no shame in admitting you're wrong. If it turns out that the entire system needs to be rewritten, because you made some mistake in your first version, then it's better to admit this as soon as possible so you can get to it. Correspondingly, don't try to make an architecture that can anticipate every possible contingency in your first version, because you don't know what every contingency is and will just get it wrong. Write once with an eye to throwing away and starting again - you may not have to, the first version may be fine, but admit it if you do."
15968,1169.0,2008-08-19T10:47:08Z,100420,4,"The memory windows, very useful if you're doing low level stuff. Control + K , Control + F - Format selection - great for quickly making code neat Regions, some love them, some hate them, most don't even know they exist Changing variables in debug windows during execution Tracepoints Conditional break points Hold down Alt and drag for 'rectangular' selection. Control+B for a breakpoint, to break at function Control+I for incremental search, F3 to iterate"
15969,1703.0,2008-08-19T10:48:18Z,15690,0,"Split the big system to smaller pieces.
And don't think that it's so complex, because it usually isn't. By thinking too complex it just ruins your thoughts and eventually the design. Some point you just realize that you could do the same thing easier, and then you redesign it. Atleast this has been my major mistake in designing. Keep it simple!"
16002,791.0,2008-08-19T11:21:50Z,15690,1,"It has been my experience that Java applications (.NET also) that consider the database last are highly likely to perform poorly when placed into a corporate environment.  You need to really think about your audience.  You didn't say if it was a web app or not.  Either way the infrastructure that you are implementing on is important when considering how you handle your data. No matter what methodology you consider, how you get and save your data and it's impact on performance should be right up there as one of your #1 priorities."
16050,,2008-08-19T12:17:14Z,15900,3,Here are some 3rd-party diagramming tools: http://www.nevron.com/Products.DiagramFor.NET.Overview.aspx http://www.nwoods.com/GO/dotnet.htm http://www.syncfusion.com/products/diagram/web/default.aspx
16103,1297.0,2008-08-19T12:53:52Z,16100,45,"You're looking for Enum.Parse . SomeEnum enum = (SomeEnum)Enum.Parse(typeof(SomeEnum), ""EnumValue"");"
16104,905.0,2008-08-19T12:54:28Z,16100,607,"It's rather ugly: StatusEnum MyStatus = (StatusEnum) Enum.Parse(typeof(StatusEnum), ""Active"", true); I tend to simplify this with: public static T ParseEnum<T>(string value)
{
    return (T) Enum.Parse(typeof(T), value, true);
} Then I can do: StatusEnum MyStatus = EnumUtil.ParseEnum<StatusEnum>(""Active""); One option suggested in the comments is to add an extension, which is simple enough: public static T ToEnum<T>(this string value)
{
    return (T) Enum.Parse(typeof(T), value, true);
}

StatusEnum MyStatus = ""Active"".ToEnum<StatusEnum>(); Finally, you may want to have a default enum to use if the string cannot be parsed: public static T ToEnum<T>(this string value, T defaultValue) 
{
    if (string.IsNullOrEmpty(value))
    {
        return defaultValue;
    }

    T result;
    return Enum.TryParse<T>(value, true, out result) ? result : defaultValue;
} Which makes this the call: StatusEnum MyStatus = ""Active"".ToEnum(StatusEnum.None); However, I would be careful adding an extension method like this to string as (without namespace control) it will appear on all instances of string whether they hold an enum or not (so 1234.ToString().ToEnum(StatusEnum.None) would be valid but nonsensical) . It's often be best to avoid cluttering Microsoft's core classes with extra methods that only apply in very specific contexts unless your entire development team has a very good understanding of what those extensions do."
16107,192.0,2008-08-19T12:55:31Z,16100,9,"Enum.Parse is your friend: StatusEnum MyStatus = (StatusEnum)Enum.Parse(typeof(StatusEnum), ""Active"");"
16112,225.0,2008-08-19T12:58:57Z,16100,6,"object Enum.Parse(System.Type enumType, string value, bool ignoreCase); So if you had an enum named mood it would look like this: enum Mood
   {
      Angry,
      Happy,
      Sad
   } 

   // ...
   Mood m = (Mood) Enum.Parse(typeof(Mood), ""Happy"", true);
   Console.WriteLine(""My mood is: {0}"", m.ToString());"
16119,832.0,2008-08-19T13:02:44Z,16110,1,"OK, are you sure that is wise? I am picturing two scenarios here: There is a default button on the form, which is ""clicked"" when enter is pushed"". There is no default button, and you want the user to have to press enter, regardless. Both of these raise the same questions: Is there any validation that is taking place on the text? Why not create a user control to encapsulate this logic? If you know the enter button is being pushed and consumed fine, how are you having problems with TextBoxName.Text = string.Empty ? Also, as a polite note, can you please try and break up your question a bit? One big block is a bit of a pain to read.."
16128,302.0,2008-08-19T13:12:17Z,13620,9,"Don't get too caught up on trying to optimize string operations in PHP. Concatenation vs. interpolation is meaningless (in real world performance) if your database queries are poorly written or you aren't using any kind of caching scheme. Write your string operations in such a way that debugging your code later will be easy, the performance differences are negligible. @uberfuzzy Assuming this is just a question about language minutia, I suppose it's fine. I'm just trying to add to the conversation that comparing performance between single-quote, double-quote and heredoc in real world applications in meaningless when compared to the real performance sinks, such as poor database queries."
16130,35.0,2008-08-19T13:12:51Z,16110,3,"Hook into the KeyPress event on the TextBox, and when it encounters the Enter key, run your hardware setting code, and then highlight the full text of the textbox again (see below) - Windows will take care of clearing the text with the next keystroke for you. TextBox1.Select(0, TextBox1.Text.Length);"
16131,1659.0,2008-08-19T13:13:38Z,16100,6,"// str.ToEnum<EnumType>()
T static ToEnum<T>(this string str) 
{ 
    return (T) Enum.Parse(typeof(T), str);
}"
16268,493.0,2008-08-19T14:28:23Z,14530,8,"A DBA has no freedom to make changes
  to the data model without forcing you
  to change your compiled code. With
  stored procedures, you can hide these
  sorts of changes to an extent, since
  the parameter list and results set(s)
  returned from a procedure represent
  its contract, and the innards can be
  changed around, just so long as that
  contract is still met. I really don't see this as being a benefit. Being able to change something in isolation might sound good in theory, but just because the changes fulfil a contract doesn't mean it's returning the correct results. To be able to determine what the correct results are you need context and you get that context from the calling code."
16337,17.0,2008-08-19T14:54:53Z,16330,-1,"There are a couple ways, create an action for each step of the wizard process, or create a parameter that is passed in to the action method.  Like step that will allow you to know what the state of the wizard is in."
16342,493.0,2008-08-19T14:57:01Z,16320,0,"It's all about the purpose of the DB Layer.
If you use an instance to access the DB layer, you are allowing multiple versions of that class to exist. This is desirable if you want to use the same DB layer to access multiple databases for example. So you might have something like this: DbController acrhive = new DbController(""dev"");
DbController prod = new DbController(""prod""); Which allows you to use multiple instances of the same class to access different databases. Conversely you might want to allow only one database to be used within your application at a time. If you want to do this then you could look at using a static class for this purpose."
16349,17.0,2008-08-19T15:00:43Z,16320,-2,"It depends which model you subscribe to.  ORM (Object Relational Model) or Interface Model.  ORM is very popular right now because of frameworks like nhibernate, LINQ to SQL, Entity Framework, and many others.  The ORM lets you customize some business constraints around your object model and pass it around with out actually knowing how it should be committed to the database.  Everything related to inserting, updating, and deleting happens in the object and doesn't really have to worry the developer too much. The Interface Model like the Enterprise Data Pattern made popular by Microsoft, requires you to know what state your object is in and how it should be handled.  It also requires you to create the necessary SQL to perform the actions. I would say go with ORM."
16357,1227.0,2008-08-19T15:06:37Z,16320,2,"I like a single object to be correlated to a single record in the database, i.e. an object must be instantiated.  This is your basic ActiveRecord pattern.  In my experience, the one-object-to-one-row approach creates a much more fluid and literate presentation in code.  Also, I like to treat objects as records and the class as the table. For example to change the name of a record I do: objPerson = new Person(id)

objPerson.name = ""George""

objPerson.save() while to get all people who live in Louisiana I might do aryPeople = Person::getPeopleFromState(""LA"") There are plenty of criticisms of Active Record.  You can especially run into problems where you are querying the database for each record or your classes are tightly coupled to your database, creating inflexibility in both.  In that case you can move up a level and go with something like DataMapper . Many of the modern frameworks and ORM's are aware of some of these drawbacks and provide solutions for them.  Do a little research and you will start to see that this is a problem that has a number of solutions and it all depend on your needs."
16361,392.0,2008-08-19T15:08:47Z,16320,0,"As lomaxx mentioned, it's all about the purpose of the DB model. I find it best to use static classes, as I usually only want one instance of my DAL classes being created. I'd rather use static methods than deal with the overhead of potentially creating multiple instances of my DAL classes where only 1 should exist that can be queried multiple times."
16372,1886.0,2008-08-19T15:13:48Z,15700,0,"Nope, you must save in order for the EntLib (and, I suspect, any other tool) to see the changes."
16376,22.0,2008-08-19T15:16:08Z,10870,0,"Yes, drag and drop is different in AIR. I HATE that! It takes a lot of playing around to figure out how to get things to work the same as custom dnd that was built in flex. As for the coordinates, maybe play around with localToContent, and localToGlobal methods. They may help in translating the coordinates to something useful. Good luck. I will let you know if I think of anything else."
16378,446497.0,2008-08-19T15:16:24Z,16340,1,"If you are looking for performance, I tested a few hash keys, and 
I recommend Bob Jenkin's hash function . It is both crazy fast
to compute and will give as few collisions as the cryptographic
hash you used until now. I don't know C# at all, and I don't know if it can link with C, but
here is its implementation in C ."
16381,905.0,2008-08-19T15:17:05Z,16340,50,"The hash code of an object shouldn't be unique. The checking rule is: Are the hash codes equal? Then call the full (slow) Equals method. Are the hash codes not equal? Then the two items are definitely not equal. All you want is a GetHashCode algorithm that splits up your collection into roughly even groups - it shouldn't form the key as the HashTable or Dictionary<> will need to use the hash to optimise retrieval. How long do you expect the data to be? How random? If lengths vary greatly (say for files) then just return the length.  If lengths are likely to be similar look at a subset of the bytes that varies. GetHashCode should be a lot quicker than Equals , but doesn't need to be unique. Two identical things must never have different hash codes. Two different objects should not have the same hash code, but some collisions are to be expected (after all, there are more permutations than possible 32 bit integers)."
16387,748.0,2008-08-19T15:19:32Z,16340,1,Is using the existing hashcode from the byte array field not good enough? Also note that in the Equals method you should check that the arrays are the same size before doing the compare.
16408,1954.0,2008-08-19T15:31:02Z,16340,0,"Generating a good hash is easier said than done.  Remember, you're basically representing n bytes of data with m bits of information.  The larger your data set and the smaller m is, the more likely you'll get a collision ... two pieces of data resolving to the same hash. The simplest hash I ever learned was simply XORing all the bytes together.  It's easy, faster than most complicated hash algorithms and a halfway decent general-purpose hash algorithm for small data sets.  It's the Bubble Sort of hash algorithms really.  Since the simple implementation would leave you with 8 bits, that's only 256 hashes ... not so hot.  You could XOR chunks instead of individal bytes, but then the algorithm gets much more complicated. So certainly, the cryptographic algorithms are maybe doing some stuff you don't need ... but they're also a huge step up in general-purpose hash quality.  The MD5 hash you're using has 128 bits, with billions and billions of possible hashes.  The only way you're likely to get something better is to take some representative samples of the data you expect to be going through your application and try various algorithms on it to see how many collisions you get. So until I see some reason to not use a canned hash algorithm (performance, perhaps?), I'm going to have to recommend you stick with what you've got."
16411,198.0,2008-08-19T15:32:05Z,16140,1,"I really like the Apache Felix tutorials . However, I think in general leveraging OSGi in your application isn't one of those ""let's use this framework, because it's hype"" decision. It's more of a design question, but then everything that OSGi gives you in terms of design, you can have with vanilla Java as well. As for the runtime, you cannot just add an existing application and make it OSGi enabled. It needs to be design to be dynamic. Spring DM makes it easy to hide that from you, but it's still there and you need to be aware of it."
16448,5.0,2008-08-19T15:53:28Z,16340,2,"Have you compared with the SHA1CryptoServiceProvider.ComputeHash method? It takes a byte array and returns a SHA1 hash, and I believe it's pretty well optimized. I used it in an Identicon Handler that performed pretty well under load."
16464,1951.0,2008-08-19T15:59:41Z,16460,1,"Unless the method provides a ""limit"" parameter (which it doesn't) your best option is to go with a simple loop that removes the items that match, breaking when your incremented ""match counter"" hits your limit. That's pretty much how the internal function works anyway, but in a more optimized way."
16466,31505.0,2008-08-19T16:00:46Z,16460,1,"In framework 3.5, RemoveAll method takes a predicate as a parameter. So you may use list.RemoveAll(item => ShouldIRemoveThis(item)); where ShouldIRemoveThis is a method that returns a boolean indicating whether the item must be removed from the list."
16468,5.0,2008-08-19T16:01:11Z,16460,1,"Can you use LINQ? If so, you can just use the .Take() method and specify how many records you want (maybe as total - N)."
16493,1200.0,2008-08-19T16:16:08Z,16460,0,"Anonymous delegates are useful here. A simple example to remove the first limit even numbers from a list. List<int> myList = new List<int>;
for (int i = 0; i < 20; i++) myList.add(i);

int total = 0;
int limit = 5;
myList.RemoveAll(delegate(int i) { if (i % 2 == 0 && total < limit) { total++; return true; } return false; });

myList.ForEach(i => Console.Write(i + "" "")); Gives 1 3 5 7 9 10 11 12 13 14 15 16 17 18 19, as we want. Easy enough to wrap that up in a function, suitable for use as a lambda expression, taking the real test as a parameter."
16496,1367.0,2008-08-19T16:18:23Z,16460,5,"@buyutec Instead of list.RemoveAll(item => ShouldIRemoveThis(item)); you can use: list.RemoveAll(ShouldIRemoveThis); The lambda has the same signature as the method, so they are equivalent so you can just pass the method directly."
16500,31505.0,2008-08-19T16:19:51Z,16460,8,"If you want to specify both a limit for number of items to remove and a condition to select the items to remove, you can use this approach: int limit = 30; // Suppose you want to remove 30 items at most
list.RemoveAll(item => ShouldIRemoveThis(item) && limit-- > 0);"
16548,142.0,2008-08-19T17:01:49Z,3150,1,"The unit tester for VS2008 is only for .NET code as far as I know. I used CppUnit on Vs2005 and found it to be pretty good. As far as I remember, the setup was relatively painless, just make sure that in your testing projects the linker (Linker->Input->Additional Dependencies) includes cppunitd.lib. Then, #include <cppunit/extensions/HelperMacros.h> in your header You can then follow the steps in http://cppunit.sourceforge.net/doc/1.11.6/cppunit_cookbook.html to get your test class working."
16558,1600.0,2008-08-19T17:06:00Z,16550,0,"Generally speaking, I get the impression that NAnt offers more flexibility compared to MSBuild, whereas (with my relatively simple needs) I've been fine with the latter so far."
16559,35.0,2008-08-19T17:06:22Z,16550,2,"We use MSBuild, because we started with VisualStudio2005 (now VisualStudio2008), and MSBuild was already ""built in"" to the SDK - there is less maintenance on the build server. It's a NAnt clone, really - both tools are infinitely flexible in that they let you create custom build tasks in code, and both have a decent set of community build tasks already created. MSBuild Community Tasks NAntContrib"
16564,1875.0,2008-08-19T17:11:37Z,16550,28,"We actually use a combination of NAnt and MSBuild with CruiseControl . NAnt is used for script flow control and calls MSBuild to compile projects. After the physical build is triggered, NAnt is used to publish the individual project build outputs to a shared location. I am not sure this is the best process. I think many of us are still looking for a great build tool. One promising thing I heard recently on .NET Rocks, episode 362 , is James Kovac's PSake , a build system he based entirely on PowerShell. It sounds really promising since what you can do with PowerShell is fairly limitless in theory."
16567,1921.0,2008-08-19T17:13:10Z,16550,1,"I've used both and prefer NAnt . It's really hard for me to say one is ""better"" than the other."
16581,1975282.0,2008-08-19T17:17:52Z,16550,0,"I have used both MSBuild and NAnt, and I much prefer MSBuild, mainly because it requires a lot less configuration by default. Although you can over-complicate things and load MSBuild down with a lot of configuration junk too, at its simplest, you can just point it at a solution/project file and have it go which, most of the time, for most cases, is enough."
16589,298.0,2008-08-19T17:20:53Z,16550,1,"It also depends on what you're building. The MSBuild SDC Task library has a couple of special tasks. For example, for AD , BizTalk , etc. There are over 300 tasks included in
  this library including tasks for:
  creating websites, creating
  application pools, creating
  ActiveDirectory users, running FxCop ,
  configuring virtual servers, creating
  zip files, configuring COM+ , creating
  folder shares, installing into the GAC , configuring SQL Server ,
  configuring BizTalk 2004 and BizTalk
  2006, etc."
16600,922.0,2008-08-19T17:27:58Z,16550,15,"I'd just like to throw FinalBuilder in to the mix. It's not free, but if you're fed up with editing XML files and want a somewhat nicer ( IMO ) environment to work in I would give it a go. I've worked with all of them and have always went back to FinalBuilder."
16636,1940.0,2008-08-19T18:02:42Z,16610,0,You should also check hardware connectivity to the database. Perhaps this thread will be helpful: http://channel9.msdn.com/forums/TechOff/234271-Conenction-forcibly-closed-SQL-2005/
16652,1795.0,2008-08-19T18:10:54Z,16320,0,"I would say that it depends on what you want the ""DB layer"" to do... If you have general routines for executing a stored procedure, or sql statement, that return a dataset, then using static methods would make more sense to me, since you don't need a permanent reference to an object that created the dataset for you. I'd use a static method as well if I created a DB Layer that returned a strongly-typed class or collection as its result. If on the other hand you want to create an instance of a class, using a given parameter like an ID (see @barret-conrad's answer), to connect to the DB and get the necessary record, then you'd probably not want to use a static method on the class.  But even then I'd say you'd probably have some sort of DB Helper class that DID have static methods that your other class was relying on."
16662,1432.0,2008-08-19T18:15:58Z,10980,6,"There are multiple issues here as ikvm is currently being transitioned away from the GNU classpath system to Sun's OpenJDK.  Both are licensed as GPL+Exceptions to state explicitly that applications which merely use the OpenJDK libraries will not be considered derived works. Generally speaking, applications which rely upon components with defined specs such as this do not fall under the GPL anyway.  For example, linking against public POSIX APIs does not trigger GPL reliance in a Linux application, despite the kernel being GPL.  A similar principal will usually (the details can be tricky) apply to replacing Sun's Java with a FOSS/GPL implementation."
16729,1818.0,2008-08-19T18:49:15Z,15310,3,"Does anyone know of anyway to
  reduce/optimize the PDF export phase
  and or the size of the PDF without
  lowering the total page count? I have a few ideas and questions: 1. Is this a graphics-heavy report?  If not, do you have tables that start out as text but are converted into a graphic by the SSRS PDF renderer (check if you can select the text in the PDF)?  41K per page might be more than it should be, or it might not, depending on how information-dense your report is.  But we've had cases where we had minor issues with a report's layout, like having a table bleed into the page's margins, that resulted in the SSRS PDF renderer ""throwing up its hands"" and rendering the table as an image instead of as text.  Obviously, the fewer graphics in your report, the smaller your file size will be. 2. Is there a way that you could easily break the report into pieces?  E.g., if it's a 10-location report, where Location 1 is followed by Location 2, etc., on your final report, could you run the Location 1 portion independent of the Location 2 portion, etc.?  If so, you could join the 10 sub-reports into one final PDF using PDFSharp after you've received them all.  This leads to some difficulties with page numbering, but nothing insurmountable. 3. Does anyone else have any other
  theories as to why this runs on the
  server but not through the API? My guess would be the sheer size of the report.  I don't remember everything about what's an IIS setting and what's SSRS-specific, but there might be some overall IIS settings (maybe in Metabase.xml) that you would have to be updated to even allow that much data to pass through. You could isolate the question of whether the time is the problem by taking one of your working reports and building in a long wait time in your stored procedures with WAITFOR (assuming SQL Server for your DBMS). Not solutions, per se, but ideas.  Hope it helps."
16737,255.0,2008-08-19T18:51:55Z,15190,1,"I am not sure I completely understand your question. If you are referring to the folder the IDE has as the current folder, then you can just change the shortcut that launches Delphi to set the current directory where ever you want it to be."
16761,1165587.0,2008-08-19T19:14:04Z,16660,5,"AS far as I know, there is no way to localize a report (meaning automating the translation of string litterals)... Like you said,you basically have to use the User!Language global variable to catch the user's settings and then use that to retrieve the appropriate strings from the DB... However, you can adapt the display of currency/numeric/date fields according to the user locale. Also possible is changing the interface of the Report Viewer to match your user's langage. Here are a few links giving tips on how to adapt the locale: http://www.ssw.com.au/Ssw/Standards/Rules/RulesToBetterSQLReportingServices.aspx#LanguageSetting Langage pack for Report Viewer: http://www.microsoft.com/downloads/details.aspx?displaylang=en&FamilyID=e3d3071b-d919-4ff9-9696-c11d312a36a0"
16810,784.0,2008-08-19T19:46:24Z,16770,17,"""Algebraic Data Types"" in Haskell support full parametric polymorphism , which is the more technically correct name for generics, as a simple example the list data type: data List a = Cons a (List a) | Nil Is equivalent (as much as is possible, and ignoring non-strict evaluation, etc) to class List<a> {
     class Cons : List<a> {
         a head;
         List<a> tail;
     }
     class Nil : List<a> {}
 } Of course Haskell's type system allows more ... interesting use of type parameters but this is just a simple example.  With regards to the ""Algebraic Type"" name, i've honestly never been entirely sure of the exact reason for them being named that, but have assumed that it's due the mathematical underpinnings of the type system.  I believe that the reason boils down to the theoretical definition of an ADT being the ""product of a set of constructors"", however it's been a couple of years since i escaped university so i can no longer remember the specifics. [Edit: Thanks to Chris Conway for pointing out my foolish error, ADT are of course sum types, the constructors providing the product/tuple of fields]"
16817,1996.0,2008-08-19T19:51:39Z,16140,36,"Well, since you can not have one part OSGi and one part non-OSGi you'll need to make your entire app OSGi. In its simplest form you make a single OSGi bundle out of your entire application. Clearly this is not a best practice but it can be useful to get a feel for deploying a bundle in an OSGi container (Equinox, Felix, Knoplerfish, etc). To take it to the next level you'll want to start splitting your app into components, components should typically have a set of responsibilities that can be isolated from the rest of your application through a set of interfaces and class dependencies. Identifying these purely by hand can range from rather straightforward for a well designed highly cohesive but loosely coupled application to a nightmare for interlocked source code that you are not familiar with. Some help can come from tools like JDepend which can show you the coupling of Java packages against other packages/classes in your system. A package with low efferent coupling should be easier to extract into an OSGi bundle than one with high efferent coupling. Even more architectural insight can be had with pro tools like Structure 101 . Purely on a technical level, working daily with an application that consists of 160 OSGi bundles and using Spring DM I can confirm that the transition from ""normal"" Spring to Spring DM is largely pain free. The extra namespace and the fact that you can (and should) isolate your OSGi specific Spring configuration in separate files makes it even easier to have both with and without OSGi deployment scenarios. OSGi is a deep and wide component model, documentation I recommend: OSGi R4 Specification : Get the PDFs of the Core and Compendium specification, they are canonical, authoritative and very readable. Have a shortcut to them handy at all times, you will consult them. Read up on OSGi best practices, there is a large set of things you can do but a somewhat smaller set of things you should do and there are some things you should never do (DynamicImport: * for example). Some links: OSGi best practices and using Apache Felix Peter Kriens and BJ Hargrave in a Sun presentation on OSGi best practices one key OSGi concept are Services, learn why and how they supplant the Listener pattern with the Whiteboard pattern The Spring DM Google Group is very responsive and friendly in my experience The Spring DM Google Group is no longer active and has moved to Eclipse.org as the Gemini Blueprint project which has a forum here ."
16825,1810.0,2008-08-19T19:58:06Z,16770,0,"For me, the concept of Haskell's algebraic data types always looked like polymorphism in OO-languages like C#. Look at the example from http://en.wikipedia.org/wiki/Algebraic_data_types : data Tree = Empty 
          | Leaf Int 
          | Node Tree Tree This could be implemented in C# as a TreeNode base class, with a derived Leaf class and a derived TreeNodeWithChildren class, and if you want even a derived EmptyNode class. (OK I know, nobody would ever do that, but at least you could do it.)"
16880,2019.0,2008-08-19T20:27:34Z,16860,1,"The xUnit family are the mainstay of unit testing. They are integrated into the likes of Netbeans, Eclipse and many other IDEs. They offer a simple, structured solution to unit testing. One thing I always try and do when writing a test is to minimise external code usage. By that I mean: I try to minimise the setup and teardown code for the test as much as possible and try to avoid using other modules/code blocks as much as possible. Well-written modular code shouldn't require too much external code in it's setup and teardown."
16881,893.0,2008-08-19T20:29:23Z,16860,3,"The so-called xUnit framework is widely used. It was originally developed for Smalltalk as SUnit, evolved into JUnit for Java, and now has many other implementations such as NUnit for .Net. It's almost a de facto standard - if you say you're using unit tests, a majority of other developers will assume you mean xUnit or similar."
16893,1611.0,2008-08-19T20:33:16Z,16860,3,"A great resource for 'best practices' is the Google Testing Blog , for example a recent post on Writing Testable Code is a fantastic resource. Specifically their 'Testing on the Toilet' series weekly posts are great for posting around your cube, or toilet, so you can always be thinking about testing."
16894,1116.0,2008-08-19T20:33:28Z,16860,19,Ok here's some best practices from some one who doesn't unit test as much as he should...cough. Make sure your tests test one thing and one thing only. Write unit tests as you go. Preferably before you write the code you are testing against. Do not unit test the GUI. Separate your concerns . Minimise the dependencies of your tests. Mock behviour with mocks .
16919,1470.0,2008-08-19T20:44:56Z,16860,0,"NUnit is a good tool for any of the .NET languages. Unit tests can be used in a number of ways: Test Logic Increase separation of code units.  If you can't fully test a function or section of code, then the parts that make it up are too interdependant. Drive development, some people write tests before they write the code to be tested.  This forces you to think about what you want the code to do , and then gives you a definite guideline on when you have acheived that."
16921,1854.0,2008-08-19T20:45:55Z,15880,6,Microsoft has documented this: .MSG File Format Specification
16933,1412.0,2008-08-19T20:53:36Z,16770,8,"Haskell's datatypes are called ""algebraic"" because of their connection to categorical initial algebras . But that way lies madness. @olliej: ADTs are actually ""sum"" types. Tuples are products."
16934,1904.0,2008-08-19T20:53:38Z,15390,5,There's an excellent article on Vitamin by Cal Henderson of Flickr fame on how they optimise delivery of their CSS and JavaScript: http://www.iamcal.com/serving-javascript-fast/
16943,380.0,2008-08-19T20:59:28Z,16940,1,You can right click on the tab strip and insert a new vertical (or horizontal) tab group. This allows you to view multiple tabs at the same time.
16954,380.0,2008-08-19T21:04:54Z,16940,0,"Hmm.. I don't think there is a way from within Visual Studio. For maximizing real estate and working on simultaneous files, I use that method plus viewing the files on Full Screen mode. Do you multiple monitors?"
16958,1048.0,2008-08-19T21:08:18Z,16940,1,"You could stretch visual studio across both monitors then put two code windows next to each other. Basically, you are manually maximizing VS across both screens."
16968,588.0,2008-08-19T21:13:25Z,15390,3,"In our big javascript applications, we write all our code in small separate files - one file per 'class' or functional group, using a kind-of-like-Java namespacing/directory structure. We then have: A compile-time step that takes all our code and minifies it (using a variant of JSMin) to reduce download size A compile-time step that takes the classes that are always or almost always needed and concatenates them into a large bundle to reduce round trips to the server A 'classloader' that loads the remaining classes at runtime on demand."
16972,1993.0,2008-08-19T21:15:18Z,11690,1,"Do note that the alt attribute isn't intended to be a tooltip. Alt is for describing the image where the image itself is not available. If you want to use tooltips, use the title attribute instead."
16993,,2008-08-19T21:28:00Z,16940,0,Tools>Options>General>Multiple Documents
16995,71.0,2008-08-19T21:28:14Z,16940,0,"If you don't need to compile one of the code screens, have you thought about just opening Notepad++ or PSPad in your other monitor and viewing the second batch of code that way? They have context sensitive coloring that would assist in reading. I do this all the time."
17004,1835.0,2008-08-19T21:31:46Z,15690,0,"I found very insightful ideas about starting a new large project, based on common good practices Test Driven Development and pragmatic approach in the book Growing Object-Oriented Software, Guided by Tests . It is still under development, but first 3 chapters may be what You are looking for and IMHO worth reading."
17014,1996.0,2008-08-19T21:37:21Z,4860,0,"If the option exists to not do an XML signature and instead just to treat the XML as a byte stream and to sign that, do it. It will be easier to implement, easier to understand, more stable (no canonicalization, transform, policy, ...) and faster. If you absolutely must have XML DSIG (sadly, some of us must), it is certainly possible these days but there are many, many caveats. You need good library support, with Java this is out of the box in JDK 1.6, I am not familiar with other platforms. You must test interoperability with the receiving end of your signed XML, especially if they are potentially on a different platform. Be sure to read Why XML Security Is Broken , it basically covers all the ground regarding the horror that is XML Canonicalization and gives some pointers to some alternatives."
17029,1799.0,2008-08-19T21:46:12Z,15240,2,Another fun way to stub out variadic functions is: #define function sizeof
17030,317.0,2008-08-19T21:46:36Z,17020,6,If you partition your drive using LVM you won't have to worry about any individual partition running out of space in the future. Just move space around as necessary.
17036,2018.0,2008-08-19T21:51:52Z,17020,2,"If you want a classic setup, I'd go for a 50GB ""/"" partition, for all your application goodness, and split the rest across users, or a full 950GB for a single user. Endless diskspace galore!"
17037,1809.0,2008-08-19T21:51:54Z,16140,2,"Is your existing application monolithic or tiered in seperate processes/layers? If tiered, you can convert the middle/app-tier to run in an OSGi container. In my team's experience, we've found trying to do web-stuff in OSGi painful.  Other pain points are Hibernate and Jakarta Commons Logging. I find the OSGi specs pretty readable and I recommend you print out the flowchart that shows the algorithm for class loading.  I'll guarantee you'll have moments of, ""why am I getting a NoClassDefFoundError?"": the flowchart will tell you why."
17048,1813.0,2008-08-19T21:58:12Z,17020,5,"My standard strategy for normal ""utility"" boxes is to give them a swap partition twice the size of their RAM, a 1GB /boot partition and leave the rest as one vast partition.  Whilst I see why some people want a separate /var, separate /home, etc., if I only have trusted users and I'm not running some production service, I don't think the reasons I've heard to date apply.  Instead, I do my best to avoid any resizing, or any partition becoming too small - which is best achieved with one huge partition. As for the size of swap and /boot - if your machine has 4GB memory, you may not want to have double that in swap.  It's nonetheless wise to at least have some.  Even if you nonetheless have double, you're using a total of 9GB, for 0.9% of your new drive.  /boot can be smaller than 1GB, this is just my standard ""will not become full, ever"" size."
17101,372.0,2008-08-19T22:28:56Z,16860,11,"You might want to look at TDD on Three Index Cards and Three Index Cards to Easily Remember the Essence of Test-Driven Development : Card #1. Uncle Bobs Three Laws Write no production code except to pass a failing test. Write only enough of a test to demonstrate a failure. Write only enough production code to pass the test. Card #2: FIRST Principles Fast: Mind-numbingly fast, as in hundreds or thousands per second. Isolated: The test isolates a fault clearly. Repeatable: I can run it repeatedly and it will pass or fail the same way each time. Self-verifying: The Test is unambiguously pass-fail. Timely: Produced in lockstep with tiny code changes. Card #3: Core of TDD Red: test fails Green: test passes Refactor: clean code and tests"
17148,456.0,2008-08-19T22:58:47Z,17140,9,"#include <stdlib.h>

int main()
{
    system(""echo HAI"");

    return 0;
}"
17150,1791.0,2008-08-19T22:59:56Z,17140,2,One solution is the system function defined in stdlib.h int system(const char *string); system api example
17177,736.0,2008-08-19T23:17:07Z,17170,2,"I don't think there are hard and fast rules for this type of thing, but I usually go by the guideline of using the lightest possible way until absolutely necessary. For example, let's say you have a Person class and a Group class. A Group instance has many people, so a List here would make sense. When I declare the list object in Group I will use an IList<Person> and instantiate it as a List . public class Group {
  private IList<Person> people;

  public Group() {
    this.people = new List<Person>();
  }
} And, if you don't even need everything in IList you can always use IEnumerable too. With modern compilers and processors, I don't think there is really any speed difference, so this is more just a matter of style."
17179,392.0,2008-08-19T23:17:46Z,17170,0,"In situations I usually come across, I rarely use IList directly. Usually I just use it as an argument to a method void ProcessArrayData(IList almostAnyTypeOfArray)
{
    // Do some stuff with the IList array
} This will allow me to do generic processing on almost any array in the .NET framework, unless it uses IEnumerable and not IList, which happens sometimes. It really comes down to the kind of functionality you need. I'd suggest using the List class in most cases. IList is best for when you need to make a custom array that could have some very specific rules that you'd like to encapsulate within a collection so you don't repeat yourself, but still want .NET to recognize it as a list."
17183,1659.0,2008-08-19T23:20:12Z,17170,4,"If you're working within a single method (or even in a single class or assembly in some cases) and no one outside is going to see what you're doing, use the fullness of a List. But if you're interacting with outside code, like when you're returning a list from a method, then you only want to declare the interface without necessarily tying yourself to a specific implementation, especially if you have no control over who compiles against your code afterward. If you started with a concrete type and you decided to change to another one, even if it uses the same interface, you're going to break someone else's code unless you started off with an interface or abstract base type."
17185,372.0,2008-08-19T23:20:22Z,17170,1,"You should use the interface only if you need it, e.g., if your list is casted to an IList implementation other than List. This is true when, for example, you use NHibernate, which casts ILists into an NHibernate bag object when retrieving data. If List is the only implementation that you will ever use for a certain collection, feel free to declare it as a concrete List implementation."
17216,1851.0,2008-08-19T23:41:13Z,17170,7,"It's always best to use the lowest base type possible. This gives the implementer of your interface, or consumer of your method, the opportunity to use whatever they like behind the scenes. For collections you should aim to use IEnumerable where possible. This gives the most flexibility but is not always suited."
17226,446497.0,2008-08-19T23:47:13Z,17140,3,"On UNIX, I think you basically need to fork it if you want the spawned process to run detached from your the spawing one : For instance if you don't want your spawned process to be terminate when you quit your spawning process. Here is a page that explains all the subtle differences between Fork, System, Exec. If you work on Win,Mac and linux, I can recommend you the Qt Framework and its QProcess object , but I don't know if that's an option for you. The great advantages is that you will be able to compile the same code on windows linux and mac : QString program = ""./yourspawnedprogram"";
 QProcess * spawnedProcess = new QProcess(parent);
 spawnedProcess->start(program);
 // or spawnedProcess->startDetached(program); And for extra, you can even kill the child process from the mother process,
and keep in communication with it through a stream."
17287,1057.0,2008-08-20T01:20:48Z,17250,0,"You can use the Chilkat library.  It's commercial, but has a free evaluation and seems pretty nice. Here's an example I got from here : import chilkat

# Demonstrates how to create a WinZip-compatible 128-bit AES strong encrypted zip
zip = chilkat.CkZip()
zip.UnlockComponent(""anything for 30-day trial"")

zip.NewZip(""strongEncrypted.zip"")

# Set the Encryption property = 4, which indicates WinZip compatible AES encryption.
zip.put_Encryption(4)
# The key length can be 128, 192, or 256.
zip.put_EncryptKeyLength(128)
zip.SetPassword(""secret"")

zip.AppendFiles(""exampleData/*"",True)
zip.WriteZip()"
17295,1954.0,2008-08-20T01:33:16Z,17170,119,"There are two rules I follow: Accept the most basic type that will work Return the richest type your user will need So when writing a function or method that takes a collection, write it not to take a List, but an IList<T>, an ICollection<T>, or IEnumerable<T>.  The generic interfaces will still work even for heterogenous lists because System.Object can be a T too.  Doing this will save you headache if you decide to use a Stack or some other data structure further down the road.  If all you need to do in the function is foreach through it, IEnumerable<T> is really all you should be asking for. On the other hand, when returning an object out of a function, you want to give the user the richest possible set of operations without them having to cast around.  So in that case, if it's a List<T> internally, return a copy as a List<T>."
17311,1199.0,2008-08-20T01:52:59Z,17140,11,"If you want to perform more complicated operations, like reading the output of the external program, you may be better served by the popen system call. For example, to programmatically access a directory listing (this is a somewhat silly example, but useful as an example), you could write something like this: #include <stdio.h>

int main()
{
  int entry = 1;
  char line[200];
  FILE* output = popen(""/usr/bin/ls -1 /usr/man"", ""r"");
  while ( fgets(line, 199, output) )
  {
    printf(""%5d: %s"", entry++, line);
  }
} to give output like this 1: cat1
2: cat1b
3: cat1c
4: cat1f
5: cat1m
6: cat1s
..."
17316,1954.0,2008-08-20T01:56:06Z,16550,6,"I use MSBuild completely for building.  Here's my generic MSBuild script that searches the tree for .csproj files and builds them: <Project xmlns=""http://schemas.microsoft.com/developer/msbuild/2003"" DefaultTargets=""Build"">
  <UsingTask AssemblyFile=""$(MSBuildProjectDirectory)\bin\xUnit\xunitext.runner.msbuild.dll"" TaskName=""XunitExt.Runner.MSBuild.xunit""/>
  <PropertyGroup>
	<Configuration Condition=""'$(Configuration)'==''"">Debug</Configuration>
    <DeployDir>$(MSBuildProjectDirectory)\Build\$(Configuration)</DeployDir>
	<ProjectMask>$(MSBuildProjectDirectory)\**\*.csproj</ProjectMask>
	<ProjectExcludeMask></ProjectExcludeMask>
    <TestAssembliesIncludeMask>$(DeployDir)\*.Test.dll</TestAssembliesIncludeMask>
  </PropertyGroup>

  <ItemGroup>
    <ProjectFiles Include=""$(ProjectMask)"" Exclude=""$(ProjectExcludeMask)""/>
  </ItemGroup>

  <Target Name=""Build"" DependsOnTargets=""__Compile;__Deploy;__Test""/>

  <Target Name=""Clean"">
    <MSBuild Projects=""@(ProjectFiles)"" Targets=""Clean""/>
    <RemoveDir Directories=""$(DeployDir)""/>
  </Target>

  <Target Name=""Rebuild"" DependsOnTargets=""Clean;Build""/>

  <!--
  ===== Targets that are meant for use only by MSBuild =====
  -->
  <Target Name=""__Compile"">
    <MSBuild Projects=""@(ProjectFiles)"" Targets=""Build"">
      <Output TaskParameter=""TargetOutputs"" ItemName=""AssembliesBuilt""/>
    </MSBuild>
    <CreateItem Include=""@(AssembliesBuilt -> '%(RootDir)%(Directory)*')"">
      <Output TaskParameter=""Include"" ItemName=""DeployFiles""/>
    </CreateItem>
  </Target>

  <Target Name=""__Deploy"">
    <MakeDir Directories=""$(DeployDir)""/>
    <Copy SourceFiles=""@(DeployFiles)"" DestinationFolder=""$(DeployDir)""/>
    <CreateItem Include=""$(TestAssembliesIncludeMask)"">
      <Output TaskParameter=""Include"" ItemName=""TestAssemblies""/>
    </CreateItem>
  </Target>

  <Target Name=""__Test"">
    <xunit Assembly=""@(TestAssemblies)""/>
  </Target>
</Project> (Sorry if it's a little dense.  Markdown seems to be stripping out the blank lines.) It's pretty simple though once you understand the concepts and all the dependencies are handled automatically.  I should note that we use Visual Studio project files, which have a lot of logic built into them, but this system allows people to build almost identically both within the Visual Studio IDE or at the command line and still gives you the flexibility of adding things to the canonical build like the xUnit testing you see in the script above. The one PropertyGroup is where all the configuration happens and things can be customized, like excluding certain projects from the build or adding new test assembly masks. The ItemGroup is where the logic happens that finds all the .csproj files in the tree. Then there are the targets, which most people familiar with make, nAnt or MSBuild should be able to follow.  If you call the Build target, it calls _Compile, _ Deploy and __Test.  The Clean target calls MSBuild on all the project files for them to clean up their directories and then the global deployment directory is deleted.  Rebuild calls Clean and then Build."
17336,1104.0,2008-08-20T02:12:21Z,17320,5,Not to directly reject your premise but I actually think being a generalist is a good position in programming. You will certainly develop expertise in specific areas but it is likely to be a product of either personal interest or work necessity. Over time the stuff you are able to transfer across languages and problem domains is at the heart of what makes good programmers.
17339,868.0,2008-08-20T02:14:12Z,17320,3,"I think the more important question is: What areas of specialization are you most interested in? Once you know, begin learning in that area!"
17345,1781.0,2008-08-20T02:20:32Z,17320,20,"Ben, Almost all seasoned programmers are still students in programming. You never stops learning anything when you are a developer. But if you are really starting off on your career then you should be least worried about the specialization thing. All APIs, frameworks and skills that you expect that gives you a long term existence in the field is not going to happen. Technology seems changing a lot and you should be versatile and flexible enough to learn anything. The knowledge you acquire on one platform/api/framework doesn't die off. You can apply the skills to the next greatest platform/api/framework. That being said you should just stop worrying about the future and concentrate on the basics. DataStructures, Algorithm Analysis and Design, Compiler Design, Operating system design are the bare minimum stuff you need.  And further you should be willing to go back and read tho books in those field any time in your career. Thats all is required. Good luck. Sorry if I sounded like a big ass advisor; but thats what I think. :-)"
17346,952.0,2008-08-20T02:22:26Z,17320,3,"I would think the greatest skill of all would be to adapt with the times, because if your employer can see this potential in you then they would be wise to hold on tightly. That said, I would advise you dive into the area YOU would enjoy. Learning is driven by enthusiasm . Since my current employ is with an internet provider, I've found networking knowledge particularly helpful. But someday I'd like to play with 3D graphics (not necessarily games)."
17348,718.0,2008-08-20T02:32:20Z,16340,-1,"RuntimeHelpers.GetHashCode might help: From Msdn: Serves as a hash function for a
  particular type, suitable for use in
  hashing algorithms and data structures
  such as a hash table."
17371,1630.0,2008-08-20T03:19:50Z,17370,29,"Yes, the OpenGL Extension Wrangler Library (GLEW) is a painless way to use OpenGL extensions on Windows. Here's how to get started on it: Identify the OpenGL extension and the extension APIs you wish to use. OpenGL extensions are listed in the OpenGL Extension Registry . Check if your graphic card supports the extensions you wish to use. Download and install the latest drivers and SDKs for your graphics card. Recent versions of NVIDIA OpenGL SDK ship with GLEW. If you're using this, then you don't need to do some of the following steps. Download GLEW and unzip it. Add the GLEW bin path to your Windows PATH environment variable. Alternatively, you can also place the glew32.dll in a directory where Windows picks up its DLLs. Add the GLEW include path to your compiler's include directory list. Add the GLEW lib path to your compiler's library directory list. Instruct your compiler to use glew32.lib during linking. If you're using Visual C++ compilers then one way to do this is by adding the following line to your code: #pragma comment(lib, ""glew32.lib"") Add a #include <GL/glew.h> line to your code. Ensure that this is placed above the includes of other GL header files. (You may actually not need the GL header files includes if you include glew.h .) Initialize GLEW using glewInit() after you've initialized GLUT or GL. If it fails, then something is wrong with your setup. if (GLEW_OK != glewInit())
{
    // GLEW failed!
    exit(1);
} Check if the extension(s) you wish to use are now available through GLEW. You do this by checking a boolean variable named GLEW _your_extension_name which is exposed by GLEW. Example: if (!GLEW_EXT_framebuffer_object)
{
    exit(1);
} That's it! You can now use the OpenGL extension calls in your code just as if they existed naturally for Windows."
17391,1432.0,2008-08-20T03:47:03Z,7540,1,"There's a thorough discussion of this that explains all of the fsync related problems that affected pre-3.0 versions of FF.  In general, I have not seen the behaviour since then either, and really it shouldn't be a problem at all if your system isn't also doing IO intensive tasks.  Firebug/Venkman make for nice debuggers, but they would be painful for figuring out these kinds of problems for someone else's code, IMO. I also wish that there was an easy way to look at CPU utilization in Firefox by tab, though, as I often find myself with FF eating 100% CPU, but no clue which part is causing the problem."
17429,342.0,2008-08-20T05:08:59Z,17370,5,"Personally I wouldn't use an exit command. I would throw an exception so you can clear any other initialisation up at the end of the function. ie: try
{
    // init opengl/directx
    // init directaudio
    // init directinput

    if (GLEW_OK != glewInit())
    {
    	throw std::exception(""glewInit failed"");
    }
}
catch ( const std::exception& ex )
{
    // message to screen using ex.what()
    // clear up
} And I agree with OJ - if you want to write tutorials for others, then this is really the wrong place for it. There are already a load of good places for opengl tutorials. Try this one for instance ."
17431,372.0,2008-08-20T05:21:20Z,17430,2,"You don't test the converter, you test the final code. If the code doesn't compile, clearly your converter is failing. If the code compiles and your functionality tests fail, then you can tweak the code so that it passes the test. If you are fairly successful you should see that you only need to fix the modules that actually fail. Goodluck!"
17507,100.0,2008-08-20T07:22:02Z,17500,0,"I can't find any documentation that says you shouldn't use those methods directly, but I haven't looked very long. Also you refer to the EditorVisibleAttribute, which doesn't exist. According to Reflector it's the EditorBrowsableAttribute . Reflector disassembly: [EditorBrowsable(EditorBrowsableState.Never)]
public bool CheckAccess()
{
//CODE
}"
17565,1908.0,2008-08-20T08:11:08Z,16970,2,"None of the APIs for the more 'Word' like online editors seem to have any 'edit' functionality, just download and upload. This is obviously due to the complexity of the documents being stored. With Google Docs API you can get a document, edit it, and then re-upload it (you may need to delete the previous version as well I think). Zoho seems to provide an 'update' method which combines the two operations. Have you looked at the Google Notebook API instead? This might provide more granular access to the data in the 'note' due to its structured approach to storing the data."
17593,580.0,2008-08-20T08:29:15Z,17320,1,"Go as deep as you can starting off in one environment, win32, .net, Java, Objective C... whatever. It is important to build the deep understanding of how X works... so that you can translate the same concepts into other languages or platforms/environments, if you so desire. ""Are there any areas of specialization that hinder your ability of developing other areas of specialization.""  Sort of, but nothing permanent i think. Since I am relatively green myself (less than 4 years) I come from a really OOP mindset.  I've rarely jumped out of .NET, so I had a hard time on one job when coming into contact with embedded code.  With embedded programmers fearing object creation and the performance loss of inheritance.  I had to learn the environment, seriously low memory and slow clock times, they were coming from.  Those are times to grow, I had a better time at it because i understood my area pretty well. I will say if you pick something to specialize in for marketability and money, you will probably burn out fast.  If you do start to specialize pick something you enjoy.  I love GUI programing and hate server side stuff, my buddy is the opposite, but we both love our jobs.  If he had to do my job, and I his, we would both go insane out of boredom."
17638,214.0,2008-08-20T09:12:08Z,17170,19,"I would agree with Lee's advice for taking parameters, but not returning. If you specify your methods to return an interface that means you are free to change the exact implementation later on without the consuming method ever knowing. I thought I'd never need to change from a List<T> but had to later change to use a custom list library for the extra functionality it provided. Because I'd only returned an IList<T> none of the people that used the library had to change their code. Of course that only need apply to methods that are externally visible (i.e. public methods). I personally use interfaces even in internal code, but as you are able to change all the code yourself if you make breaking changes it's not strictly necessary."
17683,611.0,2008-08-20T09:49:56Z,17670,2,Any semi-decent algorithm will end up with a strong chance of generating a NULL value somewhere in the resulting ciphertext. Why not do something like base-64 encode your resulting binary blob before persisting to the DB? ( sample implementation in C++ ).
17699,379.0,2008-08-20T10:06:03Z,17670,0,"That's an interesting route OJ.
We're looking at the feasability of a non-reversable method (still making sure we don't explicitly retrieve the data to decrypt) e.g. just store a Hash to compare on a submission"
17720,893.0,2008-08-20T10:41:15Z,17670,1,"Storing a hash is a good idea. However, please definitely read Jeff's You're Probably Storing Passwords Incorrectly ."
17723,1000.0,2008-08-20T10:44:32Z,17430,1,"Short of a formal mathematical proof (which I imagine would be difficult), the proof of the pudding is in the unit tests. You have to find a way to wrap the converted C# snippets, compile the and run them under a similar environment, then compare the output against the original. Unless you're rigorous in your testing, there's no way you can be confident of the result."
17750,379.0,2008-08-20T11:16:49Z,17670,0,"It seems that the developer handling this is going to wrap the existing encryption with yEnc to preserve the table integrity as the data needs to be retrievable, and this save all that messy mucking about with infinite-improbab.... uhhh changing column types on entrenched installations.
Cheers Guys"
17771,380.0,2008-08-20T11:33:56Z,17770,5,Two Things I do: I blog about it - this allows me to go back and search my own blog. We use the code snippet feature in Visual Studio. Cheers.
17773,960.0,2008-08-20T11:34:07Z,17770,2,"Why not set up a Wiki? If you are on windows, i know that ScrewTurn wiki is pretty simple to deploy on a desktop/laptop.  No database to fuss around with."
17776,372.0,2008-08-20T11:36:17Z,17770,2,"Blog about it. One of the nice side-effects of blogging is that if you use a sensible categorization or tagging system, it's quite easy to search for stuff within your blog. The fact that you wrote about it also makes it easier to remember problems you have encountered before (""hey, I blogged about that!""). That's a great benefit aside from, of course, being able to share this information publicly so that others might be able to find your solution to a particular problem using Google."
17780,2098.0,2008-08-20T11:36:45Z,17770,1,"I send them to my gmail account, that way I have them where ever I go, and they can be put into appropriate folders for later."
17789,194.0,2008-08-20T11:48:29Z,17770,0,"I second the blog about it technique...even Jeff said that's a major reason he blogs. Also, regarding the wiki idea, if you set one up at work, be sure to encourage your coworkers to do the same.  When someone finds something of interest they can just write a little ""article"" explaining what it is and how to do it... that way, not only are your own things easily available and quickly searchable, but you'll often find out things you never knew from other people in your group.  That way it benefits everyone not just you."
17792,1726.0,2008-08-20T11:51:02Z,17770,0,"I agree with emailing, the wiki and the blog. Emailing is the most useful. If you can't use GMail and you're on windows, install a desktop search utility (Windows search, Google Desktop, Copernic, etc) I also like to jot it into a textfile and save it in my documents folder. Whatever desktop search utility you use will be able to find it easily. e.g. //print spool stop.notes.txt
If the printer spooler stops, start it again by 
- Services > Provision Networks > Restart Service

tags: printer provision no printer spooler cannot print remote desktop"
17819,1042.0,2008-08-20T12:14:25Z,17770,0,Subscribe in Google Reader and then search later.
17823,1820.0,2008-08-20T12:20:35Z,17770,2,A number of people I know swear by Google Notebook
17850,1912.0,2008-08-20T12:50:05Z,17770,0,"At my last place of work they wouldn't let me set up a wiki or anything - so I just made various word documents full of tips and instructions and gave that to my successor when I left. Now though I'd use a private wiki, or maybe a blog."
17858,2078.0,2008-08-20T12:54:56Z,6890,3,"Another way of waiting for a thread to finish is using an AutoResetEvent . private readonly AutoResetEvent mWaitForThread = new AutoResetEvent(false);

private void Blah()
{
    ThreadStart tStart = new ThreadStart(MyMethod);
    Thread t = new Thread(tStart);
    t.Start();

    //... (any other things)
    mWaitForThread.WaitOne();
}

private void MyMethod()
{
     //... (execute any other action)
     mWaitForThread.Set();
}"
17872,1951.0,2008-08-20T12:59:51Z,17870,26,"Sure thing, the simplest way is this: select foo from bar where baz in (1,2,3)"
17873,91.0,2008-08-20T12:59:58Z,17870,7,"select * from TABLE where field IN (1,2,3) You can also conveniently combine this with a subquery that only returns one field: select * from TABLE where field IN (SELECT boom FROM anotherTable)"
17874,212.0,2008-08-20T13:00:01Z,17870,3,"select * from TABLE where field in (1, 2, 3)"
17875,267.0,2008-08-20T13:00:01Z,17870,3,"WHERE field IN (1, 2, 3)"
17887,2119.0,2008-08-20T13:05:32Z,17870,4,OR: SELECT foo FROM bar WHERE baz BETWEEN 1 AND 3
17894,1013.0,2008-08-20T13:07:52Z,17770,3,"I use: Google Notebook - I take notes for projects, books I'm reading, etc Delicious + Firefox plug in - Every time I see a good page I mark it. Windows Journal (in tablet pc) - When I need to draw something and then copy/cut/paste it. I have more distractions here, the web is always very close :) Small Moleskine paper notebook - Its always with me. Big paper notebook - When I need more space to write and less distractions. Obviously these are for all useful information, not just for snippets or tips and tricks."
17895,2098.0,2008-08-20T13:08:03Z,17870,1,"You can still use in for select *
from table
where field  = '1' or field = '2' or field = '3' its just select * from table where field in ('1','2','3')"
17932,1898.0,2008-08-20T13:24:41Z,15240,1,"In C++ you can use the streaming operator to simplify things: #if defined _DEBUG

class Trace
{
public:
   static Trace &GetTrace () { static Trace trace; return trace; }
   Trace &operator << (int value) { /* output int */ return *this; }
   Trace &operator << (short value) { /* output short */ return *this; }
   Trace &operator << (Trace &(*function)(Trace &trace)) { return function (*this); }
   static Trace &Endl (Trace &trace) { /* write newline and flush output */ return trace; }
   // and so on
};

#define TRACE(message) Trace::GetTrace () << message << Trace::Endl

#else
#define TRACE(message)
#endif and use it like: void Function (int param1, short param2)
{
   TRACE (""param1 = "" << param1 << "", param2 = "" << param2);
} You can then implement customised trace output for classes in much the same way you would do it for outputting to std::cout ."
17937,1130097.0,2008-08-20T13:25:53Z,13550,0,"Unfortunaly the Magic tool doesn't generates code and also it can't implement a design pattern. I don't have control over the code cause as i stated before it doesn't have code to modify. Te bottom line is that it can speed up productivity in some way but it has the impossibility to user CVS, patterns also and I can't control all the details. I agree with gary when he says ""it seems that the productivity aspect of such CASE tools is heavily dependent on customer requirements and developer skill sets/training/background"" but also I can't agree more with Klelky; Those main disadvantages are:
1. We cannot do automatic merges, making it close to impossible for parallel development on one component.
2.Developers get dependant on the tool and 'forget' how to handcode. Thanks"
17971,1517.0,2008-08-20T13:40:51Z,17320,1,"As a student I'd recommend forgetting about what you're programming and focusing on the software process itself. Understand how to analyse a problem and ask the right questions; learn every design pattern you can and actually apply them all to gain a real understanding and appreciation of object-oriented design; write tests and then code only as much as you need to in order to make the tests pass. I think the best way to really learn is to just code as much as you can - the language and the domain aren't important, browse sourceforge and freshmeat for any interesting-sounding projects and get involved. What's important is understanding the fundamentals of software engineering. And yes, this includes C. Or Assembler. This is the easiest way to get a good understanding of how your computer works and what your high-level code is actually doing. Finally, never stop learning - Service-oriented architecture, inversion of control, domain-specific languages, business process management are all showing huge benefits so they're important to be aware of - But by the time you finish studying and join the workforce who knows what the next big thing will be?"
17987,1694.0,2008-08-20T13:48:14Z,17980,7,"man 3 printf on a Linux system will give you all the information you need.  You can also find these manual pages online, for example at http://linux.die.net/man/3/printf"
17989,1912.0,2008-08-20T13:50:05Z,17980,14,"http://en.wikipedia.org/wiki/Printf#printf_format_placeholders is Wikipedia's reference for format placeholders in printf. http://www.cplusplus.com/reference/clibrary/cstdio/printf.html is also helpful Basically in a simple form it's %[width].[precision][type]. Width allows you to make sure that the variable which is being printed is at least a certain length (useful for tables etc). Precision allows you to specify the precision a number is printed to (eg. decimal places etc) and the informs C/C++ what the variable you've given it is (character, integer, double etc). Hope this helps UPDATE: To clarify using your examples: printf( ""%10.1f     %10.2\n"", radius, area ); %10.1f (referring to the first argument: radius) means make it 10 characters long (ie. pad with spaces), and print it as a float with one decimal place. %10.2 (referring to the second argument: area) means make it 10 character long (as above) and print with two decimal places."
17992,446497.0,2008-08-20T13:50:37Z,17980,0,10.1f means you want to display a f loat with 1 decimal and the displayed number should be 10 characters long.
17995,863.0,2008-08-20T13:51:18Z,17980,1,"In short, those values after the % tell printf how to interpret (or output) all of the variables coming later.  In your example, radius is interpreted as a float (this the 'f'), and the 10.1 gives information about how many decimal places to use when printing it out. See this link for more details about all of the modifiers you can use with printf."
17999,2132.0,2008-08-20T13:51:59Z,17980,0,"Man pages contain the information you want. To read what you have above: printf( ""%10.2f"", 1.5 ) This will print: 1.50 Whereas: printf(""%.2f"", 1.5 ) Prints: 1.50 Note the justification of both.
Similarly: printf(""%10.1f"", 1.5 ) Would print: 1.5 Any number after the . is the precision you want printed. Any number before the . is the distance from the left margin."
18004,1084.0,2008-08-20T13:53:08Z,17980,2,"10.1f means floating point with 10 characters  wide with 1 place after the decimal point.
If the number has less than 10 digits, it's padded with spaces.
10.2f is the same, but with 2 places after the decimal point. You have these basic types: %d   - integer
%x   - hex integer
%s   - string
%c   - char (only one)
%f   - floating point (float)
%d   - signed int (decimal)
%i   - signed int (integer) (same as decimal).
%u   - unsigned int
%ld  - long (signed) int
%lu  - long unsigned int
%lld - long long (signed) int
%llu - long long unsigned int Edit: there are several others listed in @Eli's response (man 3 printf)."
18027,227.0,2008-08-20T14:00:26Z,18010,21,"Older AnkhSVN (pre 2.0) was very crappy and I was only using it for shiny icons in the solution explorer. I relied on Tortoise for everything except reverts. The newer Ankh is a complete rewrite (it is now using the Source Control API of the IDE) and looks & works much better. Still, I haven't forced it to any heavy lifting. Icons is enough for me. The only gripe I have with 2.0 is the fact that it slaps its footprint to .sln files. I always revert them lest they cause problems for co-workers who do not have Ankh installed. Dunno if my fears are groundless or not. addendum: I have been using v2.1.7141 a bit more extensively for the last few weeks and here are the new things I have to add: No ugly crashes that plagued v1.x. Yay! For some reason, ""Show Changes"" (diff) windows are limited to only two. Meh. Diff windows do not allow editing/reverting yet. Boo! Updates, commits and browsing are MUCH faster than Tortoise. Yay! All in all, I would not use it standalone, but once you start using it, it becomes an almost indispensable companion to Tortoise."
18032,267.0,2008-08-20T14:01:24Z,18010,5,"I tried version 1, and it was unreliable to say the least. I can't say anything about 2.0. If you can afford it, the one I use, VisualSVN , is very good and uses TortoiseSVN for all its gui, except for the specialized things related to its VS integration."
18040,1911.0,2008-08-20T14:04:34Z,18010,12,"I always had stability issues with AnkhSVN. I couldn't switch everyone to Subversion where I work without an integrated solution. Thank goodness for VisualSVN + TortoiseSVN . VisualSVN isn't free, but it is cheap, and works a treat."
18058,1441.0,2008-08-20T14:10:33Z,18010,0,"I tried AnkhSVN (1.0.3, just 4 months ago), and it did not work the way I wanted it to (i.e. needed to select things in the browser window instead of based on active file). I ended up making some macros that utilize TortoiseSVN that work much more like what I expected. I've been very happy with using TortoiseSVN via explorer and my macros inside the IDE."
18061,1908.0,2008-08-20T14:11:34Z,17960,6,"I'm guessing that the settings would have to be in powershell.exe.config in the powershell directory, but that seems to be a bad way of doing things. You can use ConfigurationManager.OpenMappedExeConfiguration to open a configuration file based on the executing DLL name, rather than the application exe, but this would obviously require changes to the DLLs."
18094,194.0,2008-08-20T14:25:01Z,18080,0,"Google Analytics is free (up to 50,000 hits per month I think) and is easy to setup with just a little javascript snippet to insert into your header or footer and has great detailed reports, with some very nice graphs."
18098,2084.0,2008-08-20T14:26:37Z,18080,0,Google Analytics is quick to set up and provides more sexy graphs than you can shake a stick at. http://www.google.com/analytics/
18103,1709.0,2008-08-20T14:29:11Z,17840,2,I found an interesting article about implementing a parser combinator in C# . It also references some more general papers on the subject. The Wikipedia article on the subject also has a general explaination of the concept.
18107,2137.0,2008-08-20T14:31:47Z,18010,2,I started with AnkhSvn and then moved on to VisualSvn. I have my own gripes with VisualSvn but its far less trouble compared to Ankh. I'm yet to try the new version of Ankh which they say is a complete rewrite and had inputs from Microsoft dev team as well.
18127,2086.0,2008-08-20T14:45:47Z,18010,1,I've been using both the newest version of Ankh SVN and Tortoise on a project at home. I find them to both be very good with a caveat. I've found that both SVN tools have at times failed to keep up with my file/folder renaming and moving resulting in it thinking that a perfectly good file needs to be deleted on the next commit. This is probably down to me misusing SVN in some way but TFS at work does not have this problem.
18202,1190.0,2008-08-20T15:17:26Z,17980,2,"10.1f means floating point with 1 place after the decimal point and the 10 places before the decimal point. If the number has less than 10 digits, it's padded with spaces. 10.2f is the same, but with 2 places after the decimal point. On every system I've seen, from Unix to Rails Migrations, this is not the case.  @robintw expresses it best: Basically in a simple form it's %[width].[precision][type]. That is, not ""10 places before the decimal point,"" but ""10 places, both before and after, and including the decimal point."""
18238,1293.0,2008-08-20T15:34:34Z,18080,0,Not Invented here but it's on my todo list to setup. http://awstats.sourceforge.net/
18246,914.0,2008-08-20T15:39:34Z,18080,0,"@Ian Looks like they've raised the limit. Not very surprising, it is google after all ;) This free version is limited to 5 million pageviews a month - however, users with an active Google AdWords account are given unlimited pageview tracking. http://www.google.com/support/googleanalytics/bin/answer.py?hl=en&answer=55543"
18278,1799.0,2008-08-20T16:02:34Z,18250,1,"Delete the executable as part of a pre-link event. Edit:
Hah, I forgot about Explorer resetting the creation date if you name a file exactly the same as a file that was recently deleted. Why are you keying off the creation date anyway?"
18280,1254.0,2008-08-20T16:02:52Z,18250,0,"A complete rebuild will delete that file forcing the linker to create it, hence the reason it gets a new creation date.  You could try disabling incremental linking under project properties (Linker | General).  If that doesn't do it you could add a build event to delete the exe file and force it to create a new file each time.  Both of these things could increase your build time."
18344,1190.0,2008-08-20T16:41:15Z,18290,11,"You'll also want to make sure that your SSH system is well secured to prevent people from logging in as your Capistrano bot.  I'd suggest restricting access to password-protected key pairs. Encrypting the .yml file on the server is useless since you have to give the bot the key, which would be stored . . . on the same server.  Encrypting it on your machine is probably a good idea.  Capistrano can decrypt it before sending."
18379,,2008-08-20T17:12:57Z,18250,0,"Deleting the executable doesn't do the job. That's the problem. Also I could not identify any projectfile, whose datetime was the same as the later linked executable. That lets me conclude, that the 'creation date' is an information taken from within some project-file. The project has 400000 lines, so a full build is no option."
18469,71.0,2008-08-20T18:09:05Z,18460,5,"I've been using forms authentication and creating the necessary GenericIdentity and CustomPrincipal objects that allows me to leverage the User.IsInRole type functions you typically only get with Windows authentication. That way in my web.config file, I can do stuff like... <location path=""Login.aspx"">
   <system.web>
      <authorization>
         <allow users =""*"" />
      </authorization>
   </system.web>
</location>

<location path=""ManagementFolder"">
   <system.web>
      <authorization>
         <allow roles =""Administrator, Manager"" />
      </authorization>
   </system.web>
</location>"
18475,160.0,2008-08-20T18:12:40Z,18460,-1,"I created a base ""page"" class that handles that sort of thing. All my pages can then be decorated with the RequiresLogin attribute if a login is required to view them. If the attribute is not present, the page is accessible to all. Example: <RequiresLogin()> _ 
<RequiresPermission(""process"")> _
Partial Class DesignReviewEditProgressPage
    Inherits MyPage 'which inherits System.Web.UI.Page and deal with logins itself

    ...
End Class The MyPage class checks what attributes are being tagged to itself and if RequiresLogin is present, it forwards you to a login page. I believe this could be adapted to fit your own problem."
18487,1975282.0,2008-08-20T18:17:22Z,18450,4,"In many cases, you can take existing code and just run it on Mono, particularly if you're porting an ASP.NET application. In some cases, you may require whole new sections of code to make it work. If you use System.Windows.Forms, for example, the application won't work unmodified. Likewise if you use any Windows-specific code (registry access code, for example). But I think the worst offender is UI code. That's particularly bad on Macintosh systems."
18488,1965.0,2008-08-20T18:17:32Z,18450,37,"On the desktop side, Mono works great if you commit to using GTK#. The Windows.Forms implementation is still a little buggy (for example, TrayIcon's don't work) but it has come a long way. Besides, GTK# is a better toolkit than Windows Forms as it is. On the web side, Mono has implemented enough of ASP.NET to run most sites perfectly. The difficulty here is finding a host that has mod_mono installed on apache, or doing it yourself if you have shell access to your host. Either way, Mono is great, and stable. Key things to remember when creating a cross platform program: Use GTK# instead of Windows.Forms Ensure to properly case your filenames Use Path.Separator instead of hardcoding ""\"" , also use Environment.NewLine instead of ""\n"" . Do not use any P/Invoked calls to Win32 API. Do not use the Windows Registry."
18500,5.0,2008-08-20T18:21:49Z,18450,65,"It has pretty extensive coverage up to .NET 4.0 and even include some features from .NET 4.5 APIs, but there are a few areas that we have chosen not to implement due to the APIs being deprecated, new alternatives being created or the scope being too large.   The following APIs are not available in Mono: Windows Presentation Foundation Windows Workflow Foundation (neither of the two versions) Entity Framework The WSE1/WSE2 ""add-ons"" to the standard Web Services stack Additionally, our WCF implementation is limited to what Silverlight supported. The easiest way to check for your specific project is to run the Mono Migration Analyzer (MoMA) . The benefit is that it will notify the Mono team of issues which will prevent you from using Mono (if any), which lets them prioritize their work. I recently ran MoMA on SubSonic and found only one issue - a weird use of Nullable types. That's a big codebase, so the coverage there was pretty impressive. Mono is in active use in several commercial as well as open source products . It's in use in some large applications, such as Wikipedia and the Mozilla Developer Center , and has been used in embedded applications such as the Sansa MP3 players and powers thousands of published games. At the language level, the Mono compiler is fully compliant with the C# 5.0 language specification ."
18501,1219.0,2008-08-20T18:21:50Z,18450,1,"It really depends on the namespaces and classes that you are using from the .NET framework.  I had interest in converting one of my windows services to run on my email server, which is Suse, but we ran into several hard roadblocks with APIs that had not been completely implemented.  There is a chart somewhere on the Mono website that lists all of the classes and their level of completion.  If your application is covered, then go for it. Like any other application, do prototyping and testing before you make a full commitment, of course. Another problem we ran into is licensed software: if you are referencing someone else's DLL, you can't code your way around incompatibilities that are buried in that assembly."
18535,1432.0,2008-08-20T18:36:50Z,18450,2,"Do you know how good Mono 2.0 preview's support is for Windows Forms 2.0? From the little bit that I've played with it, it seemed relatively complete and almost usable.  It just didn't quite look right in some places and is still a little hit or miss overall.  It amazed me that it worked as well as it did with some of our forms, though honestly."
18541,922.0,2008-08-20T18:38:48Z,18450,1,I would imagine then if you have an application with some 3rd party components you may be stuffed.  I doubt a lot of vendors will develop with Mono in mind Example: http://community.devexpress.com/forums/p/55085/185853.aspx
18627,2179.0,2008-08-20T19:23:30Z,18450,4,"We've been using it for a project here at work that needed to run on Linux but reuse some .NET libraries that we built in Managed C++.  I've been very surprised at how well it has worked out.  Our main executable is being written in C# and we can just reference our Managed C++ binaries with no issue.  The only difference in the C# code between Windows and Linux is RS232 serial port code. The only big issue I can think of happened about a month ago.  The Linux build had a memory leak that wasn't seen on the Windows build.  After doing some manual debugging (the basic profilers for Mono on Linux didn't help much), we were able to narrow the issue down to a specific chunk of code.  We ended up patching a workaround, but I still need to find some time to go back and figure out what the root cause of the leak was."
18703,184.0,2008-08-20T20:08:37Z,18700,5,Popularity Community contribution Public scrutiny We will be forced to adhere to standards. (which will in turn make the product better) Goodwill
18709,1953.0,2008-08-20T20:13:13Z,18700,1,"I think the crux of the reason that open source is a good idea is because you pool together a LARGE resource of people usually working for free to create something useful and exciting. A site like Digg is churning out more and better stories than the staff @ Slashdot could because the community drives it. So too, could an open source project get more done than a dedicated team IF you have a project exciting enough to draw in participation. There's also many other benefits like improving your code and learning along the way."
18711,1013.0,2008-08-20T20:14:17Z,18700,1,"Publicity: You could exemplify with the Ruby on Rails framework. It was created to do the 37signals web apps. They open sourced it, then someone came along and build twitter. Imagine the publicity they had from that!"
18712,2183.0,2008-08-20T20:14:45Z,18700,5,"The OSI has a number of good resources with http://www.opensource.org/advocacy/case_for_business.php probably being the most relevant for you. There are loads of open source projects and when popular, the best advantage in my opinion is having bug fixes and enhancements contributed back into the project. You tend to only develop the necessary features for the use case at your job (exceptions do exist of course) and it's good to have other people work on other areas of the project. That said, people will usually only do that if they have a use for the project themselves and raising awareness can be just as hard as marketing a commercial project - you will probably find only a few people use it having stumbled upon the project through an obscure Google search! As such, whilst there are a lot of development oriented advantages, even if there aren't many (or any) actual users, it looks very good from a business/company perspective that your organisation is supporting the release of internal projects under open source licenses. This shows good things to prospective employees about the openess of the organisation. So whilst you only get the big open source advantages with scale, there are less obvious ones that start working immediately, namely building a good name for your company."
18721,1311.0,2008-08-20T20:19:25Z,18700,1,"The most important contribution of making a product open source is that it instantly becomes more accessible for people. It also helps people who are really interested in your work to see what you have done, make suggestions on improving it and even lend you a hand in doing so sometimes. In addition, you contribute something to the vast repository of open source software and help the community grow and learn in your own small way."
18924,1993.0,2008-08-20T21:46:13Z,18920,1,"It depends on what browser you use, but Internet Explorer, for example, would take you to the root directory of your harddrive (eg. C:/), while browsers such as Firefox does nothing."
19010,1336.0,2008-08-20T22:42:02Z,18700,5,"Benefits to your company are few. All of the reasons others have given assume a degree of popularity that is... unlikely. Most business folks are going to realize that without needing to think very hard about it so they aren't going to find advertising or leverage or public scrutiny or tool improvement enough of a reason to take the ""risk"" of releasing it as open source. That said, here's the best counter to the ""risk"" argument for a company to release an internal tool as Open Source: if it isn't part of your core competency and care is taken so that it doesn't suck company resources (or expose company secrets/infrastructure), there really is no risk. The company loses nothing and gains a potential for gaineven if that potential is small."
19037,598.0,2008-08-20T22:55:03Z,15470,8,"If you're running on IIS 7, you can simply delete the Default.aspx file that comes with ASP.NET MVC (assuming you're running on Preview 3 or higher). That file was needed due to an issue with Cassini that was fixed in .NET 3.5 SP1. For more details check out: http://haacked.com/archive/2008/04/10/upcoming-changes-in-routing.aspx and http://haacked.com/archive/2008/05/12/sp1-beta-and-its-effect-on-mvc.aspx"
19284,1965.0,2008-08-21T03:25:45Z,19280,2,No. Instead you could use a in browser modal popup.
19290,565.0,2008-08-21T03:31:16Z,19280,6,"Javascript offers 3 modal boxes.  Prompt, confirm and alert.  None of those satisfy your request. There are a plethora of js modal popup solutions.  Here's an example. ModalBox"
19293,1377.0,2008-08-21T03:32:42Z,19280,1,"No, but there are JavaScript libraries that can accomplish this for you. Just as an example, Ext JS can be used to create a message box dialog ."
19301,431.0,2008-08-21T03:43:02Z,18920,0,"Eric, the document root is the folder in which your file is, wherever it may be."
19307,1414.0,2008-08-21T03:48:47Z,19280,2,"Like everyone else above says, you're stuck with OK/Cancel using confirm() . I would like to recommend this jQuery plugin though: jqModal . I've used it on 3 recent projects and it has worked great for each one. Specifically check out this example: 6). FUN! Overrides -- a. view (alert), b. view (confirm) It is now time to
  show a real-world use for jqModal --
  overriding the standard alert() and
  confirm dialogs! Note; due to the
  single threaded nature of javascript,
  the confirm() function must be passed
  a callback -- it does NOT return
  true/false."
19338,,2008-08-21T04:28:01Z,18920,1,"On a Mac, the document root is what you see in the window that appears after you double click on the main hard drive icon on your desktop. The temp folder needs to be in there for a browser to find the CSS file as you have it written in your code. Actually, you could also write the code like this: <link href=""file:///temp/test.css"" rel=""stylesheet"" type=""text/css"" />"
19376,116.0,2008-08-21T05:35:00Z,18700,4,"I've released a couple of company-time developed packages as open source. The basic pitch: It is more profitable or advantageous to the company to release this: this package is not part of our core business.  We're not giving away the recipe to the secret sauce. we will get a bigger body of people exercising the code, finding bugs and thereby increasing the code quality. it's likely we can find some people who will contribute code for features that we might find useful. good recruiting tool, part 1: good programmers will be attracted to our enlightened developer-friendly organization. good recruiting tool, part 2: we can see some people in action who we might be interested in recruiting. Here's are two standalone packages that were released via this approach: http://code.google.com/p/orapig -- pretty recent http://sourceforge.net/projects/kap/ -- at my old company"
19389,46.0,2008-08-21T05:59:00Z,19030,0,"maybe you should take the approach of defaulting to: ""the filename is correct"" and work from there to disprove that statement: with the fact that you only allow filenames with: 'show name', 'season number x episode number' and 'episode name', you know for certain that these items should be separated by a ""-"" (dash) so you have to have 2 of those for a filename to be correct. if that checks out, you can use your code to check that the show name matches the show name as seen in the parent's parent folder (case insensitive i assume), the season number matches the parents folder numeric value (with or without an extra 0 prepended). if however you don't see the correct amount of dashes you instantly know that there is something wrong and stop before the rest of the tests etc. and separately you can check if the file folder.jpg exists and take the necessary actions. or do that first and filter that file from the rest of the files in that folder."
19502,841.0,2008-08-21T08:59:13Z,18080,1,"If you're after things like server data, would RRDTool be something you're after?
It's not really a webserver type stats program though, I have no idea how it would scale. Edit: I've also just found Splunk Swarm , if you're interested in something that looks ""cool""."
19571,188.0,2008-08-21T10:08:56Z,17880,1,"I'd suggest that modifying the underlying XML file is ""considered harmful"".  Especially if you haven't checked to see if the document is open! I've had a quick look at the Scripting Dictionary for Pages, and it seems pretty comprehensive; here is part of one entry: document n [inh. document > item ; see also Standard Suite] : A Pages document. elements contains captured pages, character
  styles, charts, graphics, images,
  lines, list styles, pages, paragraph
  styles, sections, shapes, tables, text
  boxes . properties body text (text) : The main text flow of the document. bottom margin (real) : The bottom margin of the publication. facing pages (boolean) : Whether or not the view is set to facing
  pages. footer margin (real) : The footer margin of the publication. header margin (real) : The header margin of the publication. id (integer, r/o) : The unique identifier of the document. ... So, I guess I'd want to know what it is that you want to do that you can't do with AppleScript?"
19805,31505.0,2008-08-21T13:05:57Z,19790,14,"You can use a DataList control instead. It has a RepeatColumns property that you can define the number of columns you want to display. In .NET Framework 3.5, there is an even better solution, the ListView control. You can find further information about how to use the ListView control here ."
19859,39.0,2008-08-21T13:27:06Z,15880,9,"Update: I have found a 3rd party COM library called Outlook Redemption which is working fine for me at the moment. If you use it via COM-Interop in .NET, don't forget to release every COM object after you are done with it, otherwise your application crashes randomly."
19978,2275.0,2008-08-21T14:00:33Z,19970,1,"I don't really know javascript, but couldn't you create a stack of windows?"
20000,1065.0,2008-08-21T14:06:11Z,19970,1,"A stack if you want to just close the window on top.
A queue if you also need to open windows at the end."
20049,202.0,2008-08-21T14:19:29Z,930,2,"If you are querying a SQL Server database (Version 7 and up) you should replace the OleDb classes with corresponding classes in the System.Data.SqlClient namespace ( SqlConnection , SqlCommand and SqlDataReader ) as those classes have been optimized to work with SQL Server. Another thing to note is that you should 'never' select all as this might lead to unexpected results later on if you add or remove columns to this table."
20058,1975282.0,2008-08-21T14:23:40Z,20040,2,"Start with the basics before you get to the higher level stuff like web services (though that is important too). The most important things you need to learn, as a project manager, are the things you're going to be questioning your underlings about later. For example, my PM (also a PHP guy) has absolutely no knowledge of garbage collection and its implications, which makes it incredibly difficult for me to explain to him why our .NET Windows service appears to be taking 80MB of RAM. Remember, you are not the one who needs to know everything. You should be issuing overarching directives, and let the people with the expertise sort out the details. That said, study up on the technicals a bit so that they can communicate effectively with you. Edit (8/24/08):You should know something about the underlying technicals; not necessarily all .NET stuff either (garbage collection, .config files, pipes and services if you're running services adjacent to your project's main focus, stuff like that). Higher-reaching concepts would probably include WPF (maybe Silverlight as well), LINQ (or your ORM of choice), as well as the Vista bridge and related bridging code if your project includes desktop apps at all. Those three things seem to be the focus for this round of .NET. Something else that's very important to have at least a passing knowledge of is the ways that .NET code can/must interoperate with native code: P/Invoke, Runtime Callable Wrapping and COM Callable Wrapping. There are still a lot of native things that don't have a .NET equivalent. As for resources, I'd highly recommend MSDN Magazine. They tend to preview upcoming technologies and tools well before average developers will ever see them."
20075,1942.0,2008-08-21T14:27:46Z,20040,1,"The biggest thing you'll probably want to learn is the differences between Windows and non-Windows programmers.  They approach fundamental things differently.  Knowing the difference will be key to successfully managing the project.  If you listen to the stack overflow podcast, and Jeff and Joel have multiple discussions on this topic.  Understanding the details of the underlying technology is mostly irrelevant and you'll never know it well enough to go toe to toe with someone who works in it day in and day out.  You can probably pick it up as you go."
20082,377.0,2008-08-21T14:30:19Z,20040,1,"The #1 thing you need to be aware of (and I'm guessing you probably already are) is that the guys doing the coding should know what they are doing.  Depending on the personailties of the members of your team, you should be able to find someone who is willing and able to explain any of the intricacies to you on an as-required basis. In my experience, the biggest hinderence to a project is the PM who understands the project, but not how to accomplish it (not in itself a problem), but who is also unwilling to listen to what his team tell him.  As with any project management, accept that you can't know everything, and be humble enough to ask for explanations where needed."
20116,1940.0,2008-08-21T14:37:57Z,20040,3,"The number one rule is do NOT just ask for status updates.  It is Especially annoying when phrases like ""where are we on this?"" are used.   If you aren't directly involved in the details then just make sure you have established communication times or plans so that you know whats going on rather than asking for updates."
20411,1026.0,2008-08-21T16:16:07Z,19970,1,"Stack/queue in JS is a simple array, which can be manipulated with .push(val) , .pop() , .shift(val) and .unshift() ."
20498,206.0,2008-08-21T16:51:54Z,20450,2,"You can strip out the tags with regular expressions.  Just make sure that your expressions will not filter tags that were actually text.  If the text had ""\b"" in the body of text, it would appear as \b in the RTF stream.   In other words, you would match on ""\b"" but not ""\b"". You could probably take a short cut and filter out the header RTF tags.  Look for the first occurrence of ""\viewkind4"" in the input.  Then read ahead to the first space character.  You would remove all of the characters from the start of the text up to and including that space character.  That would strip out the RTF header information (fonts, colors, etc)."
20532,77.0,2008-08-21T17:03:22Z,20510,0,"As far as I know, and I'm definately not a hacker, you are completely fine. Really, if someone wanted to, they could exploit your code anyway clientside, but i don't see how they could exploit your server side code using javascript (unless you use server side javascript)"
20578,1432.0,2008-08-21T17:19:03Z,20510,0,"I don't see where this lets them do anything that they couldn't do already by calling eval.  If there's a security hole being introduced here, I don't see it."
20595,2310.0,2008-08-21T17:24:41Z,20510,1,"This isn't inherently dangerous, but the moment you pass any user-provided data into the function, it's ripe for a code injection exploit. That's worrisome, and something I'd avoid. I think a better approach would be to only expose the functionality you need , and nothing more."
20628,1490.0,2008-08-21T17:39:40Z,20450,6,"I would use a hidden RichTextBox, set the Rtf member, then retrieve the Text member to sanitize the RTF in a well-supported way.  Then I would use manually inject the desired formatting afterwards."
20645,1109.0,2008-08-21T17:43:48Z,20510,7,"There's no need for the JavaScript function, the first argument to ExternalInterface can be any JavaScript code, it doesn't have to be a function name (the documentation says so, but it is wrong). Try this: ExternalInterface.call(""alert('hello')"");"
20708,770.0,2008-08-21T18:22:48Z,20450,4,"I'd do something like the following: Dim unformatedtext As String

someRTFtext = Replace(someRTFtext, ""\ul"", ""[ul]"")
someRTFtext = Replace(someRTFtext, ""\b"", ""[b]"")
someRTFtext = Replace(someRTFtext, ""\i"", ""[i]"")

Dim RTFConvert As RichTextBox = New RichTextBox
RTFConvert.Rtf = someRTFtext
unformatedtext = RTFConvert.Text

unformatedtext = Replace(unformatedtext, ""[ul]"", ""\ul"")
unformatedtext = Replace(unformatedtext, ""[b]"", ""\b"")
unformatedtext = Replace(unformatedtext, ""[i]"", ""\i"")

Clipboard.SetText(unformatedtext)

oWord.ActiveDocument.ActiveWindow.Selection.PasteAndFormat(0)"
20712,811.0,2008-08-21T18:25:08Z,15880,3,"It's a ""Structured Storage"" document. I've successfully used Andrew Peace's code to read these in the past, even under .NET (using C++/CLI) - it's clean and fairly easy to understand. Basically, you need to figure out which records you need, and query for those - it gets a little bit hairy, since different versions of Outlook and different types of messages will result in different records..."
20752,2132.0,2008-08-21T18:44:38Z,17140,9,"It really depends on what you're trying to do, exactly, as it's: OS dependent Not quite clear what you're trying to do. Nevertheless, I'll try to provide some information for you to decide. On UNIX, fork() creates a clone of your process from the place where you called fork. Meaning, if I have the following process: #include <unistd.h>
#include <stdio.h>

int main()
{
    printf( ""hi 2 u\n"" );
    int mypid = fork();

    if( 0 == mypid )
        printf( ""lol child\n"" );
    else
        printf( ""lol parent\n"" );

    return( 0 );
} The output will look as follows: hi 2 u lol child lol parent When you fork() the pid returned in the child is 0, and the pid returned in the parent is the child's pid. Notice that ""hi2u"" is only printed once... by the parent . execve() and its family of functions are almost always used with fork(). execve() and the like overwrite the current stackframe with the name of the application you pass to it. execve() is almost always used with fork() where you fork a child process and if you're the parent you do whatever you need to keep doing and if you're the child you exec a new process. execve() is also almost always used with waitpid() -- waitpid takes a pid of a child process and, quite literally, waits until the child terminates and returns the child's exit status to you. Using this information, you should be able to write a very basic shell; one that takes process names on the command line and runs processes you tell it to. Of course, shells do more than that, like piping input and output, but you should be able to accomplish the basics using fork() , execve() and waitpid() . NOTE: This is *nix specific! This will NOT work on Windows. Hope this helped."
20754,2272.0,2008-08-21T18:46:18Z,7440,9,"We use Watin at my place of employment, we are a .net shop so this solution made a lot of sense.  We actually started with Watir (the original ruby implementation) and switched after.  It's been a pretty good solution for us so far"
20879,315.0,2008-08-21T19:26:02Z,20840,6,"Generally when you need to do a sizeable (or frequent) queries to busy tables, where read committed would possibly be blocked by locks from uncommited transactions, but ONLY when you can live with inaccurate data. As an example, on a gaming web site I worked on recently there was a summary display of some stats about recent games, this was all based on dirty reads, it was more important for us to include then exclude the transactional data not yet committed (we knew anyway that few, if any, transactions would be backed out), we felt that on average the data would be more accurate that way."
20881,740.0,2008-08-21T19:26:44Z,20840,0,use it if you want the data back right away and it is not that important if it is right do not use if if the data is important to be correct or if you are doing updates with it Also take a look at snapshot isolation which has been introduced in sql server 2005
20890,51.0,2008-08-21T19:28:31Z,20840,14,"From MSDN : When this option is set, it is possible to read uncommitted or dirty data; values in the data can be changed and rows can appear or disappear in the data set before the end of the transaction. Simply put, when you are using this isolation level, and you are performing multiple queries on an active table as part of one transaction, there is no guarantee that the information returned to you within different parts of the transaction will remain the same. You could query the same data twice within one transaction and get different results (this might happen in the case where a different user was updating the same data in the midst of your transaction). This can obviously have severe ramifications for parts of your application that rely on data integrity."
20901,1471.0,2008-08-21T19:30:02Z,20880,6,"Can you give us some more information on how you add the combo boxes to the JPanel?  This is a pretty common thing to do in Swing so I doubt that it's a JVM issue but I guess anything is possible. Specifically, I would double check to make sure you're not accessing the GUI from any background threads.  In this case, maybe you're reading the choices from a DB or something and updating the JComboBox from a background thread, which is a big no-no in Swing.  See SwingUtils.invokeLater()."
20933,1862.0,2008-08-21T19:43:22Z,20910,13,"The problem with Silverlight, is that there's still a lot of people who don't have it installed.  Also, I""m not sure how well your existing .Net developers will be able to leverage their existing skills if they are only familiar with more traditional server-side .Net coding. What are your reasons for pushing Silverlight over Flex? If you have to ask the SOFlow community for reasons, it seems odd that you would be so willing to push it."
20962,856.0,2008-08-21T19:52:34Z,20910,2,"I think Silverlight and XAML is preferable to ActionScript, and though I'm not familiar with ActionScript IDE's, I am familiar with VS2008 and Expression Web/Blend, and they are very good development environments and getting better all the time.  I would go with Silverlight, and I think the key to getting users to install the plug-in is to have a good plug-in detect page that explains what SL is and why they need it.  For an example of this, go to http://memorabilia.hardrock.com/ and try it with your SL plug-in disabled."
20985,2351.0,2008-08-21T19:58:15Z,20910,3,"As Kibbee hinted at above, the argument of leveraging existing .Net developers doesn't hold much water.  It is impossible to be an expert in all facets of .Net development.  The platform is just too big.  The same goes for Java.  The only thing Silverlight has going for it from a skills perspective is that you can code in your favorite .Net language.  That advantage is fairly small if you are already doing any significant web development that utilizes JavaScript since Action script is a variation.  So really to convert a programmer to either Flex or Silverlight is all about learning the platform's API."
20995,580.0,2008-08-21T20:00:19Z,20910,22,"I think Silverlight is most advantageous for companies that have .NET developers but noone with designer experience. Skill sets will be easier to find as far as finding C# or VB developers vs finding ActionScript guru's.  However there is the trade off: Design experience is an investment not only in Designers with artistic skill, but also in the knowledge and tools provided by Adobe.  You can nearly guarantee that a professional designer uses a mac and has experience with Adobe tools. Right now the Silverlight designer tools are half baked and can be a headache.  For instance Blend errors when trying to render any xaml containing an IValueConverter, this is problematic.  I have no idea what the Adobe developer experience is, I'm sure it is as hairy. So at this stage of the game it comes down to human resources: If you have .NET experience and little invested in Design skills go Silverlight. Programming skills/tools will be transferable.
If you have Design experience and skill set go with Flex.  Designer skills/tools will be transferable. Either way both client platforms require communication with services to get data, so you will always leverage your existing programing expertise on the back end. Paraphrased Jon's opinion from a different point of view: I think you should look at Flex as a long-term play, just as Adobe seems to be doing. There's an obvious balance on when to use Silverlight vs. Flex when you're concerned about reach and install base, but here are more reasons Flex is a good direction to move in: Second mover advantage - Just as
Adobe built a ""better Java Applet""
with Flash, they're able to look at
how you'd design a runtime from
scratch, today. They have the
advantage of knowing how people use
the web today, something the
inventors of existing client
platforms could never have
accurately guessed. .NET can add
features, but they can't
realistically chuck the platform and
start over. Designer familiarity - While
Flex/AIR is a new programing model,
it's not entirely unfamiliar to
designers. They'll ""get"" the way
Flex works a lot more quickly than
they'll understand firing up a new
design environment with new feature
poor tools and new animation
paradigms. Being rid of the RGB color model in
Silverlight- .NET was originally
built for windows and it is at the
core of how it works. Flex ditched a
long time ago for an design-centric
model. All your tools run on your mac. Nuff
said. Cool features - Silverlight still
has some catching up to do with
Flash on some obvious features (like
webcam / mic integration, or 3d /
graphics acceleration)."
21019,714.0,2008-08-21T20:04:09Z,20420,5,"Xcode has refactoring for C and Objective-C built in.  Just select what you'd like to refactor, choose ""Refactor..."" from either the menu bar or the contextual menu, and you'll get a window including the available refactorings and a preview area. Xcode doesn't currently have a public plug-in API; if there are specific types of plug-ins you'd like Apple to enable, file enhancement requests in the Bug Reporter .  That way Apple can count and track such requests. However, there are third-party tools like Accessorizer and mogenerator (the latest release is mogenerator 1.10 ) that you can use to make various development tasks faster.  Accessorizer helps you create accessor methods for your classes, while mogenerator does more advanced code generation for Core Data managed object classes that are modeled using Xcode's modeling tools."
21035,5.0,2008-08-21T20:11:17Z,20910,52,"I think you should look at Silverlight as a long-term play, just as Microsoft seems to be doing. There's an obvious balance on when to use Silverlight vs. Flash when you're concerned about reach and install base, but here are some reasons Silverlight is a good direction to move in: Second mover advantage - Just as Microsoft built a ""better Java"" with .NET, they're able to look at how you'd design a RIA plugin from scratch, today. They have the advantage of knowing how people use the web today, something the inventors of Flash could never have accurately guessed. Flash can add features, but they can't realistically chuck the platform and start over. Developer familiarity - While Silverlight is a new model, it's not entirely unfamiliar to developers. They'll ""get"" the way Silverlight works a lot more quickly than they'll understand firing up a new development environment with a new scripting language and new event paradigms. Being rid of the timeline model in Flash - Flash was originally built for keyframe based animations, and while there are ways to abstract this away, it's at the core of how Flash works. Silverlight ditches that for an application-centric model. ScottGu - ScottGu is fired up about Silverlight. Nuff said. Cool new features - While Silverlight still has some catching up to do with Flash on some obvious features (like webcam / mic integration, or 3d / graphics acceleration), there are some slick new technologies built in to Silverlight - Deep Zoom is one example. I'm seeing more ""revolutionary"" technologies on the Silverlight side, while Flash seems to be in maintenance mode at this point."
21076,106.0,2008-08-21T20:27:05Z,21060,0,Not that I know of. This has been a plague of Windows versions for quite some time.
21101,234.0,2008-08-21T20:32:05Z,20850,1,"This is not directly an answer, but The thing is that to remove the disk you have to eject the volume and in this case do it for both volumes I have a similar situation. OSX remembers where you put your icons on the desktop - I've moved the icons for both of my removable drives to just above where the trash can lives. Eject procedure becomes Hit top-left of screen with mouse to show desktop Drag small box around both removable drives Drag 2cm onto trash so they both get ejected Remove firewire cable"
21106,2354.0,2008-08-21T20:33:18Z,20420,39,"You sound as if you're looking for three major things: code templates, refactoring tools, and auto-completion. The good news is that Xcode 3 and later come with superb auto-completion and template support.  By default, you have to explicitly request completion by hitting the escape key.  (This actually works in all NSTextView s; try it!)  If you want to have the completions appear automatically, you can go to Preferences -> Code Sense and set the pop-up to appear automatically after a few seconds.  You should find good completions for C and Objective-C code, and pretty good completions for C++. Xcode also has a solid template/skeleton system that you can use.  You can see what templates are available by default by going to Edit -> Insert Text Macro.  Of course, you don't want to insert text macros with the mouse; that defeats the point.  Instead, you have two options: Back in Preferences ,go to Key Bindings , and then, under Menu Key Bindings , assign a specific shortcut to macros you use often.  I personally don't bother doing this, but I know plenty of great Mac devs who do Use the CompletionPrefix .  By default, nearly all of the templates have a special prefix that, if you type and then hit the escape key, will result in the template being inserted.  You can use Control-/ to move between the completion fields. You can see a full list of Xcode's default macros and their associated CompletionPrefix es at Crooked Spin . You can also add your own macros, or modify the defaults.  To do so, edit the file /Developer/Library/Xcode/Specifications/{C,HTML}.xctxtmacro .  The syntax should be self-explanatory, if not terribly friendly. Unfortunately, if you're addicted to R#, you will be disappointed by your refactoring options.  Basic refactoring is provided within Xcode through the context menu or by hitting Shift-Apple-J.  From there, you can extract and rename methods, promote and demote them through the class hierarchy, and a few other common operations.  Unfortunately, neither Xcode nor any third-party utilities offer anything approaching Resharper, so on that front, you're currently out of luck.  Thankfully, Apple has already demonstrated versions of Xcode in the works that have vastly improved refactoring capabilities, so hopefully you won't have to wait too long before the situation starts to improve."
21155,1242.0,2008-08-21T20:52:56Z,21060,0,Actually Windows XP tries to avoid that. Of course some programs found a way to circumvented that. Microsoft Powertoy TweakUI has a way to turn the option on again in case it was turned off. You could also edit the registry yourself using the following information .
21287,1975282.0,2008-08-21T21:56:11Z,21280,2,"The point is that LINQ integrates your queries into your primary programming language, allowing your IDE to provide you with some facilities (Intellisense and debug support, for example) that you otherwise would not have, and to allow the compiler to type-check your SQL code (which is impossible with a normal string query)."
21292,1965.0,2008-08-21T21:59:00Z,21280,39,"LINQ is not about SQL. LINQ is about being apply functional programming paradigmns on objects. LINQ to SQL is an ORM built ontop of the LINQ foundation, but LINQ is much more. I don't use LINQ to SQL, yet I use LINQ all the time. Take the task of finding the intersection of two lists: Before LINQ, this tasks requires writing a nested foreach that iterates the small list once for every item in the big list O(N*M), and takes about 10 lines of code. foreach (int number in list1)
{
    foreach (int number2 in list2)
    {
        if (number2 == number)
        {
            returnList.add(number2);
        }
    }
} Using LINQ, it does the same thing in one line of code: var results = list1.Intersect(list2); You'll notice that doesn't look like LINQ, yet it is. You don't need to use the expression syntax if you don't want to."
21302,792.0,2008-08-21T22:02:01Z,19030,2,"I want to add a rule that checks for
  the presence of a folder.jpg file in
  each directory, but to add this would
  make the code substantially more messy
  in it's current state.. This doesn't look bad.  In fact your current code does it very nicely, and Sven mentioned a good way to do it as well: Get a list of all the files Check for ""required"" files You would just have have add to your dictionary a list of required files: checker = {
  ...
  'required': ['file', 'list', 'for_required']
} As far as there being a better/extensible way to do this?  I am not exactly sure.  I could only really think of a way to possibly drop the ""multiple"" regular expressions and build off of Sven's idea for using a delimiter.  So my strategy would be defining a dictionary as follows (and I'm sorry I don't know Python syntax and I'm a tad to lazy to look it up but it should make sense.  The /regex/ is shorthand for a regex): check_dict = {
  'delim'    : /\-/,
  'parts'    : [ 'Show Name', 'Episode Name', 'Episode Number' ],
  'patterns' : [/valid name/, /valid episode name/, /valid number/ ],
  'required' : ['list', 'of', 'files'],
  'ignored'  : ['.*', 'hidden.txt'],
  'start_dir': '/path/to/dir/to/test/'
} Split the filename based on the delimiter. Check each of the parts. Because its an ordered list you can determine what parts are missing and if a section doesn't match any pattern it is malformed.  Here the parts and patterns have a 1 to 1 ratio.  Two arrays instead of a dictionary enforces the order. Ignored and required files can be listed.  The . and .. files should probably be ignored automatically.  The user should be allowed to input ""globs"" which can be shell expanded.  I'm thinking here of svn:ignore properties, but globbing is natural for listing files. Here start_dir would be default to the current directory but if you wanted a single file to run automated testing of a bunch of directories this would be useful. The real loose end here is the path template and along the same lines what path is required for ""valid files"".  I really couldn't come up with a solid idea without writing one large regular expression and taking groups from it... to build a template.  It felt a lot like writing a TextMate language grammar.  But that starts to stray on the ease of use.  The real problem was that the path template was not composed of parts , which makes sense but adds complexity. Is this strategy in tune with what you were thinking of?"
21331,1538.0,2008-08-21T22:18:05Z,21280,5,"So the really, really big deal about LINQ has nothing to do with Linq to SQL. It's about the enhancements it brought to the C# language itself."
21418,332.0,2008-08-21T23:20:41Z,21280,5,"LINQ is not just an ORM system, as Jonathan pointed out it brings a lot of functional programming elements to C#. And it lets you do a lot of ""database-y"" things in regular C# code. It's difficult to explain just how incredibly powerful that can be. Consider how much having solid, well designed generic data structures (such as list, stack, dictionary/hash, etc.) included in common frameworks has improved the state of development in modern languages. Precisely because using these data structures is very common and reducing the intellectual overhead of using them is a huge benefit. LINQ doesn't do anything you can't do yourself, but it makes a lot of operations a lot more straightforward and a lot easier. Consider the time-honored example of removing duplicates from a non-ordered list. In a lower level language like C or C++ you'd probably have to sort the list and maintain two indices into the list as you removed dupes. In a language with hashes (Java, C#, Javascript, Perl, etc.) you could create a hash where the keys are the unique values, then extract the keys into a new list. With LINQ you could just do this: int[] data = { 0, 1, 3, 3, 7, 8, 0, 9, 2, 1 };

var uniqueData = data.GroupBy(i => i).Select(g => g.Key);"
21466,394.0,2008-08-21T23:54:07Z,21060,0,"It suppose to be a registry change that helps with this type of situations (mentioned in this Coding Horror post about the subject of ""focus stealing""). I try it, it doesn't work with all popups but helps with some of them, causing the offending application to flash in the taskbar instead of gain focus."
21515,2384.0,2008-08-22T00:29:17Z,21460,0,"Well, if it support Python, you might be able to run Django using a FastCGI setup, as described here: FastCGI Docs . If it supports mod_python (which I guess it's what you mean), then sure, you can install it using the steps listed here: ModPython docs"
21653,1117.0,2008-08-22T02:48:54Z,21640,18,"Request.Url will return you the Uri of the request.  Once you have that, you can retrieve pretty much anything you want.  To get the protocol, call the Scheme property. Sample: Uri url = Request.Url;
string protocol = url.Scheme; Hope this helps."
21882,122.0,2008-08-22T06:21:15Z,21870,2,"Take a look at memcached .  It is a really cool, fast and lightweight distributed caching system.  There are APIs for several of the most popular languages, including C#.  It may not serve well on the client side (unless of course the client is obtaining the cached data from a server of some kind), but if you abstract your usage of memcached to a specific interface, you could then implement the interface with another caching system."
21926,1109.0,2008-08-22T07:23:59Z,20910,30,"There's two questions here: Silverlight vs. Flash as platform and Silverlight vs. Flex as RIA framework. The first question depends on your timeframe. Flash Player has over 95% reach, Silverlight has no way near that. However, Silverlight may get there, it is after all backed by Microsoft. If you aim to launch a site next week and want a huge audience, Silverlight is not an option. If you aim to launch a really cool application that everyone would want to use it's a bit different, if your app is good enough your target audience may install Silverlight just to be able to run it. As for the second question its a matter of how easy it is to develop applications in Silverlight. Flex isn't just a set of widgets, it's a very big framework that does a lot of thing that ease the work of the developer. You could write the same applications using only the core Flash API, but it would be very much more work. Depending on what's available in Silverlight, this should be an important factor when deciding. If you can cut development time, is having two platforms worth it?"
21967,1630.0,2008-08-22T09:13:56Z,21560,3,"I finally got Multisampling working with my wxWidgets OpenGL program. It's a bit messy right now, but here's how: wxWidgets doesn't have Multisampling support in their stable releases right now (latest version at this time is 2.8.8 ). But, it's available as a patch and also through their daily snapshot. (The latter is heartening, since it means that the patch has been accepted and should appear in later stable releases if there are no issues.) So, there are 2 options: Download and build from their daily snapshot . Get the patch for your working wxWidgets installation. I found the 2nd option to be less cumbersome, since I don't want to disturb my working installation as much as possible. If you don't know how to patch on Windows, see this . At the very least, for Windows, the patch will modify the following files: $(WX_WIDGETS_ROOT)/include/wx/glcanvas.h
$(WX_WIDGETS_ROOT)/include/wx/msw/glcanvas.h
$(WX_WIDGETS_ROOT)/src/msw/glcanvas.cpp After patching, recompile the wxWidgets libraries. To enable multisampling in your wxWidgets OpenGL program, minor changes to the code are required. An attribute list needs to be passed to the wxGLCanvas constructor: int attribList[] = {WX_GL_RGBA,
                    WX_GL_DOUBLEBUFFER,
                    WX_GL_SAMPLE_BUFFERS, GL_TRUE, // Multi-sampling
                    WX_GL_DEPTH_SIZE, 16,
                    0, 0}; If you were already using an attribute list, then add the line with GL_SAMPLE_BUFFERS, GL_TRUE to it. Else, add this attribute list definition to your code. Then modify your wxGLCanvas constructor to take this attribute list as a parameter: myGLFrame::myGLFrame    // Derived from wxGLCanvas
(
    wxWindow *parent,
    wxWindowID id,
    const wxPoint& pos,
    const wxSize& size,
    long style,
    const wxString& name
)
: wxGLCanvas(parent, (wxGLCanvas*) NULL, id, pos, size, style, name, attribList)
{
    // ...
} After the wxGLCanvas element is created, multisampling is turned on by default. To disable or enable it at will, use the related OpenGL calls: glEnable(GL_MULTISAMPLE);
glDisable(GL_MULTISAMPLE); Multisampling should now work with the wxWidgets OpenGL program. Hopefully, it should be supported in the stable release of wxWidgets soon, making this information irrelevant :-)"
21983,1738.0,2008-08-22T09:37:44Z,21870,3,"Bear in mind that the EntLib documentation specifically steers you towards the ASP.NET cache for ASP.NET applications. That's probably the strongest recommendation towards using it here. Plus the EntLib cache doesn't have dependencies, which for me is a big reason not to use it. I don't think there's a technical limitation as such on shipping System.Web as part of your app, though it's slightly odd that they've put that notice in on the .NET 3.5 page. Hanselman actually says he started out being creeped out by this notion, but became convinced. Also if you read the comments, he says that the block has too many moving parts and the ASP.NET Cache is much more lightweght. I think this is exactly the kind of problem that Velocity is going to solve, but that's only a preview for now :-( I'd say use Web.Caching and see how you get on. If you put some kind of abstraction layer over the top of it, you've always got the option to swap it out for the EntLib block later on if you find problems."
22013,1908.0,2008-08-22T10:08:45Z,22000,3,"I know this might sound bad, but you need to ensure there is no whitespace between then end of you <img> tag and the start of the end </td> tag. i.e. The following will present the problem: <td>
 <img src=""image.jpg""/>
</td> And this will not: <td><img src=""image.jpg""/></td> Hope that helps. Edit: OK, that wasn't the solution at all. doh!"
22016,968.0,2008-08-22T10:09:20Z,22000,1,"I haven't looked up the whole thing, but the problem lies somewhere in the style sheets. If you copy out only the table part of it, it is displaying the map correctly. If you remove the final </span> tag from this part, it is also working (however the page gets mixed): <div class=""inner""><span class=""corners-top""><span></span></span>
<div class=""content"" style=""font-size: 1.1em;"">

<!-- Stackoverflow findy thingy -->
<table border=""0"" cellspacing=""0"" cellpadding=""0""> So either try from the beginning with the css or try to remove one-by-one them, to see, which is causing the problem."
22025,2434.0,2008-08-22T10:14:24Z,22000,16,I think you need to use display: block on your images. When images are inline there's a little extra space for the line spacing.
22068,383.0,2008-08-22T11:03:36Z,22000,4,"You could also adjust the line height of the td element: td {
    line-height: 0
}"
22210,2257.0,2008-08-22T12:34:20Z,22140,1,"I think the way to do(get around) this problem is to develop a ""./configure && make"" script that your client uses to install, specify and compile the binaries. That would offcourse require that the client has all the source-code installed on his server or you can make it available on an NFS share."
22213,2313.0,2008-08-22T12:36:40Z,10300,0,"There is Castle.Components.Validator module in Castle project. It's very agile and powerfull. It generates validation rules based on model attributes (or any other source) and even able to generate JS validation using jQuery, Prototype Validation, fValidate and other.
Of course it's wise to abstract validator away behind IValidationEngine interface."
22361,4367.0,2008-08-22T13:35:39Z,21640,101,"The following (C#) code should do the trick Uri uri = new Uri(""http://www.mywebsite.com:80/pages/page1.aspx"");
string requested = uri.Scheme + Uri.SchemeDelimiter + uri.Host + "":"" + uri.Port;"
22504,2131.0,2008-08-22T14:31:19Z,22500,1,"The biggest single difference, I think, is function prototyping and the syntax for describing the types of function arguments."
22516,2132.0,2008-08-22T14:35:12Z,22500,8,"There are some minor differences, but I think later editions of K&R are for ANSI C, so there's no real difference anymore. ""C Classic"" for lack of a better terms had a slightly different way of defining functions, i.e. int f( p, q, r )  
int p, float q, double r;  
{  
    // Code goes here  
} I believe the other difference was function prototypes. Prototypes didn't have to - in fact they couldn't - take a list of arguments or types. In ANSI C they do."
22547,1412.0,2008-08-22T14:48:18Z,22500,25,"There may be some confusion here about what ""K&R C"" is. The term refers to the language as documented in the first edition of ""The C Programming Language."" Roughly speaking: the input language of the Bell Labs C compiler circa 1978. Kernighan and Ritchie were involved in the ANSI standardization process. The ""ANSI C"" dialect superceded ""K&R C"" and subsequent editions of ""The C Programming Language"" adopt the ANSI conventions. ""K&R C"" is a ""dead language,"" except to the extent that some compilers still accept legacy code."
22579,1293.0,2008-08-22T14:56:16Z,22570,4,"where
year(date1) = year(date2)
and month(date1) = month(date2)
and day(date1) = day(date2)"
22592,,2008-08-22T15:02:46Z,22570,34,"This is much more concise: where 
  datediff(day, date1, date2) = 0"
22593,2272.0,2008-08-22T15:03:07Z,22590,1,"In our scenario, we have a separate file server that both of our front end app servers write to, that way you either server has access to the same sets of files."
22596,1908.0,2008-08-22T15:04:40Z,22590,0,"The best solution for this is usually to provide the shared area on some form of SAN, which will be accessible from all servers and contain failover. This also has the benefit that you don't have to provide sticky load balancing, the upload can be handled by one server, and the edit by another."
22600,794.0,2008-08-22T15:06:59Z,22570,1,"this will remove time component from a date for you: select dateadd(d, datediff(d, 0, current_timestamp), 0)"
22640,2199.0,2008-08-22T15:16:00Z,22570,10,"You pretty much have to keep the left side of your where clause clean. So, normally, you'd do something like: WHERE MyDateTime >= @activityDateMidnight 
      AND MyDateTime < (@activityDateMidnight + 1) (Some folks prefer DATEADD(d, 1, @activityDateMidnight) instead - but it's the same thing). The TimeZone table complicates matter a bit though. It's a little unclear from your snippet, but it looks like t.TheDateInTable is in GMT with a Time Zone identifier, and that you're then adding the offset to compare against @activityDateMidnight - which is in local time. I'm not sure what ds.LocalTimeZone is, though. If that's the case, then you need to get @activityDateMidnight into GMT instead."
22641,1432.0,2008-08-22T15:16:24Z,22590,0,A shared SAN with failover is a great solution with a great (high) cost.  Are there any similar solutions with failover at a reasonable cost?  Perhaps something like DRBD for windows? The problem with a simple shared filesystem is the lack of redundancy (what if the fileserver goes down)?
22653,1875.0,2008-08-22T15:20:19Z,22590,1,"At a former job we had a cluster of web servers with an F5 load balancer in front of them.  We had a very similar problem in that our applications allowed users to upload content which might include photo's and such.  These were legacy applications and we did not want to edit them to use a database and a SAN solution was too expensive for our situation. We ended up using a file replication service on the two clustered servers. This ran as a service on both machines using an account that had network access to paths on the opposite server.  When a file was uploaded, this backend service sync'd the data in the file system folders making it available to be served from either web server. Two of the products we reviewed were ViceVersa and PeerSync .  I think we ended up using PeerSync."
22655,740.0,2008-08-22T15:21:21Z,22570,3,Make sure to read Only In A Database Can You Get 1000% + Improvement By Changing A Few Lines Of Code so that you are sure that the optimizer can utilize the index effectively when messing with dates
22656,1349.0,2008-08-22T15:22:51Z,22570,0,"You're spoilt for choice in terms of options here. If you are using Sybase or SQL Server 2008 you can create variables of type date and assign them your datetime values. The database engine gets rid of the time for you. Here's a quick and dirty test to illustrate (Code is in Sybase dialect): declare @date1 date
declare @date2 date
set @date1='2008-1-1 10:00'
set @date2='2008-1-1 22:00'
if @date1=@date2
    print 'Equal'
else
    print 'Not equal' For SQL 2005 and earlier what you can do is convert the date to a varchar in a format that does not have the time component. For instance the following returns 2008.08.22 select convert(varchar,'2008-08-22 18:11:14.133',102) The 102 part specifies the formatting (Books online can list for you all the available formats) So, what you can do is write a function that takes a datetime and extracts the date element and discards the time. Like so: create function MakeDate (@InputDate datetime) returns datetime as
begin
    return cast(convert(varchar,@InputDate,102) as datetime);
end You can then use the function for companions Select * from Orders where dbo.MakeDate(OrderDate) = dbo.MakeDate(DeliveryDate)"
22662,225.0,2008-08-22T15:27:02Z,22570,0,"I would use the dayofyear function of datepart: Select *
from mytable
where datepart(dy,date1) = datepart(dy,date2)
and
year(date1) = year(date2) --assuming you want the same year too See the datepart reference here ."
22670,1453.0,2008-08-22T15:31:35Z,22570,0,"Regarding timezones, yet one more reason to store all dates in a single timezone (preferably UTC). Anyway, I think the answers using datediff, datepart and the different built-in date functions are your best bet."
22802,2199.0,2008-08-22T16:33:13Z,22570,2,"Eric Z Beard: I do store all dates in GMT. Here's the use case: something happened at 11:00 PM EST on the 1st, which is the 2nd GMT. I want to see activity for the 1st, and I am in EST so I will want to see the 11PM activity. If I just compared raw GMT datetimes, I would miss things. Each row in the report can represent an activity from a different time zone. Right, but when you say you're interested in activity for Jan 1st 2008 EST: SELECT @activityDateMidnight = '1/1/2008', @activityDateTZ = 'EST' you just need to convert that to GMT (I'm ignoring the complication of querying for the day before EST goes to EDT, or vice versa): Table: TimeZone
Fields: TimeZone, Offset
Values: EST, -4

--Multiply by -1, since we're converting EST to GMT.
--Offsets are to go from GMT to EST.
SELECT @activityGmtBegin = DATEADD(hh, Offset * -1, @activityDateMidnight)
FROM TimeZone
WHERE TimeZone = @activityDateTZ which should give you '1/1/2008 4:00 AM'. Then, you can just search in GMT: SELECT * FROM EventTable
WHERE 
   EventTime >= @activityGmtBegin --1/1/2008 4:00 AM
   AND EventTime < (@activityGmtBegin + 1) --1/2/2008 4:00 AM The event in question is stored with a GMT EventTime of 1/2/2008 3:00 AM. You don't even need the TimeZone in the EventTable (for this purpose, at least). Since EventTime is not in a function, this is a straight index scan - which should be pretty efficient. Make EventTime your clustered index, and it'll fly. ;) Personally, I'd have the app convert the search time into GMT before running the query."
22803,1190.0,2008-08-22T16:33:13Z,22720,0,Set a short timeout? Does isOutputShutdown() not get you what you want? You could always build a SocketWatcher class that spins up in its own Thread and repeatedly tries to write empty strings to the Socket until that raises a SocketClosedException .
22812,1453.0,2008-08-22T16:37:59Z,22570,0,"@Eric: No you won't miss anything. If you want to see what happened at 11pm EST, you look for things that happened at the corresponding time in GMT. If all the records are converted into GMT before saving, it shouldn't matter. So long as you convert the requested time (i.e. 11pm) to GMT before using it in the query."
22835,,2008-08-22T16:47:52Z,22340,1,"In most firewall setups, the TCP connection will be torn down by the firewall if it is idle to conserve resources. The idle timeout is probably not something you can control. Some will tear them down if they are idle and a resource limit is being hit. Most corp environments won't allow any machines to make an outbound TCP connection anyway. Also, using this mechanism means you are going to have scaling problems. I think more reliable solution is to queue up information and have your clients poll for them regularly. Utilize caching if possible such that a subsequent client poll will get the cached data from the customers proxy cache, if they are using one. If you have to push data in a timely manner, in sub-second land (i.e. financial services), then consider some messaging infrastructure such an NServiceBus distributor on client side, but that will require a customer install... So have you tried using Toredo? Having read that it would appear there it is prob too complicated for a user to setup."
22887,2148.0,2008-08-22T17:04:53Z,22880,-1,To reduce the risk you can also associate the originating IP with the session. That way an attacker has to be within the same private network to be able to use the session. Checking referer headers can also be an option but those are more easily spoofed.
22913,2527.0,2008-08-22T17:11:46Z,22880,79,"Encrypting the session value will have zero effect. The session cookie is already an arbitrary value, encrypting it will just generate another arbitrary value that can be sniffed. The only real solution is HTTPS. If you don't want to do SSL on your whole site (maybe you have performance concerns), you might be able to get away with only SSL protecting the sensitive areas. To do that, first make sure your login page is HTTPS. When a user logs in, set a secure cookie (meaning the browser will only transmit it over an SSL link) in addition to the regular session cookie. Then, when a user visits one of your ""sensitive"" areas, redirect them to HTTPS, and check for the presence of that secure cookie. A real user will have it, a session hijacker will not. EDIT : This answer was originally written in 2008. It's 2016 now, and there's no reason not to have SSL across your entire site. No more plaintext HTTP!"
23137,2178.0,2008-08-22T18:33:33Z,22980,10,"The following will work <%= Post.find_unread_by(current_user).size %> or <%= Post.find_unread_by(current_user).length %> However if you check your development.log you should see that it gets the unread count by Retrieving all the posts Retrieving all the posts read by the user Removing all of 2. from 1. in ruby This will be very bad performance wise with lots of posts. A better way would be to retrieve the posts read by the current user and then use ActiveRecord::Calculations to get a count without retrieving all the posts in the database Post.count(:conditions => [ ""id NOT IN (?)"", Post.find_read_by(current_user)]) This should go into your Post model to follow best practices of not having finders in the view  or controller Post.rb def self.unread_post_count_for_user(user)
  count(:conditions => [ ""id NOT IN (?)"", Post.find_read_by(user)])
end Then your view will just be <%= Post.unread_post_count_for_user(current-user) %>"
23196,1968.0,2008-08-22T18:58:07Z,23190,0,"Actually, by colllapsing the colums you already summed them, so the dimension doesn't matter at all for your example. Did I miss something or did you?"
23210,832.0,2008-08-22T19:04:34Z,23190,0,"I think the best thing to do here would be one/both of two things: Rethink the design, if its too complex, find a less-complex way. Stop trying to visualise it.. :P Just store the dimensions in question that you need to sum, then do them one at a time. Once you have the base code, then look at improving the efficiency of your algorithm."
23225,832.0,2008-08-22T19:13:10Z,23190,0,"I beg to differ, there is ALWAYS another way.. And if you really cannot refactor, then you need to break the problem down into smaller parts.. Like I said, establish which dimensions you need to sum, then hit them one at a time.. Also, stop changing the edits, they are correcting your spelling errors, they are trying to help you ;)"
23226,1799.0,2008-08-22T19:13:50Z,23190,0,"When you say you don't know how many dimensions there are, how exactly are you defining the data structures? At some point, someone needs to create this array, and to do that, they need to know the dimensions of the array. You can force the creator to pass in this data along with the array. Unless the question is to define such a data structure..."
23257,572.0,2008-08-22T19:22:48Z,23250,23,"I use it every time I refer to an instance variable, even if I don't need to. I think it makes the code more clear."
23258,1975282.0,2008-08-22T19:23:02Z,23250,4,"I use it anywhere there might be ambiguity (obviously). Not just compiler ambiguity (it would be required in that case), but also ambiguity for someone looking at the code."
23259,1075.0,2008-08-22T19:23:16Z,23250,1,I tend to underscore fields with _ so don't really ever need to use this. Also R# tends to refactor them away anyway...
23260,1782.0,2008-08-22T19:23:34Z,23250,2,"You should always use it, I use it to diferantiate private fields and parameters (because our naming conventions state that we don't use prefixes for member and parameter names (and they are based on information found on the internet, so I consider that a best practice))"
23264,1595.0,2008-08-22T19:26:06Z,23250,87,"I only use it when absolutely necessary, ie, when another variable is shadowing another.  Such as here: class Vector3
{
    float x;
    float y;
    float z;

    public Vector3(float x, float y, float z)
    {
        this.x = x;
        this.y = y;
        this.z = z;
    }

} Or as Ryan Fox points out, when you need to pass this as a parameter."
23267,55.0,2008-08-22T19:26:37Z,23250,11,"Any time you need a reference to the current object. One particularly handy scenario is when your object is calling a function and wants to pass itself into it. Example: void onChange()
{
    screen.draw(this);
}"
23275,157.0,2008-08-22T19:29:03Z,23190,0,"You're doing this in c/c++... so you have an array of array of array... you don't have to visualize 20 dimensions since that isn't how the data is laid out in memory, for a 2 dimensional: [1] --> [1,2,3,4,5,6,...]
[2] --> [1,2,3,4,5,6,...]
[3] --> [1,2,3,4,5,6,...]
[4] --> [1,2,3,4,5,6,...]
[5] --> [1,2,3,4,5,6,...]
 .           .
 .           .
 .           . so, why can't you iterate across the first one summing it's contents? If you are trying to find the size, then sizeof(array)/sizeof(int) is a risky approach. You must know the dimension to be able to process this data, and set the memory up, so you know the depth of recursion to sum. Here is some pseudo code of what it seems you should do, sum( n_matrix, depth )
  running_total = 0
  if depth = 0 then
    foreach element in the array
      running_total += elm
  else 
     foreach element in the array
       running_total += sum( elm , depth-1 )
  return running_total"
23276,1975282.0,2008-08-22T19:29:29Z,23270,-2,"Not sure what the confusion is. Sites on your intranet are in the intranet zone, web sites are in the internet zone, and sites on your computer are in the local zone, unless you've specifically overridden something in the browser's preferences."
23278,1314.0,2008-08-22T19:29:51Z,23250,1,"I pretty much only use this when referencing a type property from inside the same type.  As another user mentioned, I also underscore local fields so they are noticeable without needing this ."
23279,1327.0,2008-08-22T19:30:42Z,23250,0,"It depends on the coding standard I'm working under.  If we are using _ to denote an instance variable then ""this"" becomes redundant. If we are not using _ then I tend to use this to denote instance variable."
23281,920.0,2008-08-22T19:31:26Z,23250,7,"I tend to use it everywhere as well, just to make sure that it is clear that it is instance members that we are dealing with."
23282,1429.0,2008-08-22T19:32:36Z,23190,2,"@Jeff I actually think this is an interesting question. I'm not sure how useful it is, but it is a valid question. @Ed Can you provide a little more info on this question? You said the dimension of the array is dynamic, but is the number of elements dynamic as well? EDIT: I'm going to try and answer the question anyways. I can't give you the code off the top of my head (it would take a while to get it right without any compiler here on this PC), but I can point you in the right direction ... Let's use 8 dimensions (0-7) with indexes 0 to 3 as an example. You care about only 1,2 and 6. This means you have two arrays. First, array_care[4][4][4] for 1,2, and 6. The array_care[4][4][4] will hold the end result. Next, we want to iterate in a very specific way. We have the array input[4][4][4][4][4][4][4][4] to parse through, and we care about dimensions 1, 2, and 6. We need to define some temporary indexes: int dim[8] = {0,0,0,0,0,0,0,0}; We also need to store the order in which we want to increase the indexes: int increase_index_order[8] = {7,5,4,3,0,6,2,1};
int i = 0; This order is important for doing what you requested. Define a termination flag: bool terminate=false; Now we can create our loop: while (terminate)
{
array_care[dim[1]][dim[2]][dim[6]] += input[dim[0]][dim[1]][dim[2]][dim[3]][dim[4]][dim[5]][dim[6]][dim[7]];

while ((dim[increase_index_order[i]] = 3) && (i < 8))
{
dim[increase_index_order[i]]=0;
i++;
}

if (i < 8) {
dim[increase_index_order[i]]++; i=0;
} else {
terminate=true;
}
} That should work for 8 dimensions, caring about 3 dimensions. It would take a bit more time to make it dynamic, and I don't have the time. Hope this helps. I apologize, but I haven't learned the code markups yet. :("
23306,1790.0,2008-08-22T19:46:46Z,23250,34,"I can't believe all of the people that say using it always is a ""best practice"" and such. Use ""this"" when there is ambiguity, as in Corey's example or when you need to pass the object as a parameter, as in Ryan's example . There is no reason to use it otherwise because being able to resolve a variable based on the scope chain should be clear enough that qualifying variables with it should be unnecessary. EDIT: The C# documentation on ""this"" indicates one more use, besides the two I mentioned, for the ""this"" keyword - for declaring indexers EDIT: @Juan: Huh, I don't see any inconsistency in my statements - there are 3 instances when I would use the ""this"" keyword (as documented in the C# documentation), and those are times when you actually need it. Sticking ""this"" in front of variables in a constructor when there is no shadowing going on is simply a waste of keystrokes and a waste of my time when reading it, it provides no benefit."
23307,517.0,2008-08-22T19:46:57Z,23190,0,"x = number_of_dimensions;
while (x > 1)
{
  switch (x)
  {
    case 20:
      reduce20DimensionArray();
      x--;
    break;
    case 19:
      .....
  }
} (Sorry, couldn't resist.)"
23314,572.0,2008-08-22T19:50:14Z,23310,0,"Anything, but I would learn a modern system like git or subversion myself. My first VCS was RCS, but I got the basics down."
23316,2328.0,2008-08-22T19:51:03Z,23310,0,"Well, if you are just wanting to learn on your own, I would say you should go with something free, like subversion.  If you are a company who has never used source control before, then it really depends on your needs."
23321,1037.0,2008-08-22T19:53:08Z,23310,8,"I'd suggest you try Subversion, for example with the 1-click SVN installer . Try searching SO for ""Subversion"", and you'll find loads of questions with answers that point to good tutorials. Good luck!"
23322,96.0,2008-08-22T19:53:17Z,23310,0,"My first exposure was CVS with WinCVS as a client.  it was horrid.  Next was Subversion, with TortoiseSVN and Eclipse's integration.  It was intuitive, and heavenly.  I think that using CVS with TortoiseCVS and Eclipse's would be nice as well, though I prefer the way SVN handles revisioning. The entire repository is versioned with each check in, not individual files."
23323,414.0,2008-08-22T19:53:31Z,23250,-6,"Never. Ever. If you have variable shadowing, your naming conventions are on crack. I mean, really, no distinguishing naming for member variables? Facepalm"
23329,1680.0,2008-08-22T19:56:09Z,23310,5,"There are a few core concepts that I think are important to learn: Check-ins/check-outs (obviously) Local versions vs. server versions Mapping/Binding a local workspace to a remote store or repository. Merging your changes back into a file that contains changes from
others. Branching (what it is, when/why to use it) Merging changes from a branch back into a main branch or trunk. Most modern source control systems require some knowledge of the above topics and should help facilitate you learning them. Then you have distributed source control, which I don't have any experience with but is supposed to be fairly complicated and may not be suitable for a beginner. Subversion is great because it has all of the modern features you'd want and is free. Git is also becoming an increasingly popular option and is another free or very low cost alternative to Subversion. Knowledge regarding the concepts of branching and merging become critical for using Git, however. You can use unfuddle as a free and easy way to experiment with both Git and Subversion. I use it to host a couple of subversion repositories for some side projects I've worked on in the past."
23339,1935.0,2008-08-22T19:59:58Z,20910,6,"Silverlight programmer's don't know what they're missing out on, when it comes to Flex.  Silverlight lacks the component model and event triggering capabilites that Flex has.  Using XNA, and C#, a friend of mine has to jump through all kinds of hoops to get his Silverlight application to work.  Then, it has to be handed off to a designer to get it to look half way decent. Listen to the deepfriedbytes.com podcasts on Silverlight, and you'll hear how even a couple guys that really push Silverlight, acknowledge some of these issues.  (I think , if I recall correctly, one of the guys works for Microsoft, but I could be wrong - I listened to it last week).  They agree that Silverlight isn't quite ready for any huge applications, in its current state. I would go with Flex, for a nice clean, straightforward approach - especially if you're already familiar with Flash and ActionScript 3.0.  Flex makes alot more sense, in my opinion - Silverlight still has to mature."
23340,2084.0,2008-08-22T20:00:47Z,23250,25,I use it whenever StyleCop tells me to. StyleCop must be obeyed. Oh yes.
23353,2084.0,2008-08-22T20:09:20Z,23310,24,"Anything but Visual Source Safe; preferably one which supports the concepts of branching and merging. As others have said, Subversion is a great choice, especially with the TortoiseSVN client. Be sure to check out (pardon the pun) Eric Sink's classic series of Source Control HOWTO articles ."
23371,1807.0,2008-08-22T20:16:57Z,23190,0,"If I understand correctly, you want to sum all values in the cross section defined at each ""bin"" along 1 dimension.  I suggest making a 1D array for your destination, then looping through each element in your array adding the value to the destination with the index of the dimension of interest. If you are using arbitrary number of dimensions, you must have a way of addressing elements (I would be curious how you are implementing this).  Your implementation of this will affect how you set the destination index.  But an obvious way would be with if statements checked in the iteration loops."
23393,1441.0,2008-08-22T20:26:08Z,23310,0,"I'd also recommend Subversion. It does not take too long to set up, it is free, and there is a really good book available online that goes over the basics as well as some advanced topics: http://svnbook.red-bean.com/"
23408,1632.0,2008-08-22T20:31:59Z,23310,0,Subversion with tortoisesvn. (tortoisesvn because you can see a lot of what goes on visually and will provide a good jumping off point for the command line stuff. ) There is tons of documentation out there and most likely you will see it at least one point in your career. Almost every company I have worked for and interviewed with runs SVN.
23430,2362.0,2008-08-22T20:39:40Z,23310,2,I found http://unfuddle.com saved me messing about with installing SVN or git. You can get a free account in there and use either of those - plus you can use your OpenID there. Then you avoid having to mess about setting it up right and focus on how you're going to use it!
23431,414.0,2008-08-22T20:40:17Z,23370,1,"There are a few ambiguities in your question. What operation needs to be successful? For everything you want to know about drag and drop, browse through these search results (multiple pages worth): Raymond Chen on drag and drop"
23435,1737192.0,2008-08-22T20:42:19Z,23250,187,"I don't mean this to sound snarky, but it doesn't matter. Seriously. Look at the things that are important: your project, your code, your job, your personal life. None of them are going to have their success rest on whether or not you use the ""this"" keyword to qualify access to fields. The this keyword will not help you ship on time. It's not going to reduce bugs, it's not going to have any appreciable effect on code quality or maintainability. It's not going to get you a raise, or allow you to spend less time at the office. It's really just a style issue. If you like ""this"", then use it. If you don't, then don't. If you need it to get correct semantics then use it. The truth is, every programmer has his own unique programing style. That style reflects that particular programmer's notions of what the ""most aesthetically pleasing code"" should look like. By definition, any other programmer who reads your code is going to have a different programing style. That means there is always going to be something you did that the other guy doesn't like, or would have done differently. At some point some guy is going to read your code and grumble about something. I wouldn't fret over it. I would just make sure the code is as aesthetically pleasing as possible according to your own tastes. If you ask 10 programmers how to format code, you are going to get about 15 different opinions. A better thing to focus on is how the code is factored. Are things abstracted right? Did I pick meaningful names for things? Is there a lot of code duplication? Are there ways I can simplify stuff? Getting those things right, I think, will have the greatest positive impact on your project, your code, your job, and your life. Coincidentally, it will probably also cause the other guy to grumble the least. If your code works, is easy to read, and is well factored, the other guy isn't going to be scrutinizing how you initialize fields. He's just going to use your code, marvel at it's greatness, and then move on to something else."
23436,1490.0,2008-08-22T20:42:23Z,23370,0,"So, you intend to modify the data being dropped based on the drop target?  I don't think this is possible; after all, you populate the data when the drag is initiated."
23442,2434.0,2008-08-22T20:46:19Z,23190,2,"This kind of thing is much easier if you use STL containers, or maybe Boost.MultiArray . But if you must use an array: #include <iostream>
#include <boost/foreach.hpp>
#include <vector>

int sum(int x) {
    return x;
}

template <class T, unsigned N>
int sum(const T (&x)[N]) {
    int r = 0;
    for(int i = 0; i < N; ++i) {
        r += sum(x[i]);
    }
    return r;
}

template <class T, unsigned N>
std::vector<int> reduce(const T (&x)[N]) {
    std::vector<int> result;
    for(int i = 0; i < N; ++i) {
        result.push_back(sum(x[i]));
    }
    return result;
}

int main() {
    int x[][2][2] = {
        { { 1, 2 }, { 3, 4 } },
        { { 5, 6 }, { 7, 8 } }
    };

    BOOST_FOREACH(int v, reduce(x)) {
        std::cout<<v<<""\n"";
    }
}"
23493,398.0,2008-08-22T21:09:44Z,23490,3,A nightly build of Resharper
23496,380.0,2008-08-22T21:11:05Z,23490,14,Notepad++ for sure
23502,2361.0,2008-08-22T21:12:48Z,23250,166,"There are several usages of this keyword in C#. To qualify members hidden by similar name To have an object pass itself as a parameter to other methods To have an object return itself from a method To declare indexers To declare extension methods To pass parameters between constructors To internally reassign value type (struct) value . To invoke an extension method on the current instance To cast itself to another type To chain constructors defined in the same class You can avoid the first usage by not having member and local variables with the same name in scope, for example by following common naming conventions and using properties (Pascal case) instead of fields (camel case) to avoid colliding with local variables (also camel case). In C# 3.0 fields can be converted to properties easily by using auto-implemented properties ."
23504,1441.0,2008-08-22T21:12:57Z,23490,5,"I like Whole Tomato's Visual Assist X plug-in for Visual Studio. I think you get the ""most"" out of it when programming in C++ (and especially older versions of visual studio), but there are some additional syntax highlighting and refactoring tools, plus a decent search based on context / scope."
23507,2329.0,2008-08-22T21:15:07Z,23490,1,I cannot live without Eclipse and Mylyn
23508,1709.0,2008-08-22T21:15:12Z,23490,1,"Komodo Edit, Cygwin (ssh, cat, less, sed, grep, etc.), Python, TortoiseSVN, TortoiseCVS"
23513,1797.0,2008-08-22T21:16:54Z,23490,3,Firebug .
23514,95.0,2008-08-22T21:17:17Z,23490,9,"Scott Hanselman has a great, updated every year or two list of tools: Scott Hanselman's Ultimate Developer and Power Users Tool List for Windows"
23516,274.0,2008-08-22T21:18:41Z,23490,2,"A good editor and your compiler of choice. Sure, some tools make your job a little easier.  Developing .Net applications without using Visual Studio would be more convoluted, but I would bet that at the end of the task, using only a text editor and the csc compiler, you would have a guru like comprehension of the language in no time at all.  You would learn things that other people may never get into. Of course, a good debugger helps (Also built into VS).  I use Komodo for Perl development purely for the debugging tools involved.  Even though I still prefer to edit the code using e-TextEditor."
23520,380.0,2008-08-22T21:19:47Z,23250,4,"Here's when I use it: Accessing Private Methods from within the class (to differentiate) Passing the current object to another method (or as a sender object, in case of an event) When creating extension methods :D I don't use this for Private fields because I prefix private field variable names with an underscore (_)."
23526,2374.0,2008-08-22T21:21:20Z,23490,4,Subversion + TortoiseSVN
23531,1574.0,2008-08-22T21:22:24Z,23490,11,"In no particular order (I'm a .NET web developer if you can't tell from the list): Resharper - Keeps my code slim and clean! Reflector - Every now and then you need to figure out how the heck something is working in the .NET library. Firebug - Every web developer has this installed because it makes markup and css debugging so much easier. Tortoise SVN - By far the best version control system I have ever used.  Absolutely no complaints about it. NUnit - Unit testing that doesn't get in your way.  Plus it integrates nicely with Resharper! Notepad - For whatever reason, I can't shake the nostalgic feeling I get using this.  Still my go-to application for several things (to-do lists, quick side notes, quick and dirty clipboard, etc.)."
23538,829.0,2008-08-22T21:24:23Z,23490,1,"TextPad rocks! And CSSViewer (FF plug-in) is nice. Heard Firebug is even better, since it allows you to edit, too, but haven't tried it. Also, virtual machines. I'm using using MS Virtual PC (w/ VM additions) right now for multiple projects and it suits my purposes well. I'm sure there are better vm solutions, too, I just haven't had to look into them. CrossLoop and Skype for collaboration/pair programming (particularly for remote employees). AgentRansak for text/file/foler searching. I haven't used this to it's full extent, since I'm new to it, so I don't know how robust it can be. It works well for what I use it for, though. I am much more familiar with TextPad's search/replace functionality (which rocks!)."
23542,2272.0,2008-08-22T21:25:07Z,23490,1,Another vote for notepad++ Firebug or the dev toolbar in IE Lifehackers Texter (for text expansion) I couldn't live my life on a computer without humanized's Enso product
23544,925.0,2008-08-22T21:25:52Z,23490,1,"Notepadd++, Mercurial, FireFox, FireBug"
23550,2374.0,2008-08-22T21:30:37Z,23490,14,Winamp (I love coding with music playing in the background) Coffee
23559,1632.0,2008-08-22T21:34:14Z,23490,1,"Notepad2, e.TextEditor, Textmate WinSplit Revolution Google, Pandora Synergy FireBug SVN Visual Studio if .net app"
23605,101.0,2008-08-22T21:52:27Z,23490,1,"A lot of it depends on the kind of work I'm doing.  I use git or svn on pretty much everything I write these days. Github has raised the bar for ease of collaboration and generally what I expect from an SCM repository. TextMate always comes in useful for snippets, regex find and replace, and all sorts of little editing niceties; for most projects it's my primary text editor.  For Java I'll spend a good bit of time in Eclipse , and back when I was did .NET work I'd use Visual Studio .  If I'm scratching together a prototype design for a web site, I'll use Coda or something similar. If you count libraries and frameworks as ""development tools,"" Ruby's regexes take the cake for ease of use.  Haskell's Parsec wins for doing serious parsing, followed very closely by Java's ANTLR .  Hype be damned, I've yet to be as productive writing a web app than I am with Ruby on Rails , though Pylons in Python land is nice.  Likewise with Visual Studio for doing client side GUI work, though I think Cocoa XCode in Leopard could be very competitive if I ever get a good grasp on Objective-C. LLVM's IR is the new assembly if you're writing a compiler."
23611,91.0,2008-08-22T21:57:23Z,23610,0,"OU is an Organizational Unit (sort of like a Subfolder in Explorer), not a Group, Hence group1, 2 and 3 are not actually groups. You are looking for the DN Attribute, also called ""distinguishedName"". You can simply use DOMAIN\DN once you have that. Edit: For groups, the CN (Common Name) could also work. The full string from Active Directory normally looks like this: cn=Username,cn=Users,dc=DomainName,dc=com (Can be longer or shorter, but the important bit is that the ""ou"" part is worthless for what you're trying to achieve."
23644,2454.0,2008-08-22T22:15:04Z,23490,2,"Vim, Cygwin, TortoiseSVN, Eclipse. SoapUI is an awesome tool if you're working with SOAP web services. I also find TCPTrace a very handy little tool."
23655,2257.0,2008-08-22T22:20:58Z,23620,0,"I know at least yahoo!'s Rich Text Editor will let you use the included spell checker in FireFox. I also tested FCKeditor, but that requires the users to install additional plugins on their computer."
23656,91.0,2008-08-22T22:21:36Z,23610,4,"Programatically or Manually? Manually, i prefer AdExplorer , which is a nice Active directory Browser. You just connect to your domain controller and then you can look for the user and see all the details. Of course, you need permissions on the Domain Controller, not sure which though. Programatically, it depends on your language of couse. On .net, the System.DirectoryServices Namespace is your friend. (I don't have any code examples here unfortunately) For Active Directory, I'm not really an expert apart from how to query it, but here are two links I found useful: http://www.computerperformance.co.uk/Logon/LDAP_attributes_active_directory.htm http://en.wikipedia.org/wiki/Active_Directory (General stuff about the Structure of AD)"
23658,459.0,2008-08-22T22:22:42Z,23490,45,"Let me be general [then specific]: Your IDE of choice [ VS 2008 here] Your debugger [It is usually part of your IDE, but sometimes WinDbg is needed] Its plugins for refactoring and source control [ Resharper 4+ and Ankh SVN 2+ ] Your OS's addons for source control [ Tortoise SVN ] A better Diff and Merge Tool to plug into the above SCM tools [ WinMerge ] A fast loading text editor for when your IDE is too much [ vim , Notepad++ ] If you're doing web development, get tools for that [ Firefox 3 with Add-ons: Web Developer , Firebug , TamperData , Poster , Firecookie , FireFTP , FirePHP , Rainbow for Firebug, ReloadEvery , Selenium IDE ] Requisite tools for working with text [ GNU TextUtils , via cygwin or gnuwin32.sf.net ] Scripting tools [ Perl , Python , zsh , all those GNU base packages in cygwin] A Regular Expression testing tool for when your eyes hurt [ Expresso , RegexBuddy ] For Java I swap out 1 and 3 with Eclipse , and its plugins for Maven and SVN , I haven't found a refactoring plug in... you'd think I'd use IntelliJ IDEA but I never started using it."
23660,324.0,2008-08-22T22:23:50Z,23610,3,"You need to go to the Active Directory Users Snap In after logging in as a domain admin on the machine: Go to start --> run and type in mmc. In the MMC console go to File --> Add/Remove Snap-In Click Add Select Active Directory Users and Computers and select Add. Hit Close and then hit OK. From here you can expand the domain tree and search (by right-clicking on the domain name). You may not need special privileges to view the contents of the Active Directory domain, especially if you are logged in on that domain.  It is worth a shot to see how far you can get. When you search for someone, you can select the columns from View --> Choose Columns.  This should help you search for the person or group you are looking for."
23663,1580.0,2008-08-22T22:26:09Z,23610,0,"Thanks adeel825 & Michael Stum. My problem is, though, i'm in a big corporation and do not have access to log in as the domain admin nor to view the active directory, so i guess my solution is to try and get that level of access."
23665,2134.0,2008-08-22T22:29:43Z,23640,6,"I know a few of the developers on the Carleton University developed Blindside Project .  They are actively developing an open-source web conferencing and presentation tool for e-learning, with the intent of eventually offering university courses online. It's pretty fully featured software, and is meant to be installed as a server that can host many conference rooms at a time.  It has voice, video, text, and a whiteboard/slideshow ( Edit: supports PDF at the moment) capability.  One feature I think it neat is that students can 'raise their hands' in the class to ask the instructor a question, where they can take the floor for a moment. Check out the demo on the site (if it's not working anymore I'll nudge the developers).  Another pro is that the clients only need to have flash installed. I just logged onto the online demo and created this preview: This project is now called BigBlueButton : http://code.google.com/p/bigbluebutton/ Here is the demo: http://demo.bigbluebutton.org/"
23666,2257.0,2008-08-22T22:29:51Z,23640,1,"I do not have personal experience with this product, but dokeos is recommended by several people on other sites."
23672,2257.0,2008-08-22T22:34:38Z,23640,1,@adeel - I think this blog entry can give you some details at least about one user that has tried DimDim .
23679,91.0,2008-08-22T22:37:15Z,23610,0,"Well, AdExplorer runs on your Local Workstation (which is why I prefer it) and I believe that most users have read access to AD anyway because that's actually required for stuff to work, but I'm not sure about that."
23692,2134.0,2008-08-22T22:46:06Z,23640,3,The BlindSide site also listed these other projects: ePresence OpenMeetings DimDim WebHuddle All opensource as well.
23709,2230.0,2008-08-22T23:03:51Z,23610,2,"You do not need domain admin rights to look at the active directory. By default, any (authenticated?) user can read the information that you need from the directory. If that wasn't the case, for example, a computer (which has an associated account as well) could not verify the account and password of its user. You only need admin rights to change the contents of the directory. I think it is possible to set more restricted permissions, but that's not likely the case."
23733,811.0,2008-08-22T23:24:39Z,23620,4,"TinyMCE only goes out of its way to disable spell-checking when you don't specify the gecko_spellcheck option (i verified this with their example code). Might want to double-check your tinyMCE.init() call - it should look something like this: tinyMCE.init({
	mode : ""textareas"",
	theme : ""simple"",
	gecko_spellcheck : true
});"
23777,1797.0,2008-08-23T00:12:10Z,23770,6,"In the past I have used triggers to construct db update/insert/delete logging. You could insert a record each time one of the above actions is done on a specific table into a logging table that keeps track of the action, what db user did it, timestamp, table it was preformed on, and previous value. There is probably a better answer though as this would require you to cache the value before the actual delete or update was preformed I think. But you could use this to do rollbacks."
23780,1219.0,2008-08-23T00:14:00Z,23770,17,"One strategy you could use is MVCC, Multi-Value Concurrency Control.  In this scheme, you never do updates to any of your tables, you just do inserts, maintaining version numbers for each record.  This has the advantage of providing an exact snapshot from any point in time, and it also completely sidesteps the update lock problems that plague many databases. But it makes for a huge database, and selects all require an extra clause to select the current version of a record."
23825,1793.0,2008-08-23T01:28:23Z,23770,10,"If you are using Hibernate, take a look at JBoss Envers . From the project homepage: The Envers project aims to enable easy versioning of persistent JPA classes. All that you have to do is annotate your persistent class or some of its properties, that you want to version, with @Versioned. For each versioned entity, a table will be created, which will hold the history of changes made to the entity. You can then retrieve and query historical data without much effort. This is somewhat similar to Eric's approach , but probably much less effort. Don't know, what language/technology you use to access the database, though."
23856,2199.0,2008-08-23T02:14:08Z,22570,1,"Eric Z Beard: the activity date is meant to indicate the local time zone, but not a specific one Okay - back to the drawing board. Try this: where t.TheDateINeedToCheck BETWEEN (
    dateadd(hh, (tz.Offset + ISNULL(ds.LocalTimeZone, 0)) * -1, @ActivityDate)
    AND
    dateadd(hh, (tz.Offset + ISNULL(ds.LocalTimeZone, 0)) * -1, (@ActivityDate + 1))
) which will translate the @ActivityDate to local time, and compare against that. That's your best chance for using an index, though I'm not sure it'll work - you should try it and check the query plan. The next option would be an indexed view, with an indexed, computed TimeINeedToCheck in local time . Then you just go back to: where v.TheLocalDateINeedToCheck BETWEEN @ActivityDate AND (@ActivityDate + 1) which would definitely use the index - though you have a slight overhead on INSERT and UPDATE then."
23901,1046.0,2008-08-23T02:57:55Z,23490,4,"For Windows work: Beyond Compare - great diffing tool, works well with files and folders. Launchy - lets me start programs without moving my hands from the keyboard."
23932,1337.0,2008-08-23T03:47:32Z,23930,2,"Perl 6: Functional multi factorial ( Int $n where { $n <= 0 } ){
  return 1;
}
multi factorial ( Int $n ){
   return $n * factorial( $n-1 );
} This will also work: multi factorial(0) { 1 }
multi factorial(Int $n) { $n * factorial($n - 1) } Check Jonathan Worthington's journal on use.perl.org , for more information about the last example."
23936,1337.0,2008-08-23T03:48:58Z,23930,2,"Perl 6:Procedural sub factorial ( int $n ){

  my $result = 1;

  loop ( ; $n > 0; $n-- ){

    $result *= $n;

  }

  return $result;
}"
23938,813.0,2008-08-23T03:50:32Z,23930,2,"C: Edit: Actually C++ I guess, because of the variable declaration in the for loop. int factorial(int x) {
      int product = 1;

      for (int i = x; i > 0; i--) {
           product *= i;
      }

      return product;
 }"
23939,745.0,2008-08-23T03:51:24Z,23490,1,"For Python stuff, a good text editor (TextMate on OS X, [g]vim on Linux, Programmers Notepad on Windows), VCS (I'm mainly using git currently).. That's about it.. A bit of a stretch to call it a dev-tool, but searching Google for ""python [module name]"" is incredibly useful (I use it even though I can put the cursor over the import abc module and be taken to the pydoc page, I always found the first-google-result much better than the PyDoc page TextMate invokes.. I use PyLint to check I've not done anything stupid, but I'd hardly consider it 'must have' (I mostly use it for keeping consistent white-spacing, after commands and around x = 123 statements and so on). I'm also considering learning pdb (python debugger), but I've always found the odd print statement, or the logging module (in larger scripts) more than adequate. ..that's about it.. Text editor, VCS, module documentation."
23958,2118.0,2008-08-23T04:10:43Z,23930,2,"Javascript: factorial = function( n )
{
   return n > 0 ? n * factorial( n - 1 ) : 1;
} I'm not sure what a Factorial is but that does what the other programs do in javascript."
23969,784.0,2008-08-23T04:20:56Z,23930,12,"Haskell: ones = 1 : ones
integers   = head ones     : zipWith (+) integers   (tail ones)
factorials = head integers : zipWith (*) factorials (tail integers)"
23974,1190.0,2008-08-23T04:24:49Z,23970,2,Try ruby2ruby
23976,658.0,2008-08-23T04:25:31Z,23930,7,"Scheme Here is a simple recursive definition: (define (factorial x)
  (if (= x 0) 1
      (* x (factorial (- x 1))))) In Scheme tail-recursive functions use constant stack space. Here is a version of factorial that is tail-recursive: (define factorial
  (letrec ((fact (lambda (x accum)
                   (if (= x 0) accum
                       (fact (- x 1) (* accum x))))))
    (lambda (x)
      (fact x 1))))"
23979,184.0,2008-08-23T04:27:12Z,23930,1,"C++ factorial(int n)
{
    for(int i=1, f = 1; i<=n; i++)
        f *= i;
    return f;
}"
23982,1897.0,2008-08-23T04:32:03Z,23930,5,"C/C++ : Procedural unsigned long factorial(int n)
{
    unsigned long factorial = 1;
    int i;

    for (i = 2; i <= n; i++)
    	factorial *= i;

    return factorial;
} PHP : Procedural function factorial($n)
{
    for ($factorial = 1, $i = 2; $i <= $n; $i++)
    	$factorial *= $i;

    return $factorial;
} @Niyaz : You didn't specify return type for the function"
23989,522.0,2008-08-23T04:40:15Z,23930,124,"lolcode: sorry I couldn't resist xD HAI
CAN HAS STDIO?
I HAS A VAR
I HAS A INT
I HAS A CHEEZBURGER
I HAS A FACTORIALNUM
IM IN YR LOOP
    UP VAR!!1
    TIEMZD INT!![CHEEZBURGER]
    UP FACTORIALNUM!!1
    IZ VAR BIGGER THAN FACTORIALNUM? GTFO
IM OUTTA YR LOOP
U SEEZ INT
KTHXBYE"
24003,423.0,2008-08-23T05:05:44Z,23250,35,"Personally, I try to always use this when referring to member variables.  It helps clarify the code and make it more readable.  Even if there is no ambiguity, someone reading through my code for the first time doesn't know that, but if they see this used consistently, they will know if they are looking at a member variable or not."
24023,198.0,2008-08-23T05:39:15Z,23950,6,"class Producer implements Runnable {
   private final BlockingQueue queue;
   Producer(BlockingQueue q) { queue = q; }
   public void run() {
     try {
       while (true) { queue.put(produce()); }
     } catch (InterruptedException ex) { ... handle ...}
   }
   Object produce() { ... }
 }

 class Consumer implements Runnable {
   private final BlockingQueue queue;
   Consumer(BlockingQueue q) { queue = q; }
   public void run() {
     try {
       while (true) { consume(queue.take()); }
     } catch (InterruptedException ex) { ... handle ...}
   }
   void consume(Object x) { ... }
 }

 class Setup {
   void main() {
     BlockingQueue q = new SomeQueueImplementation();
     Producer p = new Producer(q);
     Consumer c1 = new Consumer(q);
     Consumer c2 = new Consumer(q);
     new Thread(p).start();
     new Thread(c1).start();
     new Thread(c2).start();
   }
 } This example was taken from the JDK 1.6 docs of BlockingQueue . So You can see that you are doing it the right way. Here's the quote which tells you that it have to work: Memory consistency effects: As with
  other concurrent collections, actions
  in a thread prior to placing an object
  into a BlockingQueue happen-before
  actions subsequent to the access or
  removal of that element from the
  BlockingQueue in another thread."
24047,832.0,2008-08-23T06:43:54Z,24040,6,"So far, from what I have seen, RMC seems to be one-of-a-kind, which is both cool, but sad :( It seems pretty inactive, but I have posted a couple of questions on there, and amazed at the response (a lot of people just have the RSS feed and wait for something to happen). So I would definately recommend just getting active on there.. However, I have read some great books on Safari including MS Press' ""Test Driven Development"" which I found really helped . However I have to admit, I still need a lot of work in adjusting my brain for TDD etc. Following this question though! (+1) look forward to seeing if there is more out there! :) Edit: Just found this blog post that outlines a number of good bits of software, websites and books :) I knew I had one somewhere!"
24131,1912.0,2008-08-23T10:42:51Z,24130,4,"It depends exactly what you mean by 'better'. I'd go for the object oriented way (using classes) because I find it makes for cleaner code (at least in my opinion). However, I'm not sure what the speed penalties might be for that option."
24134,1344.0,2008-08-23T10:52:51Z,24130,9,"The ""class"" that you've constructed above is what most people would use a struct for in other languages. I'm not sure what the performance implications are in PHP, though I suspect instantiating the objects is probably more costly here, if only by a little bit. That being said, if the cost is relatively low, it IS a bit easier to manage the objects, in my opinion. I'm only saying the following based on the title and your question, but:
Bear in mind that classes provide the advantage of methods and access control, as well. So if you wanted to ensure that people weren't changing weights to negative numbers, you could make the weight field private and provide some accessor methods, like getWeight() and setWeight() . Inside setWeight() , you could do some value checking, like so: public function setWeight($weight)
{
    if($weight >= 0)
    {
        $this->weight = $weight;
    }
    else
    {
        // Handle this scenario however you like
    }
}"
24136,1693.0,2008-08-23T10:56:10Z,24130,4,"Generally, I follow this rule: 1) Make it a class if multiple parts of your application use the data structure. 2) Make it a 2D array if you're using it for quick processing of data in one part of your application."
24138,888.0,2008-08-23T11:00:49Z,17980,-1,"One issue that hasn't been raised by others is whether double is the same as a float .  On some systems a different format specifier was needed for a double compared to a float.  Not least because the parameters passed could be of different sizes. %f - float
 %lf - double
 %g - double"
24148,137.0,2008-08-23T11:10:43Z,24130,3,"It's the speed that I am thinking of mostly, for anything more complex than what I have here I'd probably go with classes but the question is, what is the cost of a class? This would seem to be premature optimisation. Your application isn't going to take any real-world performance hit either way, but using a class lets you use getter and setter methods and is generally going to be better for code encapsulation and code reuse. With the arrays you're incurring cost in harder to read and maintain code, you can't unit test the code as easily and with a good class structure other developers should find it easier to understand if they need to take it on. And when later on you need to add other methods to manipulate these, you won't have an architecture to extend."
24171,,2008-08-23T12:12:18Z,24130,2,"The class that you have is not a real class in OO terms - its just been contructed to take the space of the instance variables. That said - there propably isnt much issue with speed - its just a style thing in your example. The intresting bit - is if you contsrtucted the object to be a real ""person"" class - and thinkng about the other attributes and actions that you may want of the person class - then you would notice not only a style performance - writting code - but also speed performance."
24204,1293.0,2008-08-23T12:56:59Z,24200,0,Yes your ideas will help. Lean on option 1 if there are no reads happening while your loading. Lean on option 2 if you destination table is being queried during your processing. @Andrew Question.  Your inserting in chunks of 300.  What is the total amount your inserting?  SQL server should be able to handle 300 plain old inserts very fast.
24209,1709.0,2008-08-23T13:00:12Z,24200,0,"How about increasing the memory allocated to the server or the buffer size used by the server, if possible?"
24214,2384.0,2008-08-23T13:10:32Z,24200,4,"Have you tried using transactions? From what you describe, having the server committing 100% of the time to disk, it seems you are sending each row of data in an atomic SQL sentence thus forcing the server to commit (write to disk) every single row. If you used transactions instead, the server would only commit once at the end of the transaction. For further help: What method are you using for inserting data to the server? Updating a DataTable using a DataAdapter, or executing each sentence using a string?"
24224,1690.0,2008-08-23T13:27:42Z,24200,18,"You're already using SqlBulkCopy , which is a good start. However, just using the SqlBulkCopy class does not necessarily mean that SQL will perform a bulk copy. In particular, there are a few requirements that must be met for SQL Server to perform an efficient bulk insert. Further reading: Prerequisites for Minimal Logging in Bulk Import Optimizing Bulk Import Performance Out of curiosity, why is your index set up like that? It seems like ContainerId/BinId/Sequence is much better suited to be a nonclustered index. Is there a particular reason you wanted this index to be clustered?"
24259,2509.0,2008-08-23T14:28:23Z,3260,6,"This problem has been studied in some detail. There are a set of very up-to-date implementations in the TSpectrum* classes of ROOT (a nuclear/particle physics analysis tool). The code works in one- to three-dimensional data. The ROOT source code is available, so you can grab this implementation if you want. From the TSpectrum class documentation: The algorithms used in this class have been published in the following references: [1] M.Morhac et al.: Background
  elimination methods for
  multidimensional coincidence gamma-ray
  spectra. Nuclear Instruments and
  Methods in Physics Research A 401
  (1997) 113-
  132. [2]  M.Morhac et al.: Efficient one- and two-dimensional Gold
  deconvolution and its application to
  gamma-ray spectra decomposition.
  Nuclear Instruments and Methods in
  Physics Research A 401 (1997) 385-408. [3]  M.Morhac et al.: Identification of peaks in
  multidimensional coincidence gamma-ray
  spectra. Nuclear Instruments and
  Methods in Research Physics A 
  443(2000), 108-125. The papers are linked from the class documentation for those of you who don't have a NIM online subscription. The short version of what is done is that the histogram flattened to eliminate noise, and then local maxima are detected by brute force in the flattened histogram."
24271,,2008-08-23T14:45:23Z,24270,28,"Verging on religious but I would say that you're painting an overly grim picture of the state of modern OOP. I would argue that it actually has reduced costs, made large software projects manageable, and so forth. That doesn't mean it's solved the fundamental problem of software messiness, and it doesn't mean the average developer is an OOP expert. But the modularization of function into object-components has certainly reduced the amount of spaghetti code out there in the world. I can think of dozens of libraries off the top of my head which are beautifully reusable and which have saved time and money that can never be calculated. But to the extent that OOP has been a waste of time, I'd say it's because of lack of programmer training, compounded by the steep learning curve of learning a language specific OOP mapping. Some people ""get"" OOP and others never will."
24275,1968.0,2008-08-23T14:48:10Z,24270,119,"The real world isn't ""OO"", and the idea implicit in OO--that we can model things with some class taxonomy--seems to me very fundamentally flawed While this is true and has been observed by other people (take Stepanov, inventor of the STL), the rest is nonsense. OOP may be flawed and it certainly is no silver bullet but it makes large-scale applications much simpler because it's a great way to reduce dependencies. Of course, this is only true for good OOP design. Sloppy design won't give any advantage. But good, decoupled design can be modelled very well using OOP and not well using other techniques. There are much better, more universal models ( Haskell's type model comes to mind) but these are also often more complicated and/or difficult to implement efficiently. OOP is a good trade-off between extremes."
24284,2608.0,2008-08-23T15:02:58Z,24270,7,"I have been writing OO code for the last 9 years or so.  Other than using messaging, it's hard for me to imagine other approach.  The main benefit I see totally in line with what CodingTheWheel said: modularisation.  OO naturally leads me to construct my applications from modular components that have clean interfaces and clear responsibilities (i.e. loosely coupled, highly cohesive code with a clear separation of concerns). I think where OO breaks down is when people create deeply nested class heirarchies.  This can lead to complexity.  However, factoring out common finctionality into a base class, then reusing that in other descendant classes is a deeply elegant thing, IMHO!"
24286,2131.0,2008-08-23T15:06:40Z,24270,6,"@CodingTheWheel But to the extent that OOP has been a waste of time, I'd say it's because of lack of programmer training, compounded by the steep learning curve of learning a language specific OOP mapping. Some people ""get"" OOP and others never will. I dunno if that's really surprising, though.  I think that technically sound approaches (LSP being the obvious thing) make hard to use , but if we don't use such approaches it makes the code brittle and inextensible anyway (because we can no longer reason about it).  And I think the counterintuitive results that OOP leads us to makes it unsurprising that people don't pick it up. More significantly, since software is already fundamentally too hard for normal humans to write reliably and accurately, should we really be extolling a technique that is consistently taught poorly and appears hard to learn? If the benefits were clear-cut then it might be worth persevering in spite of the difficulty, but that doesn't seem to be the case."
24288,2131.0,2008-08-23T15:08:58Z,24270,7,"@Sean However, factoring out common finctionality into a base class, then reusing that in other descendant classes is a deeply elegant thing, IMHO! But ""procedural"" developers have been doing that for decades anyway.  The syntax and terminology might differ, but the effect is identical.  There is more to OOP than ""reusing common functionality in a base class"", and I might even go so far as to say that that is hard to describe as OOP at all; calling the same function from different bits of code is a technique as old as the subprocedure itself."
24290,2131.0,2008-08-23T15:11:06Z,24270,6,@Konrad OOP may be flawed and it certainly is no silver bullet but it makes large-scale applications much simpler because it's a great way to reduce dependencies That is the dogma. I am not seeing what makes OOP significantly better in this regard than procedural programming of old. Whenever I make a procedure call I am isolating myself from the specifics of the implementation.
24291,279.0,2008-08-23T15:11:50Z,24270,1,"""Even if there is no actual [information architecture], it doesnt mean we dont experience or perceive it as such. Zen Buddhists say there is no actual self but they still name their kids.""-Andrew Hinton"
24293,832.0,2008-08-23T15:14:09Z,24270,14,"Its a programming paradigm.. Designed to make it easier for us mere mortals to break down a problem into smaller, workable pieces.. If you dont find it useful.. Don't use it, don't pay for training and be happy. I on the other hand do find it useful, so I will :)"
24296,1337.0,2008-08-23T15:22:08Z,23930,7,"D Templates: Functional template factorial(int n : 1)
{
  const factorial = 1;
}

template factorial(int n)
{
  const factorial =
     n * factorial!(n-1);
} or template factorial(int n)
{
  static if(n == 1)
    const factorial = 1;
  else 
    const factorial =
       n * factorial!(n-1);
} Used like this: factorial!(5)"
24300,1013.0,2008-08-23T15:31:41Z,23930,2,"Python: Recursive def fact(x): 
    return (1 if x==0 else x * fact(x-1)) Using iterator import operator

def fact(x):
    return reduce(operator.mul, xrange(1, x+1))"
24305,1414.0,2008-08-23T15:40:00Z,24200,1,"I think that it sounds like this could be done using SSIS packages . They're similar to SQL 2000's DTS packages. I've used them to successfully transform everything from plain text CSV files, from existing SQL tables, and even from XLS files with 6-digit rows spanned across multiple worksheets. You could use C# to transform the data into an importable format (CSV, XLS, etc), then have your SQL server run a scheduled SSIS job to import the data. It's pretty easy to create an SSIS package, there's a wizard built-into SQL Server's Enterprise Manager tool (labeled ""Import Data"" I think), and at the end of the wizard it gives you the option of saving it as an SSIS package. There's a bunch more info on Technet as well."
24307,1897.0,2008-08-23T15:42:51Z,24130,0,"If your code uses lot of functions that operate on those attributes (name/height/weight), then using class could be a good option."
24308,1553.0,2008-08-23T15:43:04Z,24270,14,"Relative to straight procedural programming, the first fundamental tenet of OOP is the notion of information hiding and encapsulation. This idea leads to the notion of the class that seperates the interface from implementation. These are hugely important concepts and the basis for putting a framework in place to think about program design in a different way and better (I think) way. You can't really argue against those properties - there is no trade-off made and it is always a cleaner way to modulize things. Other aspects of OOP including inheritance and polymorphism are important too, but as others have alluded to, those are commonly over used. ie: Sometimes people use inheritance and/or polymorphism because they can, not because they should have. They are powerful concepts and very useful, but need to be used wisely and are not automatic winning advantages of OOP. Relative to re-use. I agree re-use is over sold for OOP. It is a possible side effect of well defined objects, typically of more primitive/generic classes and is a direct result of the encapsulation and information hiding concepts. It is potentially easier to be re-used because the interfaces of well defined classes are just simply clearer and somewhat self documenting."
24311,1965.0,2008-08-23T15:49:53Z,24310,10,The IRC Specification is laid out in RFC 1459 http://www.irchelp.org/irchelp/rfc/rfc.html
24316,2131.0,2008-08-23T15:54:57Z,24270,5,"@Jeff Relative to straight procedural programming, the first fundamental tenet of OOP is the notion of information hiding and encapsulation. This idea leads to the notion of the class that seperates the interface from implementation. Which has the more hidden implementation: C++'s iostreams, or C's FILE*s? I think the use of opaque context objects (HANDLEs in Win32, FILE*s in C, to name two well-known examples--hell, HANDLEs live on the other side of the kernel-mode barrier, and it really doesn't get much more encapsulated than that) is found in procedural code too; I'm struggling to see how this is something particular to OOP. I suppose that may be a part of why I'm struggling to see the benefits: the parts that are obviously good are not specific to OOP, whereas the parts that are specific to OOP are not obviously good! (this is not to say that they are necessarily bad, but rather that I have not seen the evidence that they are widely-applicable and consistently beneficial)."
24320,1965.0,2008-08-23T15:57:02Z,24270,0,"OOP has reduced costs, and increased efficiency. When I made the jump from classic ASP/VBScript to C# I noticed a HUGE increase in productivity thanks to OOP."
24322,1965.0,2008-08-23T15:59:07Z,24310,7,"I found this gem on Wikipedia. Sounds intimidating. It's actually not. Telnet onto an IRC Server and witness the simplicity of the protocol first hand. The hardest part is the handshake, after that its very simple."
24331,1968.0,2008-08-23T16:05:08Z,24270,21,"I think the use of opaque context objects (HANDLEs in Win32, FILE*s in C, to name two well-known examples--hell, HANDLEs live on the other side of the kernel-mode barrier, and it really doesn't get much more encapsulated than that) is found in procedural code too; I'm struggling to see how this is something particular to OOP. HANDLE s (and the rest of the WinAPI) is OOP! C doesn't support OOP very well so there's no special syntax but that doesn't mean it doesn't use the same concepts. WinAPI is in every sense of the word an object-oriented framework. See, this is the trouble with every single discussion involving OOP or alternative techniques: nobody is clear about the definition, everyone is talking about something else and thus no consensus can be reached. Seems like a waste of time to me."
24343,279.0,2008-08-23T16:27:07Z,23930,2,"two of many Mathematica solutions (although ! is built-in and efficient): (* returns pure function *)
(FixedPoint[(If[#[[2]]>1,{#[[1]]*#[[2]],#[[2]]-1},#])&,{1,n}][[1]])&

(* not using built-in, returns pure function, don't use: might build 1..n list *)
(Times @@ Range[#])&"
24349,2131.0,2008-08-23T16:31:57Z,24270,1,"HANDLEs (and the rest of the WinAPI) is OOP! Are they, though?  They're not inheritable, they're certainly not substitutable, they lack well-defined classes... I think they fall a long way short of ""OOP""."
24361,1968.0,2008-08-23T16:42:13Z,24270,11,"HANDLEs (and the rest of the WinAPI) is OOP! Are they, though? They're not inheritable, they're certainly not substitutable, they lack well-defined classes... I think they fall a long way short of ""OOP"". Have you ever created a window using WinAPI? Then you should know that you define a class ( RegisterClass ), create an instance of it ( CreateWindow ), call virtual methods ( WndProc ) and base-class methods ( DefWindowProc ) and so on. WinAPI even takes the nomenclature from SmallTalk OOP, calling the methods messages (Window Messages). Handles may not be inheritable but then, there's final in Java. They don't lack a class, they are a placeholder for the class: That's what the word handle means. Looking at architectures like MFC or .NET WinForms it's immediately obvious that except for the syntax, nothing much is different from the WinAPI."
24362,1644.0,2008-08-23T16:42:45Z,24270,43,"All too often, the class is used
  simply for its syntactic sugar; it
  puts the functions for a record type
  into their own little namespace. Yes, I find this to be too prevalent as well. This is not Object Oriented Programming. It's Object Based Programming and data centric programing. In my 10 years of working with OO Languages, I see people mostly doing Object Based Programming. OBP breaks down very quickly IMHO since you are essentially getting the worst of both words: 1) Procedural programming without adhering to proven structured programming methodology and 2) OOP without adhering to to proven OOP methodology. OOP done right is a beautiful thing. It makes very difficult problems easy to solve, and to the uninitiated (not trying to sound pompous there), it can almost seem like magic. That being said, OOP is just one tool in the toolbox of programming methodologies. It is not the be all end all methodology. It just happens to suit large business applications well. Most developers who work in OOP languages are utilizing examples of OOP done right in the frameworks and types that they use day-to-day, but they just aren't aware of it. Here are some very simple examples: ADO.NET, Hibernate/NHibernate, Logging Frameworks, various language collection types, the ASP.NET stack, The JSP stack etc... These are all things that heavily rely on OOP in their codebases."
24369,1287.0,2008-08-23T16:50:42Z,11720,3,"Checkout Powershell Management library for Hyper-V on CodePlex . Some features: Finding a VM Connecting to a VM Discovering and manipulating Machine states Backing up, exporting and snapshotting VMs Adding and removing VMs, configuring motherboard settings. Manipulating Disk controllers, drives and disk images Manipluating Network Interface Cards Working with VHD files"
24398,2131.0,2008-08-23T17:13:26Z,24270,4,"Have you ever created a window using WinAPI? More times than I care to remember. Then you should know that you define a class (RegisterClass), create an instance of it (CreateWindow), call virtual methods (WndProc) and base-class methods (DefWindowProc) and so on. WinAPI even takes the nomenclature from SmallTalk OOP, calling the methods messages (Window Messages). Then you'll also know that it does no message dispatch of its own, which is a big gaping void. It also has crappy subclassing. Handles may not be inheritable but then, there's final in Java. They don't lack a class, they are a placeholder for the class: That's what the word handle means. Looking at architectures like MFC or .NET WinForms it's immediately obvious that except for the syntax, nothing much is different from the WinAPI. They're not inheritable either in interface or implementation, minimally substitutable, and they're not substantially different from what procedural coders have been doing since forever. Is this really it?  The best bits of OOP are just... traditional procedural code? That's the big deal?"
24442,580.0,2008-08-23T18:04:49Z,24270,45,"OOP isn't about creating re-usable classes, its about creating Usable classes."
